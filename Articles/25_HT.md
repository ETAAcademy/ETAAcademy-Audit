# ETAAcademy-Audit: 25. Hardware Trojans

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>25 HT</td>
    <td>
      <table>
        <tr>
          <th>audit</th>
          <th>basic</th>
          <th>article</th>
          <td>HT</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-Audit](https://github.com/ETAAcademy/ETAAcademy-Audit)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# Hardware Trojan Threats Across Modern ICs and AI Accelerators: Detection, Defense, and Verification Frameworks

Amid VLSI evolution and supply‑chain outsourcing, ICs face hardware Trojan (HT) threats—especially in complex open‑source hardware (e.g., RISC‑V, BTC PoW miners, Web 3 wallet and PQC hardware). HTs comprise triggers and payloads that can cause data leakage, functional tampering, and denial of service, with diverse insertion layers and trigger/payload forms.

Detection spans post‑silicon (destructive de‑packaging, non‑destructive logic/ATPG, side‑channel SCA) and pre‑silicon (RTL/Gate‑Level Netlist static/dynamic analyses), alongside exploration of QDM imaging and PUF/PReF as roots of trust. ML and graph learning leverage AST/DFG/VDG representations to improve detection but face scalability and subgraph‑isomorphism bottlenecks; FPGA bitstream analysis using TSVD+RF/SVM achieves solid metrics, though heuristic features generalize poorly. LLMs can generate SystemVerilog security assertions, locate vulnerabilities, and map CWEs, showing auxiliary verification potential, but most are closed‑source and misaligned with HT‑specific goals.

HTs also threaten AI inference and accelerators, where low‑precision adders/multipliers and Approximate Computing (AC) are prevalent; while AC boosts efficiency, it opens attack surfaces at truncation/approximation points, MSB carry prediction, LSB compressors, and final accumulation. Recommended countermeasures include CPU, PERL, ECU, TAI bypassing, and PPM anomaly monitoring, and avoiding approximate‑adder‑based final accumulation in safety‑critical contexts.

Spiking Neural Networks (SNNs), despite low power from asynchronous, event‑driven operation, are vulnerable to input‑trigger HTs that use Gumbel‑Softmax and STE to craft rare spike sequences, forcing neuron saturation and permanent firing with minimal area/power overhead; defenses rely on post‑silicon methods plus neuron monitoring and input‑layer out‑of‑distribution filtering, but the lack of internal spike logging in many SNNs enables evasion. Overall, integrating pre/post‑silicon techniques, SCA/QDM, DG/DA, ML/GNN, and LLM‑based assertion generation—while hardening approximate arithmetic and SNN runtime monitoring—builds a scalable HT detection and verification framework.

---

## 1. Hardware Trojan Detection: Challenges and Machine Learning-Based Advances

The advances in **Very Large Scale Integration (VLSI)** technology have dramatically increased the complexity and cost of **Integrated Circuit (IC)** design. To meet the demand for time-efficient and cost-effective manufacturing, the IC industry has long relied on **third-party (3P) Electronic Design Automation (EDA)** tools and **Intellectual Property (IP)** cores. However, the globalization and outsourcing of the IC supply chain have introduced numerous security vulnerabilities—ranging from design flaws and system-level impacts to malicious modifications.

During the hardware development lifecycle, devices may be compromised at multiple stages. Typically, an IC design begins with a **Register-Transfer Level (RTL)** description based on IP cores. EDA tools then convert this design into a gate-level netlist and perform **physical layout design** before manufacturing and assembling the final IC modules. While **open-source hardware** has increased transparency, it does not inherently guarantee security. Among the key threats is the **Hardware Trojan (HT)**, capable of causing information leakage, functional corruption, denial-of-service, or degradation of performance and reliability.

As ICs grow more complex and Trojans become increasingly stealthy, **traditional algorithmic methods** have become inefficient and ineffective for post-silicon detection. In contrast, **machine learning (ML)**—as a data-driven paradigm—offers inherent generalization capabilities, automated workflows, and efficient inference, all of which make it promising for HT detection. The intersection of ML and hardware security introduces new opportunities for both **attack and defense**, from designing stealthy HTs to developing automated test pattern generation.

Some ML-based detection approaches utilize **side-channel information** to reveal _non-functional_ HTs—leakages that traditional static or logic testing methods fail to capture. Others focus on the **structural and behavioral logic** of ICs to understand their inherent properties.

### Nature and Taxonomy of Hardware Trojans

A **Hardware Trojan (HT)** is defined as a malicious, unintended, and deliberate modification of an electronic circuit. It generally comprises two main components: a **trigger mechanism** and a **payload mechanism**. The trigger defines the condition that activates the Trojan—such as a specific input combination—while the payload performs the malicious action, such as leaking encryption keys or disabling certain functionalities.

Existing literature classifies HTs based on the **attacker**, **target circuit type**, **insertion level** (RTL, gate-level, transistor-level, or layout-level), **trigger type** (random, input-defined, or always-on), and **payload effect** (information leakage, performance degradation, functional modification, or denial of service). Attackers aim to design stealthy Trojans that evade known countermeasures, while defenders seek to prevent insertion or detect their presence.

HTs range from simple **combinational and sequential designs** to more sophisticated forms such as **hidden side-channel attacks**, **silicon wear-out mechanisms**, **dopant polarity modifications**, **charge siphoning (A2 attacks)**, and **analog/mixed-signal Trojans** that leak sensitive data via covert communication channels.

### HT Detection Paradigms

**Hardware Trojan detection** remains a challenging research problem, with extensive efforts dedicated to developing high-accuracy detection methods. Broadly, detection techniques are categorized into **post-silicon** and **pre-silicon** approaches.

#### Post-Silicon Techniques

These serve as the last line of defense, analyzing circuits _after_ fabrication.

- **Destructive methods** include delayering and imaging using scanning electron microscopy (SEM), followed by comparison with a golden reference design. While effective, these methods permanently damage the IC.
- **Non-destructive methods** rely on **logic testing**, **Automatic Test Pattern Generation (ATPG)**, or **Side-Channel Analysis (SCA)** to detect HT presence without harming the chip. SCA methods monitor **power consumption**, **timing**, or **electromagnetic (EM)** emissions during runtime to infer Trojan-induced anomalies.

Although these methods can detect both functional and non-functional Trojans, they are often time-consuming and require additional on-chip hardware for observability. Moreover, ATPG-based techniques struggle to detect dormant or information-leakage Trojans.

Emerging tools such as the **Quantum Diamond Microscope (QDM)** have shown promise in imaging current flows in ICs by exploiting magnetic field side channels, potentially localizing Trojan-induced anomalies with high spatial resolution and sensitivity.

#### Pre-Silicon Techniques

Pre-silicon detection analyzes circuits _before_ fabrication, usually at the RTL or gate-level netlist. Early methods relied on static circuit analysis, exploiting the stealthiness of HTs—namely their tendency to remain inactive and trigger under rare conditions. Tools like **UCI**, **FANCI**, and **ANGEL** identify unused or rarely activated logic, while dynamic approaches such as **VeriTrust** target similar patterns.

However, literature shows that well-crafted Trojans can easily bypass these methods. Consequently, **machine learning-based** pre-silicon approaches have become increasingly relevant.

#### Graph-Based Machine Learning for HT Detection

Recent advances in **Graph Learning (GL)**, particularly **Graph Neural Networks (GNNs)**, have achieved state-of-the-art performance in hardware security tasks. This is because IC designs—whether RTL, gate-level, or physical layouts—are inherently **graph-structured**.

Frameworks like **HW2VEC** generate **Abstract Syntax Trees (AST)** and **Data Flow Graphs (DFG)** from Verilog RTL code, extract node embeddings, and train GNNs for Trojan detection. **GNN4HT** employs **Graph Isomorphism Networks (GIN)** to better distinguish near-isomorphic subgraphs, thereby enhancing detection accuracy.

Other models such as **NetWise** and **NetVGE** convert RTL or GLN into **Variable Dependency Graphs (VDGs)** and apply **Knowledge Graph Embedding (KGE)** techniques to map nodes into high-dimensional latent spaces.

However, static graph analysis faces scalability issues—detecting Trojans in large-scale designs can lead to **NP-complete** problems like subgraph isomorphism. While GNNs provide improved accuracy and flexibility, training such models on gate-level datasets is often computationally expensive and time-consuming.

#### Bitstream-Level ML Detection and FPGA Security

Some methods analyze **bitstream files** post place-and-route, especially for **Field Programmable Gate Arrays (FPGAs)**. By processing unencrypted bitstreams, ML models can detect malicious configurations inserted by adversaries—e.g., attacks that induce heating or denial-of-service through malicious ring oscillators.

In this context, **Random Forests (RF)** achieved the best detection F1-score (97.4%), while **Support Vector Machines (SVM)** performed best for localization. Key ML features include conditional branch sizes and structural statistics extracted from the bitstream. **Gradient Boosting (GB)** models showed strong performance, with _maxbranchbit_ (maximum branch block size) emerging as the most discriminative feature.

Nevertheless, these methods depend heavily on **heuristic features**, which may not generalize to other designs or Trojan types.

#### LLMs and Secure Hardware Design

Recent research explores using **fine-tuned Large Language Models (LLMs)** for tasks such as **error localization**, **secure Verilog code generation**, and **vulnerability detection** across hardware design documents. For example, a fine-tuned **BERT** model on documentation from **RISC-V**, **OpenRISC**, **MIPS**, **OpenSPARC**, and **OpenTitan SoC** successfully identified multiple security flaws in unseen designs.

Other studies compare **OpenAI’s ChatGPT**, **Salesforce CodeGen**, and **DaVinci** for generating **SystemVerilog Assertions (SVA)**—a potential complement to HT detection pipelines for verifying design-level security properties.

By leveraging LLMs for **assertion generation**, **vulnerability mapping to CWE (Common Weakness Enumeration)**, and **security specification validation**, these approaches could form an integral part of the hardware trust ecosystem—though most remain focused on closed-source models and non-Trojan-specific tasks.

---

### 1.1 Machine Learning Methods for Hardware Trojan Detection

The concept of a **weak learner** originates from _ensemble learning_, a machine learning paradigm that combines multiple moderately performing models to create a stronger predictive system. In this framework, several weak classifiers are aggregated to form a _strong learner_ capable of superior generalization and accuracy. Common ensemble learning frameworks include **Bagging** (Bootstrap Aggregating) — represented by algorithms such as _Random Forests_ — and **Boosting**, which encompasses algorithms such as _AdaBoost_, _XGBoost_, and _LightGBM_.

In the context of **Hardware Trojan (HT)** detection, each machine learning model can be viewed as a distinct weak learner with a particular strategy:

- **K-Nearest Neighbors (KNN)** classifies a circuit based on the “votes” of its _k_ nearest neighbors (e.g., if 3 out of 5 are Trojans, the sample is labeled as a Trojan).
- **Support Vector Machine (SVM)** identifies a hyperplane that separates “normal circuits” from “Trojan-infected circuits.”
- **Logistic Regression** computes the probability of Trojan presence (e.g., 0.83).
- **Random Forest** and **XGBoost** aggregate the outputs of many decision trees — each acting as a weak learner — to perform robust classification.
  Ultimately, the outputs of these classifiers can be fused through an additional integration layer to construct a powerful ensemble detection system.

Previous research in ML-based HT detection has employed **XGBoost**, **Random Forests**, **Convolutional Neural Networks (CNNs)**, **SVMs**, and **Multilayer Neural Networks (MLPs)**. However, these approaches still face challenges in achieving consistent performance across diverse Trojan datasets and evaluation metrics. For instance, SVM and MLP models often yield relatively low _True Positive Rates (TPR)_ (~85%) and _True Negative Rates (TNR)_ (~70%) on benchmarks such as the Trust-Hub netlist dataset.

#### Support Vector Machine (SVM)

The **Support Vector Machine (SVM)** is a supervised learning algorithm used for classification tasks. It constructs a hyperplane in the feature space to separate different classes and effectively handles nonlinear relationships using kernel functions. SVMs are robust against overfitting and are particularly effective when equipped with the **Radial Basis Function (RBF)** kernel.

The workflow for applying SVMs includes dataset preparation and splitting into training and testing subsets, kernel selection, and regularization parameter tuning. Model performance is evaluated using metrics such as accuracy, recall, _F1-score_, and _AUC-ROC_. Hyperparameters are fine-tuned via cross-validation to achieve optimal generalization.

#### Random Forest

The **Random Forest** algorithm, another supervised learning technique, is widely applied for both classification and regression problems. It consists of an ensemble of decision trees. In our framework, Random Forest serves as the backbone of an **ensemble classification method** for detecting hardware Trojans.

Each decision tree is constructed using a bootstrap sample from the training dataset and a random subset of features. For a given training set $D$ and feature set $F$ of size $s$, a tree is grown to a maximum depth $m$ using selected features. The final model $F$ is an ensemble of all decision trees built in this manner. Performance is evaluated using accuracy, recall, and _F1-score_.

#### K-Nearest Neighbors (KNN)

The **K-Nearest Neighbors (KNN)** algorithm is a simple yet effective supervised learning approach that classifies samples based on proximity in the feature space. In HT detection, KNN has been used to classify circuits based on side-channel and structural features.

In our experiments, the best performance was achieved with $K = 5$. Although KNN is flexible and interpretable, it can be sensitive to measurement variability. To mitigate this, KNN is integrated as a **weak learner** within our ensemble model. Model training includes data preparation, dataset splitting, parameter tuning, and cross-validation based on accuracy and recall performance metrics.

#### Logistic Regression

**Logistic Regression** is a supervised learning algorithm used for binary classification tasks. It models the probability of a binary outcome using a logistic function, establishing a linear relationship between predictor variables and the log-odds of the outcome. The cost function quantifies model performance, and gradient descent is applied to minimize it.

The model is trained using optimal parameter values and validated via metrics such as accuracy, recall, and _F1-score_. When performance is suboptimal, parameters are fine-tuned through cross-validation. In this study, logistic regression serves as one of the weak learners in our ensemble framework.

#### XGBoost

**Extreme Gradient Boosting (XGBoost)** is a high-performance supervised learning algorithm widely used for classification tasks, including HT detection based on side-channel features. XGBoost builds an ensemble of complementary decision trees, optimizing a regularized objective function that balances model accuracy and complexity.

The XGBoost pipeline includes dataset loading, preprocessing, splitting into training and testing sets, and defining hyperparameters such as learning rate and tree depth. The model is trained iteratively, monitored via validation performance, and fine-tuned using cross-validation. Evaluation metrics such as accuracy and recall quantify overall performance.

---

### 1.2 NLP- and ML-Based Methods for Hardware Trojan Detection

Approaches leveraging **Natural Language Processing (NLP)** and **Machine Learning (ML)** for **Hardware Trojan (HT)** detection are founded on the use of _NLP embeddings_ as features for _classical ML classifiers_—that is, non–deep learning models.

In text classification tasks such as sentiment analysis, _word embeddings_ are widely used as feature representations for traditional ML classifiers. Word embeddings translate natural language information into numerical vectors that capture semantic meaning. Within NLP, embeddings range from simple frequency-based representations to more complex contextualized models.

In the context of HT detection, **Bag of Words (BoW)** and **Term Frequency–Inverse Document Frequency (TF-IDF)** are the most common simple embedding techniques. For both methods, the 1,000 most relevant features are selected and used as input to a binary ML classifier, where “1” denotes the presence of a Trojan and “0” denotes a clean (Trojan-free) design. All comments within Verilog source files are removed prior to processing. The primary evaluation metric used during training is **accuracy**, and experiments employ **10-fold cross-validation**, with each fold comprising 70% training data and 30% testing data.

The **PyCaret** open-source ML library is employed to train and evaluate HT detection classifiers. PyCaret offers a unified interface with minimal code while supporting a wide range of algorithms and hyperparameter optimization options. Among the tested classifiers, notable models include **Decision Trees**, **Random Forests**, **K-Nearest Neighbors (KNN)**, **Support Vector Machines (SVMs)**, and **Naïve Bayes**.

#### Bag of Words and TF-IDF Representations

The **Bag of Words (BoW)** model represents documents—such as text files, code snippets, or hardware designs—as unordered collections of words, disregarding syntax and context. In HT detection, BoW can be applied to Verilog designs by converting code tokens (e.g., _always_, _if_, _wire_, _assign_) into numerical frequency features. Since embedding dimensionality scales with vocabulary size, a fixed number of the most frequent tokens are typically retained as features.

**TF-IDF** is an enhanced embedding method that assigns weights to words based on their frequency within a document and their rarity across multiple documents. Compared to BoW, TF-IDF downweights common, uninformative tokens (e.g., _module_, _endmodule_) while amplifying the influence of rare or suspicious keywords that may indicate anomalous or malicious patterns in the design.

#### Leveraging Large Language Models (LLMs)

Another promising direction involves using **Large Language Models (LLMs)** to generate semantic embeddings that capture complex contextual relationships. LLMs are deep learning architectures based on the **Transformer** model, which uses _attention mechanisms_ to process text and represent each token according to its contextual dependencies. These models are trained on massive datasets using next-token prediction objectives, requiring substantial computational and data resources.

In this approach, various open-source LLMs are employed, including **Olmo**, **GPT-2**, **CodeGen**, and **V-Gen**. Embeddings are obtained through the **Hugging Face API**, which provides cloud-based inference and model hosting capabilities.

- **Olmo** is a general-purpose open-source LLM trained on a diverse corpus comprising web content, academic papers, code, books, and encyclopedic materials.
- **GPT**, developed by OpenAI, is trained on large-scale English text corpora and is widely used as a benchmark Transformer model.
- **CodeGen**, introduced by Salesforce, is specialized for code generation and trained on multilingual programming datasets (e.g., C, C++, Go, Java, JavaScript, Python).
- **V-Gen**, derived from CodeGen, is fine-tuned specifically for **Verilog code generation**, demonstrating superior performance over the OpenAI Codex model in generating functionally correct Verilog programs.

Additionally, **Llama 3.1** and **Llama 3.3** (70B parameter variants) from Meta are utilized via the Hugging Face inference API to obtain high-quality embeddings.

For architectural modeling and simulation, **Gem5**, a widely used open-source platform for computer architecture research, provides cycle-accurate simulation of multiple processor architectures—including RISC-V—allowing detailed system-level performance analysis.

#### Prompt Engineering and Optimization

LLM-based HT detection experiments employ four distinct prompting strategies:

- **Simple Prompts** — concise, objective questions directly related to Trojan detection.
- **Trust-Hub Definitions and Examples** — prompts incorporating benchmark definitions and examples from the Trust-Hub dataset.
- **Checklist-Based Reviews** — structured prompts based on evaluation criteria for hardware security.
- **Perspective-Based Reviews** — multi-role prompt formulations derived from prompt optimization techniques.

**Prompt Engineering** is the process of iteratively refining manually designed prompts to better align with LLM reasoning capabilities. Different prompt strategies—such as task decomposition, step-by-step reasoning, self-evaluation, and multi-model collaboration—can significantly affect model outputs.

**Prompt Optimization** extends this process using an auxiliary LLM to refine prompts automatically. The optimization pipeline consists of two stages:

- **Inference Stage:** the optimizing LLM analyzes why a prompt underperforms and generates refinement suggestions.
- **Refinement Stage:** human evaluators review and select suggestions to improve the prompt, which is then re-evaluated using the target LLM.

Optimization experiments utilize **Google Gemini 2.0** and **NotebookLM** tools. NotebookLM is a **Retrieval-Augmented Generation (RAG)** platform that allows users to upload documents as external knowledge sources for LLM reasoning. RAG enhances task-specific performance and reduces hallucination by grounding responses in verifiable references.

---

### 1.3 Machine Learning for Hardware Trojan Detection under Distribution Shift and Domain Generalization

Machine Learning (ML) has demonstrated significant effectiveness in **integrated circuit (IC) security**, particularly in **Hardware Trojan (HT) detection**. However, the generalization capability of ML-based models largely depends on their ability to handle **distribution shifts (DS)**—a crucial aspect of AI alignment. Addressing DS improves a model’s adaptability to unseen IC designs and evolving HT threats.

Most ML methods are built on the **i.i.d. assumption**, i.e., that training and testing data are independently drawn from the same distribution. In practice, this assumption often fails: when the training data does not accurately represent the real-world deployment distribution, **distribution shift** occurs, leading to poor generalization and degraded performance on unseen data. In HT detection, distribution shifts naturally arise due to differences **between IC types** (inter-design variance, e.g., cryptographic vs. communication modules) and **within IC types** (intra-design variance, e.g., AES vs. DES—both cryptographic cores but with different architectures). Ignoring DS can result in poor detection robustness, leading to increased false positives and false negatives during testing.

A prominent strategy to mitigate DS is through **domain-invariant representation learning**. Techniques such as **Domain Generalization (DG)** and **Domain Adaptation (DA)** often employ **domain discriminators** and **adversarial training**—for instance, Domain-Adversarial Neural Networks (DANN)—to learn features that are invariant or minimally sensitive to the underlying distribution differences. These approaches have shown strong performance across various machine learning applications. More advanced strategies, such as **disentangled representation learning** and **dynamic test-time adaptation**, have also been explored, though their benefits are typically more pronounced in low-data or low-shift regimes. In HT detection, such challenges are especially relevant due to limited data diversity and uncertainty in both IC design and Trojan structures.

#### Hardware Trojan Detection under Distribution Shift and Domain Generalization

In the context of IC design, a **Hardware Trojan (HT)** is a maliciously inserted modification—often a small set of gates or logic triggers—hidden within a circuit. HTs are typically activated only under **rare trigger conditions**, while for most inputs the circuit behaves identically to the original (functionally equivalent). For example, if the original design computes a function $g(x)$, the Trojan-inserted version $\tilde{x} = x + t$ may still satisfy $g(\tilde{x}) = g(x)$, making detection challenging. Thus, the goal of HT detection is to identify subtle structural or distributional differences represented by $t$.

An ML-based detection model $f: X \to Y$ classifies an IC design as Trojan-free or Trojan-infected. The input $X$ consists of IC design features (e.g., logic structure, gate-level topology, or power features), and the output $Y \in {0, 1}$ indicates whether a Trojan is present. During training, the model learns from a dataset sampled from distribution $P_{XY}^{\text{train}}$. However, IC designs vary widely—different fabrication technologies, architectures, and modular structures cause the **test distribution $P_{XY}^{\text{test}}$** to differ from the training one. Consequently, a model trained on a specific IC family may overfit to design-specific artifacts (spurious correlations) and fail to generalize to new designs—a classic **distribution shift** problem.

#### Domain-Based View of IC Designs

Each **IC design** can be viewed as a distinct **domain** $S = {(x_i, y_i)}^N_{i=1}$, characterized by a joint distribution $P_{XY}$. Samples within the same design share a common distribution, while samples from different designs vary across domains. The objective is to learn a prediction function $f_\theta: X \to Y$ that performs well even on unseen domains $S_{\text{test}}$:

$$
\min_\theta \mathbb{E}_{(x,y) \sim S_{\text{test}}} [ \ell(f_\theta(x), y) ],
$$

where $\ell(\cdot,\cdot)$ is the loss function, and the expectation is over the test distribution.

To achieve **domain generalization**, the model learns **domain-invariant features** $h_x$ that are insensitive to domain-specific variations. This is typically achieved through a **domain classifier** $F_d(h_x; \theta_{Fd})$ that predicts which domain a sample originates from, combined with a **Gradient Reversal Layer (GRL)** that adversarially prevents the encoder from encoding domain-specific information. The domain classifier’s prediction is:

$$
\hat{d} = F_d(h_x; \theta_{Fd}) = \text{Softmax}(MLP(h_x)),
$$

and the domain classification loss is optimized as:

$$
\min_{\theta_{Enc}, \theta_{Fd}} \sum_i \ell_d(\hat{d}_i, d_i).
$$

#### Adversarial Training and the Gradient Reversal Mechanism

The **Gradient Reversal Layer (GRL)** is the core of distribution-aware learning. While the domain classifier attempts to **identify** the source domain of each sample, the encoder is trained to **confuse** it, thereby enforcing domain invariance in the learned features. In the **forward pass**, GRL behaves as an identity function. During **backpropagation**, however, it multiplies the gradient by (-\lambda), reversing its direction and updating the encoder to minimize domain-specific signals. The combined training objective is:

$$
\min_{\theta_{Enc}, \theta_{Fy}, \theta_{Fd}}
\sum_i \ell_y(\hat{y}_i, y_i) - \lambda \ell_d(\hat{d}_i, d_i),
$$

where the first term promotes accurate Trojan detection, and the second term enforces domain invariance. The parameter $\lambda$ controls the trade-off between detection accuracy and generalization robustness.

#### Semi-Supervised Domain Extension

A **semi-supervised extension** further enhances robustness by incorporating **unlabeled target-domain samples**—data from new IC designs where only the domain label (not the Trojan label) is known. These unlabeled samples contribute to domain discrimination but not to the detection loss:

$$
\min_{\theta_{Enc}, \theta_{Fy}, \theta_{Fd}}
\sum_{i=1}^{N} \ell_y(\hat{y}_i, y_i)
\lambda \sum_{j=1}^{N+M} \ell_d(\hat{d}_j, d_j),
$$

where $N$ is the number of labeled samples and $M$ is the number of unlabeled target-domain samples. This strategy effectively reduces the gap between training and testing distributions, improving adaptability to unseen IC designs.

---

## 2. Approximate Computing, ML Accelerators, and Hardware-Trojan Risks

Modern machine learning—particularly inference acceleration—commonly exploits **reduced-precision arithmetic** (e.g., 8-bit or 4-bit adders/multipliers) and **approximate computing (AC)** techniques to trade small, controlled numerical errors for significant gains in energy efficiency and throughput. Convolutional neural networks (CNNs), which are central to image recognition tasks such as face recognition, autonomous driving, and biometric authentication, are often deployed on energy- and compute-constrained edge devices. To meet these constraints, lightweight CNN architectures (e.g., MobileNet, ShuffleNet, GhostNet) have been developed, and hardware designers frequently combine these networks with approximate arithmetic and custom accelerators (e.g., DNNbuilder, Multi-CLP) to improve performance. However, approximate arithmetic introduces unique hardware-security concerns that must be addressed.

#### Motivation for Approximate Computing

Approximate computing exploits the inherent error tolerance and redundancy in many algorithms to relax numerical precision requirements and thus improve computational efficiency, energy usage, and hardware utilization. At the hardware level, AC may be realized by deploying approximate arithmetic units (e.g., adders and multipliers), using approximate data storage, modifying ISA semantics, or applying aggressive voltage scaling. These changes yield substantial benefits in contexts that do not require strict numerical fidelity, but they also open a new attack surface: an adversary may hide a **hardware trojan (HT)** inside approximate logic so that the malicious behavior is mistaken for the expected approximation error and therefore remains undetected.

#### Adversary Goals in Approximate Systems

In contrast to conventional HTs that aim to cause functional failures, HTs targeted at approximate circuits typically seek to introduce **subtle, controlled inaccuracies** that degrade application-level outputs (for example, reducing a model’s classification accuracy) without triggering suspicion during validation. Such attacks are most effective when implemented at the gate or cell level because they can blend with the expected approximation behavior of the circuit. The adversary’s objective is thus not binary failure but stealthy degradation—minimizing detectability while maximizing downstream impact.

### Approximate Adders: Architectures, Attacks, and Defenses

Approximate adders come in various topologies (truncation/rounding, carry-predicting, block-based). Below we summarize common architectures, the classes of attacks that exploit them, and representative mitigation strategies.

#### Truncation- and Rounding-based Adders

Truncation/rounding adders split the input operand into two regions: a **right (LSB) region** that is truncated or approximated, and a **left (MSB) region** that is computed exactly. An attacker can shift the truncation boundary toward the MSB so that more significant bits are approximated, thereby producing larger errors while remaining plausibly within the design’s approximation tolerance.

A useful attack model defines rare input patterns—**Trojan Activation Inputs (TAIs)**—that cause erroneous carry propagation. For example, a predicate such as

$$
(A_{p-1} \oplus B_{p-1}) \cdot (A_p \oplus B_p) = 1
$$

identifies adjacent bit positions where XORs equal 1, enabling carry errors to propagate under those specific, low-probability inputs (the TAI).

**Defense — Carry Predicting Unit (CPU):**
One mitigation inserts a carry prediction stage that estimates the carry into the MSB region. The predicted carry is used for TAI inputs to inhibit error propagation.

#### Carry-Predicting Adders

Carry-predicting adders replace a fixed truncation point with an **approximation point** where the MSBs of the right half are used to predict carries for the left half. The right half can itself be implemented as a bitwise OR or an approximate adder. Attackers can tamper with the MSB summation logic or with the carry-predictor to induce mispredictions. Rare input combinations, such as $(A_{p-1} + B_{p-1})(A_{p-2} + B_{p-2}) = 1$ or cases where $(A_p \oplus B_p) = 1$, increase the likelihood of incorrect predictions.

**Defense — PERL (Propagating Error Rectification Logic):**
PERL extends the carry prediction to examine additional higher-order bits (e.g., 3–4 MSBs) to reduce misprediction probability. This technique can reduce the raw error propagation probability from 25% to 6.25% by increasing the predictor’s observability at modest area cost.

#### Block-Based Adders

Block-based adders partition an adder into multiple **summation blocks**, each containing local sum logic, carry prediction, and carry selection, enabling parallel computation. The attacker’s strategy varies with block granularity:

- **Two-block case:** Target the MSB logic of the right block—rare input patterns such as $(A_{N/2-1}+B_{N/2-1})(A_{N/2-2}+B_{N/2-2}) = 1$ can trigger faults.
- **Four-block case:** The vulnerability arises when rare patterns appear in the MSBs of the lower two blocks; the predicate generalizes to combinations of block-level MSB sums.

Increasing the number of blocks improves nominal accuracy but enlarges the attack surface: attackers tend to modify MSB-block sum logic to introduce stealthy faults.

### Approximate Multipliers: Structure, Attack Vectors, and Countermeasures

A multiplier typically implements three logical stages:

- **Partial Product Matrix (PPM) generation**—bitwise ANDs create the partial products.
- **Compression**—compressor trees (e.g., Wallace or Dadda) reduce the partial product rows to a set of partial sums.
- **Accumulation**—final additions compute the product.

Approximate multipliers simplify one or more of these stages (e.g., by truncating LSBs, using approximate compressors, or employing approximate adders in the accumulation stage) to save area and power. These optimizations also provide attackers with injection points.

#### Narrow-bit (Truncated) Multipliers

Narrow-bit multipliers truncate the least significant (t) bits of the operands and perform multiplication on the remaining (N-t) bits. The resulting product is then shifted to restore bit-width. An adversary can replace some precise full adders in the compression or final adder stages with tampered versions that only produce errors for rare PPM patterns—these rare patterns act as **TAIs**. For example, a TAI predicate over PPM columns may be written as:

$$
ppm(1,2(N-t)-3)\cdot ppm(2,2(N-t)-3) + ppm(2,2(N-t)-3)\cdot ppm(3,2(N-t)-3) + ppm(1,2(N-t)-3)\cdot ppm(3,2(N-t)-3) = 1,
$$

which characterizes a rare combination of partial-product bits that triggers the trojanous behavior.

**Defense — ECU (Error Compensating Unit):**
An ECU detects TAI patterns (e.g., columns equal to ((0,0,0)) or other rare combinations) and activates compensation logic—e.g., OR-based corrections—to restore the contaminated product bits. This approach costs modest additional hardware but can neutralize tainted outputs for the identified TAIs.

#### Truncation & Rounding-based and Reduced-PPM Multipliers

- **Truncation & Rounding-based Multipliers** combine LSB truncation with rounding logic and dedicated compensation hardware.
- **Reduced-PPM Multipliers** omit generation of certain LSB partial products and rely on precise compression to mitigate accuracy loss.

In both cases, attackers may replace precise compressors with approximate or malicious ones; the **LSB compressors** are particularly attractive targets. TAIs for these architectures can be expressed as logic predicates over partial-product columns (e.g., combinations of ppm indices that, when true, cause an erroneous reduction).

**Mitigation:** Maintain separate, protected logic that computes the correct output bits for identified TAI signatures (i.e., compute correct bit(s) in parallel on a small precise path and use them when a TAI is detected). This localized bypass increases area and power slightly but preserves functional correctness for hostile inputs.

#### Approximate-Adder-Based Multipliers

In some designs, the final accumulation stage uses approximate adders rather than exact adders. A trojan embedded in such an approximate adder is particularly stealthy because the corruption manifests only at the end of the computation; the intermediate stages are not directly observable. Detecting the TAI pattern is also harder here because the attack triggers based on the unobservable internal state of the accumulation stage rather than on PPM columns. When the adder module is inaccessible for independent inspection, defenders must either avoid these multipliers in safety-critical contexts or implement PPM anomaly detectors that flag suspicious partial-product distributions. If the adder itself is compromised, however, the system’s ability to produce correct results is fundamentally undermined, making these attacks among the most difficult to defend.

### Practical Recommendations and Defensive Tradeoffs

- **Avoid unverified approximate arithmetic** in security-critical subsystems where small degradations are unacceptable.
- **Harden MSB logic and compressor trees**—attackers preferentially target MSB blocks and LSB compressors, so increased validation and redundancy in these areas pays off.
- **TAI detection and compensation** (ECU, PERL) are effective defenses with moderate area overhead. They are especially suitable when the TAI predicate set is small and can be efficiently detected in hardware.
- **Design-for-security (DfS)** approaches—e.g., combining lightweight runtime monitors, redundancy, and occasional golden-path checks—provide layered defense while retaining many AC benefits.
- **Testing and formal verification** must be extended to consider rare-input patterns and the idiosyncrasies of approximate arithmetic; standard functional tests may miss TAI-triggered trojans.

<details><summary>Code</summary>

```ALGORITHM
ALGORITHM 1: Pseudo code to compute ppm for narrow-bit multipliers
1: Initialize partial product matrix 𝑝𝑝𝑚 of size 𝑁 − 𝑡 × (2𝑁 − 2𝑡 − 1) to all zeros
2: for 𝑖 ← 1 to 𝑁 − 𝑡 do
3:      for 𝑗 ← 1 to 𝑁 − 𝑡 do
4:          𝑝𝑝𝑚(𝑖, 2(𝑁 − 𝑡) − 𝑖 + 1 − 𝑗) ← (input1((𝑁 − 𝑡) − 𝑗 + 1) & input2((𝑁 − 𝑡) − 𝑖 + 1))
5:      end for
6: end for
```

</details>

---

### Spiking Neural Networks, Neuromorphic Accelerators, and Input-Triggered Hardware-Trojan Threats

Spiking Neural Networks (SNNs) and neuromorphic computing are emerging as promising alternatives to conventional artificial neural networks (ANNs) for ultra-low-power inference and edge deployment. Unlike ANNs, SNNs process information in an **event-driven**, asynchronous manner inspired by biological neurons: information is encoded in sequences of spikes (either by spike timing or firing rate), and computation occurs only when spikes are emitted. This sparse, temporally precise operation enables **low latency** and **very low energy consumption**, which has driven significant investment in neuromorphic hardware platforms and accelerators.

#### Security Gap in SNNs

Despite their architectural advantages, the security properties of SNNs—particularly at the hardware level—are underexplored. Recent work has identified a novel class of **input-triggered hardware-trojan (HT)** attacks specifically tailored to SNN accelerators. These attacks embed a compact malicious modification in the hardware that is concentrated around a small group of neurons. The trigger is crafted in the spike domain so that a chosen neuron produces a rare, malicious spike pattern that would not appear under normal operation. When the neuron emits this pattern, the implanted Trojan forces the neuron into a saturating state: it begins to discharge persistently and cannot return to rest even after the triggering stimulus ceases. The resulting excess spike activity contaminates network computation and causes misclassification or other erroneous downstream behavior. Crucially, the area and power overheads of such SNN-targeted hardware Trojans can be negligible, enabling them to evade routine detection.

#### Attack Formulation

The attacker's goal in the **input-trigger generation** phase is to find an input $I_{tr}$ that causes the trojaned neuron’s output spike sequence $O(t)$ within a designated time window to match a predesigned rare trigger pattern $P$. The trigger pattern $P \in {0,1}^d$ is a length-(d) binary spike vector chosen to be rare and constrained by the neuron’s refractory period so that no two spikes occur inside the refractory window:

$$
P(i)P(j) = 0,\quad \forall j\in[i+1, i+\tau_{\text{ref}}].
$$

The pattern must also be Hamming-distant from typical (benign) outputs:

$$
d_H(O_i, P) \ge 1 \quad \text{for all benign } O_i.
$$

To synthesize $I_{tr}$, the attacker minimizes a distance objective that aligns the neuron’s output over the final $d$ timesteps with $P$:

$$
L = d_H\bigl(O((1+T/T_f-d):T/T_f),,P\bigr),
$$

so that the neuron’s final $d$ outputs exactly equal $P$ when $L=0$. Because spike events are non-differentiable, the attacker relaxes the discrete optimization problem by mapping a continuous real-valued input $I_{\text{real}}$ into a differentiable proxy $I_{\text{soft}}$ via **Gumbel-Softmax** and then binarizing with a **Straight-Through Estimator (STE)** to obtain a discrete input $I$. During optimization, $L$ is evaluated in the forward pass, and gradients are used to update the real input via Adam:

$$
I_{\text{real}} \leftarrow I_{\text{real}} - \text{lr}\cdot\nabla_{I_{\text{real}}} L,
$$

until $L=0$ or a time limit $t_{\text{limit}}$ is reached. The resulting $I_{tr}$ is a rare input sample capable of reliably activating the hardware Trojan.

#### Impact and Stealth

These SNN-specific Trojans are particularly stealthy because (1) they exploit timing-domain spike patterns rather than simple logic conditions, (2) they blend into the expected variability of approximate neuromorphic implementations, and (3) they can produce persistent, subtle degradation in downstream tasks (e.g., steadily decreasing classification accuracy) instead of catastrophic functional failure—making detection via standard functional tests difficult. On the hardware side, neuromorphic accelerators frequently lack fine-grained logging mechanisms for internal spike traces, further reducing observability and increasing the attack's stealth.

#### Defenses and Mitigations

When defenders do not know the specific trigger, they must rely primarily on post-silicon detection and run-time mitigation. In addition to general hardware security techniques such as reverse engineering, logic testing and ATPG, and side-channel monitoring, SNN deployments require tailored defenses:

- **Neuron Monitoring:** Real-time monitoring of individual neuron outputs (spike counts, interspike intervals, saturation indicators) to flag anomalous behaviors such as sustained firing or saturation. Monitors should be lightweight but sensitive to persistent deviations from expected firing statistics.

- **Out-of-Distribution (OOD) Input Filtering:** Many trigger inputs are OOD relative to normal input distributions; input prefilters or anomaly detectors at the sensor/ingest stage can reject or sanitize suspicious inputs before they reach the SNN.

- **Runtime State Checks and Reset Paths:** Provide mechanisms to detect and forcibly reset neurons showing persistent discharge (e.g., watchdog timers, supervisory circuits that can reinitialize membrane potential).

- **Spike-Trace Logging/Attestation:** Where feasible, include secure logging or cryptographic attestation of internal spike traces to enable offline forensic analysis and post-deployment auditing.

- **Design-Time Hardening:** At design and verification stages, include randomized stimulation and adversarial input testing (including optimization-based trigger search) to expose latent trojans, and apply formal methods where applicable to bound allowable spike patterns.

Each defense has cost-performance tradeoffs: e.g., fine-grained spike logging increases area and energy, whereas wholesale avoidance of neuromorphic optimizations undermines power benefits. Hence, a layered approach combining lightweight monitoring, OOD detection, and selective secure logging provides pragmatic protection for SNN accelerators.

<details><summary>Code</summary>

```Algorithm
Algorithm 2: Input trigger generation pseudo algorithm
Data: SNN model, dataset, global clock period Tf , Trojan neuron, neurons’ refractory period τref, maximum optimization time tlimit
Result: Input trigger Itr
Perform full inference and record Trojan neuron outputs Oi;
Define P of minimum length d that satisfies the conditions in Eqs. (1)-(2);
Randomize a real-valued input Ireal with duration T = d × Tf ;
t ← current time;
tlimit ← current time + tlimit;
    while L ≠ 0 ∧ t < tlimit do
        Generate the binary input I from Ireal using Eqs. (4)-(5);
        Perform a forward pass and calculate L in Eq. (6);
        if L = 0 then
            Itr ← I;
        Perform a backward pass and use Eq. (7) to refine Ireal;
        t ← current time;
If L ≠ 0, then repeat with T = T + 1 and/or use a more sparse P ;
```

</details>

---

[Trust-Hub](https://trust-hub.org/)
[NMSU-PEARL](https://github.com/NMSU-PEARL/Hardware-Trojan-Insertion-and-Detection-with-Reinforcement-Learning?utm_source=chatgpt.com)
[PyCaret](https://pycaret.gitbook.io/docs/)
