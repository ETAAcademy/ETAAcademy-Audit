# ETAAcademy-Audit: 31. Side-Channel Security 2

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>31 SCA</td>
    <td>
      <table>
        <tr>
          <th>audit</th>
          <th>basic</th>
          <th>article</th>
          <td>SCA</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-Audit](https://github.com/ETAAcademy/ETAAcademy-Audit)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# Fortifying the Pipeline: Modern Hardware and Software Defenses Against Side Channels

Side channel attacks exploit indirect system artifacts like timing, power consumption, and resource contention to leak sensitive information, with microarchitectural attacks being particularly potent as they leverage shared hardware components such as caches and branch predictors across logically isolated boundaries.

To mitigate these threats, defenses have evolved from historical high-assurance systems like the VAX VMM security kernel to modern hardware partitioning mechanisms like DAWG, which strictly isolates cache ways and metadata to prevent information leakage between protection domains.

However, attackers continue to refine techniques, moving from standard cache attacks like Flush+Reload and Prime+Probe to sophisticated methods targeting non-inclusive caches and coherence directories, while the advent of complex instruction pipelining and speculative execution has unleashed a new class of transient execution vulnerabilities—epitomized by Spectre and Meltdown—where attackers exploit the CPU's own performance optimizations to access secret data that leaves microarchitectural traces even after the speculative instructions are discarded.

Although defenses such as Kernel Page Table Isolation (KPTI) and Speculative Load Hardening (SLH) offer protections against specific variants, the discovery of deeper vulnerabilities like Microarchitectural Data Sampling (MDS), which leaks in-flight data from internal buffers, and the resilience of attacks like Branch History Injection (BHI) and Speculative Interference against advanced hardware defenses like InvisiSpec, highlights the necessity of establishing explicit hardware-software security contracts and formal frameworks to guarantee non-interference and close the gap between architectural assumptions and microarchitectural reality.

## 1. Fundamentals of Side Channels and Defenses

### 1.1. Introduction to Side Channels and Microarchitectural Threats

**Covert Channels** are intentional communications between two colluding parties using non-communication media. **Side Channels** are information unintentionally leaked by a system (e.g., power consumption, timing). The commonality lies in exploiting the indirect effects of the system rather than directly attacking the code. For example, acoustic side channels use microphones and machine learning techniques to "listen" to keystrokes or even screen content (detecting screen content via remote acoustic side channels). Network side channels, "website fingerprinting," identify visited websites based on packet size/frequency or search bar autocomplete times. Timing side channels guess passwords by measuring the time a system takes to reject incorrect input (e.g., if the system checks character by character and returns early on a mismatch). Side channel attacks are categorized based on what the attacker can observe: physical channels, timing channels, and microarchitectural channels. Physical channels require physical access (measuring power, electromagnetic radiation, sound); timing channels can measure response times remotely; microarchitectural channels exploit internal processor events (cache hits/misses, branch prediction) and can be executed by a co-located attacker (e.g., malware on the same cloud server).

**Microarchitectural (uArch)** introduces a threat model where the sender and receiver are isolated by the OS/hypervisor but share physical hardware (L1/L2 cache, LLC, System Bus). For example, exploiting the disk elevator scheduling algorithm (SCAN). A classic storage side-channel attack case (1977) inferring neighbor process disk access locations via request order and latency showed that even if the OS correctly implements the disk scheduling algorithm, information might still be leaked. In disk scheduling adopting the elevator algorithm (SCAN), disk requests are sorted by cylinder number and executed sequentially in one direction. An attacker sharing the same disk queue with a victim process can infer whether requests from the victim process have been inserted into the scheduling queue by submitting requests distributed across different cylinders and observing the execution order and wait time changes of their own requests, thereby leaking the approximate track location accessed by the victim process. This case revealed a fundamental security issue: resource scheduling strategies designed for performance optimization may themselves constitute side channels, even if the system has no functional vulnerabilities.

Microarchitectural (uArch) side-channel attacks exploit specific hardware implementation details (microarchitecture) of the processor, rather than vulnerabilities in code or software logic. Even if software is logically secure, hardware execution may leak confidential information. Attackers initiate contention for these shared resources. For example, an attacker can evict a victim's data by filling the cache with their own data. They then time how long it takes to access their data again; if it is slow, it means the victim has accessed that memory and evicted the attacker's cache line. The threat model includes:

- **Isolation:** Victim and attacker are logically isolated from each other via the OS or hypervisor (e.g., separate VMs or processes).
- **Shared Resources:** Crucially, they share the physical hardware that constitutes this isolation.
- **Processors:** Cores, branch predictors.
- **Caches:** L1/L2 (sometimes private, but inclusive L1 and L2), LLC (Last Level Cache) is typically shared.
- **Interconnects:** System bus.
- **Memory:** DRAM controller.

Side channels are theoretically modeled as a communication system: the **Sender** (victim) is the entity holding secret information; the **Receiver** (attacker) is the entity trying to obtain secret information; the **Channel** is the shared medium (e.g., Cache, Bus, Power line); **Bandwidth** is the rate of data leakage (e.g., LLC allows a leakage rate of about 1.2 Mbps, while GPGPU structures may leak up to 4 Mbps); **Protocols** include encoding (mapping secret information to time/space) and decoding (interpreting observations). Synchronization is key to increasing bandwidth. Mitigation strategies include:

(1) **Eliminating the Sender** (data-independent execution), ensuring program execution time and memory access patterns are independent of secret data, using techniques like "constant-time programming" (e.g., using bitwise operations instead of `if (secret) ...` branches).

(2) **Eliminating the Channel** (disjoint channels), partitioning resources physically or logically so they are not shared, such as cache partitioning (cache coloring), where the victim and attacker use different sets of cache lines.

(3) **Reducing the Channel** (adding noise), injecting random activity (fake requests, random delays) to lower the Signal-to-Noise Ratio (SNR), making it statistically difficult or impossible for the receiver to distinguish secret information from noise.

---

### 1.2. Speculative Execution and Hardware Defenses (DAWG)

Modern processors use **speculative execution** to improve performance. However, attacks like **Spectre/Meltdown** exploit this to access secret data during speculative execution and "leak" it via side channels (usually CPU cache). Attackers create contention in the shared cache (e.g., filling the cache with their data) and infer which data the victim program accessed by measuring timing differences. Existing defenses: Pure software defenses typically target specific attack variants and are easily bypassed. Existing hardware partitioning (e.g., Intel's CAT) is designed for Quality of Service (QoS), not security, and still leaks information (e.g., via cache hits or replacement metadata).

**DAWG (Dynamically Allocated Way Guard)** is a hardware defense mechanism designed to block software side-channel attacks, especially those exploiting cache timing and speculative execution (e.g., Spectre and Meltdown). DAWG is a general mechanism for securely partitioning ways of set-associative structures (e.g., L1/L2/L3 caches). It divides the "ways" of the cache into isolated protection domains, including **No Hits Across Domains**, **No Metadata Interference**, and **Strong Non-Interference**.

- **No Hits Across Domains:** Unlike Intel CAT, DAWG prevents a domain from even knowing if a line exists in another domain's partition (it forces a miss).
- **No Metadata Interference:** It partitions "replacement policy" metadata (e.g., LRU bits), thereby preventing activity in one domain from updating history bits of another domain.
- **Strong Non-Interference:** Completely plugs the leak. If an attacker cannot hit the victim's lines and cannot influence/observe the victim's replacement state, the cache timing channel is effectively closed.

DAWG's core innovation lies in achieving strong isolation by modifying the cache controller logic and utilizing two key bit-vector policies: `policy_hitmap` and `policy_fillmap`, to strictly isolate "Protection Domains".

- **Protection Domain Policy ("The Brain"):** DAWG associates every memory request with a protection domain ID (`domain_id`). The hardware uses this ID to look up two bitmasks (MSRs) to determine which "ways" of the cache the request can access.
- `policy_hitmap` (Hit Mask): Defines which ways are visible to the domain.
- `policy_fillmap` (Fill Mask): Defines which ways the domain can write to (fill) or evict from.

**Cache Hit Isolation (Closing the "Hit" Channel):** In standard caches (even in Intel CAT), if the requested address exists anywhere in the set, it is considered a "Hit". This allows an attacker to probe whether a victim has loaded a specific line. The solution involves checking and masking. When a request is received, the cache controller performs standard tag comparisons on all ways, conceptually performing a logical AND operation with `policy_hitmap`. Even if data is in Way 0 (belonging to the victim), if Way 0 is masked in the attacker's `policy_hitmap`, the attacker gets a Miss result. They cannot "see" the victim's cache line.

```code
if (Way_Tag == Request_Tag) AND (policy_hitmap[Way_Index] == 1) THEN Hit
ELSE Miss
```

**Cache Eviction/Fill Isolation (Closing the "Displacement" Channel):** Standard caches share the entire eviction set. If an attacker fills the set, they can force the eviction of the victim's data (using prime and probe attacks).

- **Restricting Allocation:** When a new line needs to be added (fill), the cache controller only considers ways allowed by `policy_fillmap`.
- **Restricting Eviction:** If a victim needs to be selected to make space, the replacement policy is forced to select a victim only from ways set in `policy_fillmap`.
  Result: Attackers can only fill/discard ways allocated to themselves. They cannot evict the victim's data because the hardware forbids touching the victim's ways during the replacement process.

**Metadata Isolation (Closing the "Replacement Logic" Channel):** Modern caches use "replacement policy metadata" (e.g., LRU bits) to decide which cache line to evict. In standard partitioning (e.g., Intel CAT), accessing a line in Way 0 might update the LRU bits for the entire cache set, thereby affecting the eviction decision for Way 7. An attacker can influence the target cache's LRU state.

- **Solution:**

  - **Partitioned Updates:** When updating metadata (e.g., setting a cache line as "most recently used"), DAWG masks the update so it doesn't affect history bits related to other domains.
  - **Partitioned Selection:** When looking for a "least recently used" target cache line, the logic is limited to decision tree branches related to `policy_fillmap`.

Result: The "history" of access patterns is completely separated. Domain A's activity does not change the likelihood of Domain B's data being evicted.

```text
Request (Addr, Domain_ID)
      |
      v
[Tag Compare] <--- MASKED BY ---> [policy_hitmap for Domain_ID]
      |
      +---> IF Match AND Allowed: RETURN DATA (Hit)
      +---> ELSE: TRIGGER FILL
                 |
                 v
           [Victim Selection] <--- MASKED BY ---> [policy_fillmap for Domain_ID]
                 |
                 +---> Select Victim ONLY from allowed ways
                 +---> Update Replacement Metadata ONLY for allowed ways

```

- **Hardware:** Only minor changes to the cache controller are needed to mask hits and partition metadata updates based on the current protection domain ID.
- **Software:** Requires OS support to manage these domains (e.g., updating MSR registers during context switches or system calls).
- **Performance:** Overhead is reasonable. Although partitioning reduces the effective cache size for each process (potentially leading to more cache misses), it provides strong security against a wide range of attacks without requiring continuous software patches.

In summary, DAWG attempts to make shared caches behave like logically independent caches for different programs, thereby eliminating the primary method attackers use to "read" leaked secrets from hardware.

---

### 1.3. Historical Context: The VAX VMM Security Kernel

**VAX VMM (Virtual-Machine Monitor) Security Kernel** was a research project by Digital Equipment Corporation (DEC) in the late 1980s, aiming to build a production-grade security kernel capable of achieving the US National Computer Security Center (NCSC) A1 rating. It had 5 main goals:

- **A1 Security:** Compliance with the highest security requirements for verification and configuration management.
- **Hardware Compatibility:** Run on commercial VAX hardware (specifically VAX 8500 and 8800 series), using microcode extensions rather than custom hardware.
- **Software Compatibility:** Support unmodified applications from VMS and ULTRIX-32 operating systems.
- **High Performance:** Achieve "acceptable" performance levels (typically 30% to 90% of VMS native performance).
- **Commercial Standards:** Provide management tools and documentation expected of professional software products.

Since the standard VAX architecture was not originally designed for virtualization, the team implemented several key innovations:

- **Ring Compression:** VAX has four protection rings (User, Supervisor, Executive, Kernel). To protect the VMM (which must run in real kernel mode), the VMM "compressed" the VM's executive and kernel modes into the real executive mode.
- **I/O Emulation (KCALL):** Instead of simulating complex I/O registers (CSRs), the kernel used a specialized "kernel call" mechanism. This reduced the number of expensive "traps" (context switches) to the security kernel, significantly improving performance.
- **Layered Design:** The kernel was built as 15 strict abstraction layers (e.g., hardware interrupt handlers, scheduler, virtual VAX layer) to minimize complexity and facilitate formal verification.

Security mechanisms included:

- **Access Control:** Enforced Mandatory Access Control (MAC) using the Bell-LaPadula model (confidentiality) and Biba model (integrity), and Discretionary Access Control (DAC) via Access Control Lists (ACLs).
- **Trusted Path:** Users interacted with a "Security Server" via a dedicated Secure Attention Key (BREAK key), ensuring untrusted VMs could not spoof the login process or administrative commands.
- **Formal Verification:** The system's Top-Level Specification (TLS) and Formal Top-Level Specification (FTLS) were written in Ina Jo and verified using the Formal Development Methodology (FDM).

## 2. Cache-Based Side-Channel Attacks

### 2.1. Understanding Cache Leakage and Attack Primitives

Cache mechanisms can leak sensitive information, such as encryption keys from secure libraries (e.g., RSA and AES). Why are caches targets?

- **Large Attack Surface:** Caches are shared across CPU cores and sockets.
- **High Speed:** They allow for high-bandwidth covert channels.
- **Fine Granularity:** Many states allow spatial encoding of secret information, enabling high precision.
- **Monitoring Cache Line Level Access Patterns:** By observing which parts of memory the victim accesses, attackers can infer internal states or keys.

Key attack strategies include:

- **Flush+Reload:** The attacker explicitly flushes a shared cache line using instructions like `clflush` (x86) or `DC CIVAC` (ARM). After waiting for a period, the attacker reloads the data. A "low latency" reload indicates the victim accessed (and thus recached) the data. A "high latency" reload means the victim did not access the data. Requires shared memory between attacker and victim (e.g., shared libraries).
- **Evict+Reload:** Used when specific flush instructions are unavailable (e.g., on many ARM processors or Apple devices). Instead of using a flush instruction, the attacker accesses a large buffer to force the target shared line to be evicted from the cache.
- **Prime+Probe:**
  - **Prime:** The attacker fills a specific cache set with their own data ("balls in a bucket").
  - **Wait:** The victim runs a program and potentially accesses memory mapped to the same cache set.
  - **Probe:** The attacker measures the time to access their data again. High latency means the victim has evicted the attacker's data, exposing the victim's access pattern. Effective even without shared memory, relying instead on cache set contention.

Technical enablers include:

- **Shared Memory:** Often exploits how the OS shares memory pages (e.g., standard libraries) between untrusted processes to save memory.
- **VIPT Caches:** Virtually Indexed, Physically Tagged L1 caches allow simultaneous lookup without waiting for TLB translation. Attackers must traverse the TLB (often using huge pages) to determine which cache sets correspond to specific virtual addresses.

---

### 2.2. Deep Dive into FLUSH+RELOAD

**FLUSH+RELOAD** does not breach memory access permissions but exploits shared physical pages and the observability of cache state. By precisely measuring access latency, it infers the victim's access patterns to code or data in shared memory, thereby indirectly recovering sensitive information like keys. It is a pure side-channel attack leaking "behavior," not "data itself."
FLUSH+RELOAD is a sophisticated cache side-channel attack targeting the Last Level Cache (LLC), typically L3 on modern Intel processors. It exploits two key features of modern systems:

- **Memory Page Sharing:** OSs and hypervisors often share the same memory pages between processes (e.g., shared libraries or memory deduplication) to reduce memory footprint.
- **Inclusive Caches:** In Intel architecture, the LLC is inclusive, meaning it contains copies of all data stored in lower cache levels (L1 and L2).

The FLUSH+RELOAD attack has three distinct phases:

(1) **FLUSH:** The attacker uses the x86 `clflush` instruction to remove a specific memory line (from a shared page) from the entire cache hierarchy (L1, L2, and L3).
(2) **WAIT:** The attacker waits a short period, allowing the victim process to execute.
(3) **RELOAD:** The attacker reloads the same memory line and measures the access time using a high-resolution timer (e.g., `rdtsc`). - **Fast access:** Indicates the victim accessed the memory line during the wait phase (it was reloaded into the cache). - **Slow access:** Indicates the victim did not access the memory line (it had to be fetched from main memory).

FLUSH+RELOAD features high resolution, cross-core and cross-VM capability, and low noise. Unlike many previous attacks targeting cache sets, FLUSH+RELOAD targets specific memory lines, enabling attackers to distinguish accesses to individual functions or data structures. Since it targets the LLC (shared by all cores), the attacker and victim do not need to share an execution core. If page deduplication is enabled, it can even work across VM boundaries. Compared to techniques like PRIME+PROBE, it offers higher fidelity and fewer false positives. For example, by monitoring the square, multiply, and modulo operation functions in the "square-and-multiply" exponentiation algorithm, FLUSH+RELOAD can recover over 96% of key bits in a single decryption process.

---

### 2.3. Attacks on Non-Inclusive Caches and Directories

Cache-based side-channel attacks (e.g., Prime+Probe or Flush+Reload) traditionally rely on **inclusive cache hierarchies**. In inclusive caches (common in older Intel CPUs), the LLC contains copies of all data in private L1/L2 caches. This allows an attacker to evict data from a victim's private cache simply by filling a set in the shared LLC. However, modern processors (e.g., Intel Skylake-X and AMD chips) are moving towards **non-inclusive or exclusive cache hierarchies** to save space and power as core counts increase. In these systems, the shared LLC does not strictly enforce containment of private cache lines. This change breaks many existing cross-core attacks because attackers lose the ability to easily manipulate the contents of the victim's private cache.
Switching to a non-inclusive cache hierarchy does not automatically eliminate side-channel threats. By shifting the target from cache storage to the **coherence directory**, hardware resource contention remains a viable avenue for sensitive information leakage in the cloud.

Although the data cache itself may not be inclusive, the **Directory**—the hardware structure responsible for tracking cache coherence (i.e., which core owns what data)—must remain inclusive to function correctly. By attacking the directory structure instead of the cache lines themselves, powerful side-channel attacks can be re-enabled on modern hardware.

- **Skylake-X Reverse Engineering:** Researchers successfully reverse-engineered the directory structure of undisclosed Intel Skylake-X processors, revealing it is sliced and uses specific hash functions.
- **New Eviction Set Algorithms:** Used to create "eviction sets" (groups of addresses mapping to the same set) specifically targeting non-inclusive caches, overcoming the unpredictability of L2/LLC replacement policies.
- **New Attack Primitives:**
  - **Directory-based Prime+Probe:** The first cross-core Prime+Probe attack targeting non-inclusive caches, requiring no shared memory between victim and attacker.
  - **Multithreaded Evict+Reload:** A high-bandwidth attack utilizing helper threads to bypass complex replacement policies of non-inclusive caches typically used to protect shared data.
- **Proof of Concept:** They demonstrated these attacks by stealing key bits from an RSA implementation (GnuPG) running on a state-of-the-art server.

---

## 3. Transient and Speculative Execution Vulnerabilities

### 3.1. Complexity in Processor Pipelining

When processors are no longer limited to simple single-cycle integer operations but must handle high-performance scenarios, **Instruction Pipelining** becomes "complex." These scenarios include multi-cycle operations (e.g., Floating Point Units or long-latency operations like division), variable latency operations (especially memory systems with unpredictable access times, e.g., cache misses), and functional unit replication (multiple units, e.g., two multipliers or multiple memory units) running in parallel.
**CDC 6600 (1963)**, designed by Seymour Cray, was the first machine to successfully implement these concepts. It featured ten independent parallel functional units, a load/store architecture (arithmetic operations strictly register-to-register), and dynamic scheduling via a scoreboard, allowing hardware to manage instruction execution based on resource availability.

Complex pipelines introduce several control problems not found in simple pipelines:

- **Structural Hazards:** Occur if functional units are not fully pipelined, or multiple units try to write results to the register file simultaneously (write-back stage conflict).
- **Data Hazards:** Include RAW (Read-After-Write), WAR (Write-After-Read), and WAW (Write-After-Write).
  - **RAW:** An instruction tries to read a register before a previous instruction has finished writing to it.
  - **WAR:** An instruction tries to write to a register before a previous instruction has finished reading from it.
  - **WAW:** An instruction tries to write to a register before a previous instruction has finished writing to it, potentially overwriting the correct final value.
- **Out-of-Order Execution:** Different latencies mean later-issued instructions may complete before earlier-issued ones, making exception handling difficult.

**Hardware Solution: Scoreboard.** To manage these hazards, complex pipelines use a scoreboard. This hardware data structure tracks:

- **Functional Unit Status:** Which units are busy and what operations they are performing.
- **Register Status (Write Pending):** Which registers are waiting for results.
- **Safety Checks:** Before "issuing" an instruction, the scoreboard checks if the unit is free, if there are pending writes (source registers, RAW), and if there are pending writes (destination registers, WAW).

Three stages of pipeline complexity:

(1) **Complex In-Order Pipeline:** Instructions are issued in order but may complete out of order. Uses bypassing and stalling to manage hazards.

(2) **Superscalar In-Order Pipeline:** Fetches and issues multiple instructions per cycle (e.g., one integer and one float instruction).

(3) **Out-of-Order Execution (Preview):** The foundation of modern cores. Instructions execute as soon as data is ready, regardless of their original program order.

---

### 3.2. Introduction to Transient Execution Attacks

**Transient Execution Attacks** exploit performance optimization mechanisms in modern processors—specifically Out-of-Order (OoO) execution and Speculative Execution—to leak sensitive data. These attacks allow attackers to read data they shouldn't access (e.g., kernel memory or other processes' data) by observing the side effects of instructions speculatively executed by the CPU. These instructions are eventually discarded (suppressed) because they shouldn't have been executed in the first place.

To understand these attacks, let's look at how modern CPUs maximize speed:

- **Out-of-Order Execution:** Instructions execute as soon as inputs are ready, rather than strictly in program order. This fills idle time in the CPU pipeline.
- **Speculation:** The CPU guesses the direction of branches (e.g., `if` statements). It tentatively executes instructions on that path. If the guess is wrong, execution results are discarded (rolled back).
- **Side Effects:** While architectural state (registers, memory) is rolled back after a misprediction, **microarchitectural state** (specifically cache) is often not. This "residual" state in the cache is what attackers exploit.

**Spectre and Meltdown** are severe processor security vulnerabilities that exploit the speculative execution capabilities of modern high-performance CPUs to steal protected information via side channels.

**Main Variants:**

- **Spectre-v1 (Bounds Check Bypass):** Exploits conditional branch vulnerabilities. It tricks the CPU into performing a speculative read beyond array bounds, allowing access to private memory within the same address space (e.g., browser sandboxes).
- **Spectre-v2 (Branch Target Injection):** Targets indirect jumps and returns. The attacker "trains" the Branch Target Buffer (BTB) to incorrectly predict a jump to a "gadget" (a useful existing code snippet), thereby leaking confidential information.
- **Spectre-v3 (Meltdown / Rogue Data Cache Load):** Exploits delayed handling of permission checks during speculation. It allows user-level applications to read restricted kernel memory because the CPU executes the load before the "access denied" exception is fully handled.

**Meltdown** breaks the isolation between user applications and the OS. It combines hardware optimization (OoO execution) and software optimization (mapping kernel memory into user space for faster system calls).
**Attack Process:** The attacker attempts to read a kernel memory address. The CPU speculatively executes this load before checking if the user has read permission. The read secret value is used to access a specific index in a user-controlled array (`probe_array`). The CPU eventually realizes the permission violation and throws an error (fault), discarding the instruction. However, the access to `probe_array` brings a specific line of code into the CPU cache. The attacker uses side-channel techniques (Flush+Reload) to check which line was cached, thereby leaking the secret value.

**Mitigation:** KPTI (Kernel Page Table Isolation), which unmaps kernel memory when running user code.

**Spectre** tricks the processor into executing instructions along the wrong path.

- **Variant 1 (Bounds Check Bypass):** `if (x < array_size) { ... }`. The attacker trains the branch predictor to expect the condition to be true. Then they pass an `x` value exceeding the array size. The CPU speculatively executes the body, accessing out-of-bounds secret memory before realizing the error.
- **Variant 2 (Branch Target Injection):** Exploits the Branch Target Buffer (BTB). Attackers train the CPU to mispredict the path of an indirect jump, redirecting speculative execution to an information-leaking "gadget."
  Unlike Meltdown, which is often considered a design flaw (delayed permission check), Spectre exploits the fundamental (and correct) behavior of speculative execution, making it harder to fix completely.

Both attacks follow a general pattern involving **Transient Execution** (executing instructions that are subsequently discarded):

(1) **Access:** Speculatively access a secret value.

(2) **Transmit:** Use that secret value to modify the state of a microarchitectural element (e.g., cache) via a side channel.

(3) **Receive:** Measure the side channel (e.g., access time) to recover the key.

**Mitigation Strategies:**

- **Software:** "Page Table Isolation" (PTI) for Meltdown, inserting serializing instructions (e.g., `lfence`), or using "retpolines" to prevent branch target injection.
- **Microcode/Firmware:** Updates to CPU behavior to limit speculation or flush predictors (BTB/RSB) when switching security contexts.
- **Hardware:** Processors (e.g., Intel Cascade Lake) include hardware-level fixes for safer permission handling and predictor isolation.

---

### 3.3. Meltdown and the KPTI Defense

**Meltdown** breaks the fundamental memory isolation between user applications and the OS kernel. The attack exploits Out-of-Order execution (a performance optimization in modern CPUs) to read memory that should be inaccessible (e.g., kernel data, passwords, private keys). It affects major Intel microarchitectures (since 2010) and potentially others, impacting Linux, Windows, Android, and containers (Docker/LXC). Attackers can dump the machine's entire physical memory, including other processes, the kernel, and even other VMs in cloud environments, achieving read speeds of 3.2 KB/s to 503 KB/s.

**Mitigations:** Hardware fixes and the software-based **KAISER** patch. Researchers found that KAISER has an "important (but unintended) side effect" of preventing the spread of the Meltdown vulnerability. Since Meltdown relies on the kernel being mapped into the user space address space to perform its "transient" memory lookups, KAISER effectively blocks this attack. If kernel memory is not mapped, the hardware cannot even begin to load secret data out-of-order because the virtual address cannot be resolved.

**KAISER (Kernel Address Isolation to have Side-channels Efficiently Removed)** is a software-based defense mechanism originally designed to protect KASLR (Kernel Address Space Layout Randomization) from side-channel attacks. KAISER's core principle is to enhance memory isolation between the kernel and user processes. Traditionally, the OS maps the kernel into every user process's address space to improve translation efficiency (e.g., for system calls or interrupts). KAISER changes this by removing kernel mappings and using minimal mappings.

- **Remove Kernel Mappings:** Ensures kernel memory is not mapped in user space page tables at all.
- **Minimal Mappings:** Only the absolute minimum kernel code required for context switching (e.g., interrupt handlers) remains in user space.

Due to KAISER's effectiveness against Meltdown, its variants were quickly adopted by all major OSs: Linux integrated it as **KPTI (Kernel Page Table Isolation)**, Windows implemented it as **KVA Shadow**, and macOS as **Double Map**.

---

### 3.4. EntryBleed: Breaking KPTI

**"EntryBleed"** exploits the prefetch function to break KPTI (or its original name KAISER). This vulnerability reveals a significant implementation flaw in Linux KPTI. Although KPTI was designed to isolate kernel memory to prevent side-channel attacks (like Meltdown), unprivileged local attackers can still bypass KASLR on Intel-based systems.

KPTI is supposed to map only the minimal amount of kernel memory into user space (e.g., syscall entry handlers). However, researchers found that `entry_SYSCALL_64` is mapped at the same address in user space and kernel space, and its location relative to the kernel base lacks sufficient randomness. The side-channel attack utilizes a **prefetch side-channel**. By timing the execution of `prefetch` instructions, attackers can determine if an address is cached in the TLB (Translation Lookaside Buffer).

- **Basic Principle:** The attacker ensures `entry_SYSCALL_64` is in the TLB by repeatedly executing system calls. Then, they use prefetch timing to "scan" potential KASLR ranges. Faster execution reveals the exact location of the syscall handler, thereby leaking the KASLR base address.

The exploit is very concise (less than 100 lines of C code) and runs in seconds. It works well under normal system load and applies to various Linux distributions (Arch, Ubuntu, Manjaro, Debian) and Intel CPU generations. While KASLR bypass is technically information disclosure, it undermines a fundamental security layer, making other kernel vulnerabilities easier to exploit. Due to this specific mapping oversight, KPTI "failed" in one of its primary goals—acting as a barrier against KASLR bypass via CPU side channels.

---

### 3.5. Speculative Load Hardening (SLH)

**Speculative Load Hardening (SLH)** is a comprehensive software-level mitigation technique designed to defend against Spectre Variant #1 (Bounds Check Bypass). Unlike many mitigations relying on manual code audits or "isolation," SLH is intended to be applied automatically by the compiler to the entire program.

**Spectre Variant #1:** In modern CPUs, the branch predictor might guess an `if` condition is true and speculatively execute its body. If an attacker provides an "out-of-bounds" index, the CPU might speculatively load secret data from memory before realizing the branch misprediction. Even though the CPU eventually discards these instructions, the secret data often leaves traces in the CPU cache that attackers can measure.

SLH's high-level goal is to ensure that even if a load operation is executed on a mispredicted path, it will not access or leak secret data. It achieves this via **Branchless Logic**:

- **Predicate State Tracking:** The compiler introduces a "predicate state" (typically a register like `%rax` on x86) to track if the current execution path is valid.
  - **Normal State:** All bits are 0 (indicating "correct" path).
  - **Poisoned State:** All bits are 1 (indicating "mispredicted" path).
- **Branchless Updates:** After every conditional branch, the code uses instructions like `CMOV` (Conditional Move) to update the predicate state. Since `CMOV` is not predicted by the CPU, it acts as a reliable source of the actual execution state.
- **Hardened Loads:** Before a memory load occurs, the predicate state is applied to the address or the loaded value.
  - **Address Hardening:** Bitwise OR the pointer with the predicate state. If the predicate state is poisoned (all 1s), the pointer becomes an invalid/huge address, causing the load to fail or access non-sensitive memory.
  - **Value Hardening:** The load result is masked (e.g., `value &= ~predicate_state`). If the predicate state is poisoned, the value becomes zero, ensuring the attacker sees nothing.

**Automatic and Scalable:** No need for programmers to manually identify "risky" loads; the compiler hardens every load in the application.

**Inter-procedural Protection:** The predicate state can be passed between functions (e.g., by embedding it in the high bits of the stack pointer), preventing data leakage across function calls.

**Performance:** While there is a performance hit (typically 10% to 50%), it is much faster (approx. 1.8x to 2.5x) than using `LFENCE` instructions, which completely stall the CPU execution pipeline.

**Recompilation:** Only software compiled with SLH is protected.

**Address Space Requirements:** For full security, the OS must ensure "poisoned" addresses (e.g., lower 2GB of memory) do not contain any secret data.

**Does Not Prevent Speculation:** Unlike `LFENCE`, SLH allows the CPU to continue speculating; it just ensures speculation is "blind" and leaks no secrets.

**Advanced Attacks Beyond `if` Statements:** Moving from simple conditional branches to more complex and dangerous speculative execution vulnerabilities. Extending SLH to defend against attacks on memory stores and indirect control flow, including:

- **Variants 1.1 and 1.2:** Mitigating speculative "Stack Overflow" and Virtual Function Table/Jump Table Poisoning.
- **Indirect Branches:** SLH can work with Retpolines as an alternative or supplement.
  **SLH vs. Standard LFENCE:** Empirical data shows that while SLH is not "free" (typically 10-30% slowdown), it is about 2x faster than LFENCE-based methods, making it a more viable choice for production software.

**Variant 1.1: Speculative "Stack Smash"** attacks involve "Bounds Check Bypass Store." Attackers exploit bounds checks to speculatively execute memory write operations (e.g., `memcpy`). They speculatively overwrite the return address on the stack with a malicious pointer. When the function speculatively "returns," the CPU jumps to the attacker's gadget.

- **SLH Fix:** SLH mitigates this by hardening the load of the return address. Specifically, since SLH poisons the stack pointer (`%rsp`) during misprediction, the CPU cannot correctly read the (forged) return address from the stack, causing the speculative attack to fail.

**Variant 1.2: Virtual Function Table and Jump Table Poisoning** is another variant of speculative store attacks. Instead of attacking the stack directly, attackers speculatively overwrite pointers in the virtual function table (used for C++ virtual methods) or jump table (used for switch statements). Subsequent calls to virtual methods speculatively use the attacker's malicious address.

- **SLH Fix:** Ensures any operation loading from a vtable or jump table is hardened. If the CPU is in a mispredicted state, the loaded address is masked/poisoned, preventing the attacker's intended "type confusion" or redirection.

**Indirect Branches and Backtracking:** Dealing with indirect calls, jumps, and returns, which are typical targets for Spectre Variant #2. Indirect branches are harder to track because they don't have simple true/false flags; one must verify if the CPU jumped to the expected target.

- **SLH and Backtracking:** Backtracking is a common defense that completely stops speculation on indirect branches. SLH can serve as an alternative or "safety net." It allows speculation to proceed but uses data flow tracking to ensure no secret information is loaded even if the branch is mispredicted.
- **Strategy (x86):** A hybrid approach is recommended: use Retpolines for indirect calls/jumps (they are very effective) and SLH to protect returns and other complex paths where Retpolines might be too slow or unnecessary on new hardware.

By extending SLH to cover stores, stack returns, and indirect branches, we move from protecting individual "risky" code snippets to providing a systematic, complete program defense without relying on developers to spot every potential vulnerability.

---

### 3.6. Microarchitectural Data Sampling (MDS)

**Microarchitectural Data Sampling (MDS)** is a class of speculative execution vulnerabilities in Intel CPUs that allows attackers to leak sensitive "in-flight" data. Unlike attacks like Meltdown or Spectre targeting CPU caches, MDS exploits vulnerabilities deep within the CPU's internal buffers.

**Key Characteristics of MDS:**

- **Target Buffers:** Can leak data from CPU internal structures previously thought inaccessible, including:
  - **Line Fill Buffers (LFB):** Used for moving data between cache levels.
  - **Load Ports:** Used for loading data from memory to registers.
  - **Store Buffers:** Used whenever the CPU needs to store data.
- **In-Flight Data:** Can capture data being processed by the pipeline, even if it is never stored in the CPU cache.
- **Security Bypass:** MDS can leak data across almost any security boundary, including between applications, from OS kernel to user space, between VMs in cloud environments, and out of secure enclaves like Intel SGX.

**Major MDS Attacks include:**

- **RIDL (Rogue In-Flight Data Load):** Proven that attackers can use unprivileged code (including JavaScript in web browsers) to leak data from Line Fill Buffers and Load Ports.
- **Fallout:** Specifically targets Store Buffers, allowing attackers to leak sensitive data written by the kernel and bypass Kernel Address Space Layout Randomization (KASLR).
- **ZombieLoad:** A related MDS variant (mentioned in broader research) that also samples microarchitectural data.

MDS research shows that hardware vulnerabilities are systemic rather than one-off bugs. Since these attacks target the fundamental way CPUs process data, they are often immune to pure software defenses designed for cache-based attacks, sometimes requiring complex microcode updates or hardware changes to mitigate.

---

## 4. Hardware-Software Contracts and Advanced Mitigations

### 4.1. Redefining the ISA for Security

**"Software-Hardware Contract"** traditionally refers to the Instruction Set Architecture (ISA). Ideally, it acts as an abstraction layer. Software developers write code assuming a functional behavior (Input A to Output B), and hardware designers build processors to efficiently execute that behavior. The problem is that the traditional contract only guarantees **functional correctness**. It does not cover non-functional properties like timing, power consumption, or cache state.

**Why the Contract "Fails":** Modern hardware includes optimizations (caches, branch prediction, speculative execution) invisible to the ISA but affecting physical side channels. This creates a disconnect: software developers assume the program is secure if the logic is correct. They don't understand specific hardware details (e.g., how a specific cache replacement policy works). Hardware designers optimize speed for "average" programs, ignoring the specific security requirements of cryptographic code. Several vulnerabilities result from software relying on a simple understanding of hardware:

- **Timing Vulnerabilities:** Password checks returning "false" immediately upon a mismatch. Execution time leaks the length of the matching prefix.
- **RSA Cache Vulnerabilities:** Secret-dependent memory access patterns (e.g., `if (bit == 1) multiply`) affect cache state, allowing attackers to infer the key.
- **Meltdown/Spectre:** Hardware speculatively executes instructions (guessing paths). Even if the guess is wrong and rolled back, side effects (loading data into cache) remain and can be measured by attackers.

To solve this, we need a security property called **"Non-Interference."** Changes in high-level inputs (e.g., secret data like passwords) should not cause observable differences in low-level outputs (public behavior, including timing).
**Formal Check:** For all $M_1$ and $M_2$ (memories with same public data but different secret data), execution traces $O_1$ and $O_2$ (observations) must be identical.

**Solutions:**

- **Constant-Time Programming:** Since we cannot easily change all hardware, software must be written to be data-independent or constant-time.
  - **No Secret-Dependent Branches:** Do not use `if (secret)`. Instead, use bitwise operations or `CMOV` instructions so the control flow graph is independent of secret data.
  - **No Secret-Dependent Memory Accesses:** Accessing `array[secret]` leaks secret data via cache timing.
- **Hardware Mitigations:** New ISA extensions, like ARM DIT (Data Independent Timing) and Intel DOIT, aim to make the timing contract explicit, guaranteeing that execution time for certain instructions is always the same regardless of operand values.

The **"Software-Hardware Contract" for security** aims to move from implicit assumptions (leading to attacks like Spectre) to explicit guarantees, where hardware explicitly states what information might leak (e.g., "speculative execution might access cache"), and software uses specific patterns (fences, retpolines, constant-time logic) to ensure confidentiality remains secure under these conditions.

---

### 4.2. Formal Frameworks for Hardware-Software Contracts

The **"Hardware-Software Contracts" formal framework** ensures the safety of speculative execution in processors. Despite many hardware defenses against Spectre-style attacks, software developers lack a clear, formal way to know the actual security guarantees provided by specific hardware components.

The contract framework is used to formally define CPU security guarantees. These contracts explicitly specify what information an attacker can observe from microarchitectural state (e.g., cache).

- **Hardware Analysis:** Utilizing the framework to formallyize and compare a set of representative hardware defenses (e.g., delaying speculative loads, speculative taint tracking), building a clear hierarchy of security strength.
- **Software Co-design:** Defines precise and verifiable properties for software in two common security scenarios:
  - **Constant-Time Programming:** Ensures benign code (e.g., crypto functions) does not leak the secrets it processes.
  - **Sandboxing:** Ensures potentially malicious code (e.g., Web scripts) cannot read memory outside its designated sandbox.
- **Static Analysis Tool:** Extensions to **Spectector**, enabling it to automatically check if a program is secure under a given hardware contract.

**Formal Language:** Used to precisely define "what leaks" on a specific processor using labeled program execution traces. A "Contract" is essentially a promise: if two program executions produce the same trace (the contract), an attacker cannot distinguish between them even by looking at hardware side channels (cache, branch predictors).
The framework is based on two customizable dimensions:

(1) **Observer Mode (What is observed):**

    - `ct` (Constant Time): Exposes Program Counter (PC) and accessed memory addresses. This is the standard model for cryptographic security.
    - `arch` (Architecture): Exposes everything in `ct`, plus actual data values loaded from memory. This sounds insecure, but effectively models systems that only protect transient (speculative) data while allowing normal data flow.

(2) **Execution Mode (When it occurs):**

    - `seq` (Sequential): What happens during normal, correct execution (no speculation).
    - `spec` (Speculative): What happens during mispredicted paths (branch predictor guesses wrong).

- **`seq-ct`:** The "Gold Standard" for crypto. No leakage during normal execution. (Traditional hardware relies on this assumption).
- **`spec-ct`:** Leaks memory access patterns even on mispredicted paths. (Vulnerable to Spectre).
- **`seq-arch`:** Leaks values of committed instructions but hides values on mispredicted paths. (Describes some modern hardware defenses).

A formal model of a simple Out-of-Order (OoO) processor with cache and branch predictor was used to test real hardware defenses. Defenses were categorized into a hierarchy based on the contracts they satisfy:

- `seq` (Disable Speculation): The strongest but slowest defense. Satisfies the `seq-ct` contract.
- `loadDelay` (Fence/Delay Load): Waits for branch resolution before loading data. Effectively blocks data leakage but might still leak control flow info (e.g., guessed branches) via instruction cache. Satisfies the `seq-spec-ct-pc` contract.
- `tt` (Taint Tracking): Simulates advanced defenses like STT (Speculative Taint Tracking) and NDA. These track "tainted" data (loaded during speculation) and block its use in "transmit" instructions (e.g., loads or branches). These satisfy the `seq-arch` contract. Ideally suited for sandboxing (stopping scripts from stealing data) but may be bad for constant-time programming (crypto) as they might allow non-speculative secrets to subtly affect execution time.

**Software Co-design:** Bridges the gap between high-level code and hardware reality. Introduced a method to prove program security by combining two checks: "Vanilla" property (standard security check) + "Bridging" property (speculative check).

- **For Sandboxing:** Goal is to prevent malicious scripts from reading memory outside their bounds.
  - **Check Vanilla Sandbox:** Does the code respect bounds in a perfect, non-speculative world? (Standard compilers do this).
  - **Check wSNI (Weak Speculative Non-Interference):** Does the code leak secrets during speculation?
    If both are true, the program is generally sandboxed on the hardware.
- **For Constant-Time (Crypto):** Prevent benign crypto functions from leaking key bits via timing side channels.
  - **Check Standard Constant-Time:** Does the code avoid secret-dependent branches/memory accesses? In an ideal world?
  - **Check Speculative Non-Interference (SNI):** Can it still avoid these accesses even if the CPU mispredicts branches?
    If both are true, the program is generally constant-time.

**Spectector** is a static analysis tool that finds leaks by symbolically executing code. It was modified to check specific bridging properties (wSNI and SNI) and compare them against different contracts defined in the framework. Various Spectre v1 variants (classic bounds check bypass) compiled for x86 were tested.

**Results:** The tool successfully identified that standard Spectre v1 is insecure on standard hardware (`spec-ct`) but secure on hardware with Taint Tracking (`seq-arch`). Some variants (e.g., leakage via control flow) might still be insecure even with Taint Tracking enabled. The emergence of hardware proves the necessity of signing precise contracts.

---

### 4.3. Branch History Injection (BHI)

**Branch History Injection (BHI)** shifts from software mitigations (e.g., retpoline) to hardware mitigations (e.g., Intel eIBRS and Arm CSV2). Hardware isolation of the Branch Target Buffer (BTB) across privilege domains (e.g., user vs. kernel) is sufficient to stop attacks. Although targets are isolated, the **Branch History Buffer (BHB)**—used to track the history of executed branches to aid prediction—is often shared across privilege boundaries.

**Branch History Injection (BHI)**, also known as Spectre-BHB, allows unprivileged attackers to manipulate shared branch history state. Even if they cannot directly inject branch targets, they can "poison" the history, tricking the hardware predictor into mispredicting branches in a higher-privileged context (e.g., OS kernel). This re-enables cross-privilege attacks, allowing user-space attackers to speculatively execute gadgets in the kernel and leak sensitive data.

Modern Intel and Arm processors with eIBRS and CSV2 deployed are vulnerable to BHI.

- **End-to-End Exploit:** On Intel systems (with eIBRS enabled), the exploit leaks kernel memory at a rate of 160 bytes/sec.
- **Intra-Mode Attack:** Further demonstrated that even if history data is fully isolated, "intra-mode" (same privilege level) attacks are still viable, specifically using unprivileged eBPF (Extended Berkeley Packet Filter) to confuse the kernel.

Current hardware isolation strategies are insufficient for this issue.
**Main Recommendation:** Revert to software defenses. Even on modern CPUs, **retpolines** should be re-enabled as they remain the only fully viable mitigation. Disable unprivileged eBPF to limit access to features that allow attackers to easily generate gadgets and measure timing in the kernel.

---

### 4.4. InvisiSpec: Making Speculation Invisible

**Speculative Execution Attacks** (e.g., Spectre and Meltdown) exploit "traces" left in the CPU cache by speculative instructions (instructions executed before the processor knows they are needed) to leak sensitive information via side channels.
**InvisiSpec** is a novel hardware defense mechanism. Its core idea is to load "unsafe" speculative load operations into a temporary **Speculative Buffer (SB)** instead of the main cache hierarchy. This prevents these load operations from modifying cache state (occupancy, replacement bits, coherence state) until they are confirmed to be safe (visible), effectively closing the side channel.

InvisiSpec is a hardware defense strategy designed to mitigate speculative execution attacks in multiprocessors.

- **Invisible Speculation:** Unsafe speculative loads read data into a local Speculative Buffer (SB) instead of the standard cache hierarchy. This prevents speculative operations from leaving traces in the cache that other threads can observe.
- **Points of Visibility:** Once a speculative load is determined to be "safe" (e.g., after branch prediction is resolved or the instruction reaches the head of the Reorder Buffer), InvisiSpec makes it visible to the rest of the system.
- **Validation vs. Exposure:** To ensure memory consistency, the system either **validates** the data (re-reads and compares to ensure data hasn't changed) or **exposes** it (loads it directly into the cache), depending on security conditions and the memory model (e.g., TSO).

InvisiSpec is significantly more efficient than software-based mitigations like inserting fences. For example, under the TSO memory model, simulation results show that InvisiSpec reduces the performance overhead of preventing Spectre attacks from 74% (using fences) to just 21%.

**Two Variants:**

- **InvisiSpec-Spectre:** Targets branch misprediction attacks.
- **InvisiSpec-Future:** A broader defense targeting potential future attacks involving any speculative load.

**"Futuristic" Attack Model:** Extends speculative attacks from branch mispredictions (Spectre) to include any speculative load. InvisiSpec uses microarchitectural "Validation" (checking data consistency) and "Exposure" (committing data to cache) mechanisms to maintain memory consistency models like TSO (Total Store Order). Evaluated using SPEC and PARSEC benchmarks, the design showed significantly lower performance overhead (21% drop for Spectre defense) compared to traditional software fence-based mitigations (74% drop).

---

### 4.5. Speculative Interference

**Speculative Interference** is a class of side-channel attacks that exploits a subtle interaction in modern processors: mispredicted (newer) instructions can affect the execution time of older instructions that are about to retire. These attacks aim to break "invisible speculation" mechanisms (e.g., InvisiSpec, Delay-on-Miss, etc.). Invisible speculation mechanisms were created to stop Spectre-style attacks by preventing speculative loads from modifying cache state until they are confirmed correct (non-speculative). However, Speculative Interference proves these defenses are insufficient.

The attack converts transient timing differences into persistent cache state changes via three steps:

(1) **Resource Contention (Interference):** The attacker uses a "gadget"—a sequence of code executed during misprediction (e.g., inside a branch that will eventually be squashed). This gadget accesses a secret value and uses it to cause contention for shared hardware resources (e.g., execution ports, MSHRs, or Reservation Stations).

(2) **Time Shift:** This contention delays the execution of an earlier, non-speculative instruction ("interference target"), such as a legitimate memory load instruction about to retire. Even though the speculative instructions are eventually discarded, the delay they cause to the legitimate instruction effectively "travels back" in time.

(3) **Persistent Cache Trace:** By altering the issue time of the legitimate load instruction, the attacker changes the order in which that load instruction accesses the cache relative to other memory operations. Since cache replacement policies (e.g., LRU) rely on access order, this reordering leaves a permanent and observable trace in the cache state, which the attacker can then read to infer the secret information.

**Fundamental Discovery:** Timing variations can be translated into persistent state changes. While invisible speculation mechanisms successfully prevent speculative loads from directly writing to the cache, they often allow speculative instructions to execute and use internal resources to maintain performance. Speculative Interference exploits this resource usage to leak secrets indirectly via the cache.
