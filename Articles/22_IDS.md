# ETAAcademy-Audit: 22. Intrusion Detection Systems (IDS)

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>22 IDS</td>
    <td>
      <table>
        <tr>
          <th>audit</th>
          <th>basic</th>
          <th>article</th>
          <td>IDS</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-Audit](https://github.com/ETAAcademy/ETAAcademy-Audit)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# From Signature-Based IDS to AI Approaches: Challenges and Opportunities

Intrusion Detection Systems (IDS) remain a cornerstone of modern cybersecurity. What started with signature-based rules, anomaly checks, and protocol analysis has now shifted toward AI-powered detection, built to handle zero-day exploits, APTs, ransomware, and threats across cloud and IoT environments.

Today, machine learning and deep learning models—such as SVMs, DNNs, RNNs, LSTMs, GRUs, Transformers and LLMs—are widely used to spot patterns in network traffic and system logs. These models help identify classic attack types like DoS, probing, R2L, and U2R. CNNs, SDN-based setups, and hybrid approaches further boost accuracy and efficiency, while optimization techniques like Particle Swarm Optimization (PSO) support feature selection and hyperparameter tuning, making IDS more adaptive in real-world networks.

For DDoS in particular, Transformers are showing strong results. By treating multidimensional traffic data as “sentences,” they use self-attention to capture long-term relationships between events. This makes it easier to catch complex, multi-step attack chains that might otherwise slip past traditional models.

There are still challenges—such as heavy compute demands, exposure to adversarial attacks, and the need for better interpretability—but the overall trend is clear: IDS is moving toward smarter, more adaptive, and more resilient defenses.

---

## 1. Intrusion Detection Systems (IDS)

The evolution of cybersecurity threats has followed a dramatic trajectory—from the early days of simple malware (viruses and worms spreading via physical media), through the rise of network-based threats such as DoS/DDoS attacks, phishing campaigns, and botnets, to today’s complex ecosystem of cyber risks. Modern adversaries are no longer just pranksters; they include organized crime groups and even state-backed actors. These attackers leverage advanced techniques such as social engineering, zero-day vulnerabilities, and lateral movement to launch long-term, targeted campaigns against governments, financial institutions, and critical infrastructure.

At the same time, ransomware has become one of the most profitable cybercrime models, often employing “double extortion” (data encryption combined with threats to leak stolen data). Meanwhile, the rapid proliferation of IoT devices and the expansion of cloud environments have dramatically widened the attack surface.

Traditional defense mechanisms—firewalls, antivirus software, signature-based detection, and conventional intrusion detection systems (IDS)—rely heavily on predefined rules and static signatures. These methods are increasingly ineffective in the face of dynamic, sophisticated, and previously unseen threats such as zero-day exploits and advanced persistent threats (APTs). In this context, artificial intelligence (AI) and machine learning (ML) have emerged as essential technologies for building next-generation intelligent defense systems. With their ability to process vast datasets, adapt in real time, detect novel threats, and automate responses, AI/ML models represent a shift from passive defense to proactive, intelligent security.

However, the integration of AI also introduces new risks. Adversarial machine learning (AML) allows attackers to manipulate models by feeding deceptive inputs that cause misclassification and bypass defenses. In addition, the lack of explainability in AI-driven systems limits trust in critical environments, as security teams may struggle to validate or act upon alerts generated by opaque models. Despite these challenges, AI/ML remain central to the future of proactive cyber defense.

### IDS: Role and Types

An Intrusion Detection System (IDS) is a foundational component of cybersecurity infrastructure. Its core function is to monitor network traffic and system activity, analyzing logs from routers, firewalls, servers, and other devices to identify suspicious behaviors and unauthorized access attempts. IDS typically compares activity patterns against known attack signatures in a database and alerts administrators when anomalies are detected.

IDS comes in several forms:

- **Network-based IDS (NIDS):** Deployed at key network nodes, NIDS passively inspects traffic across the network. It provides broad coverage but has limited visibility into encrypted traffic.
- **Host-based IDS (HIDS):** Installed directly on individual hosts, HIDS monitors system logs, file integrity, and local activity. While customizable and precise, it is resource-intensive.
- **Hybrid IDS:** Combines the strengths of both NIDS and HIDS for comprehensive detection.

Other specialized forms include:

- **Protocol-based IDS (PIDS):** Focused on monitoring specific protocols (e.g., HTTP, SQL) at the application layer.
- **Distributed IDS (DIDS):** Uses remote sensors (network, host, or hybrid) that report to a central management station, enabling coordinated detection in large-scale networks.
- **Honeypots:** Decoy systems designed to lure attackers and study their methods.

IDS differs from firewalls and Intrusion Prevention Systems (IPS). Firewalls block unauthorized access, whereas IDS focuses on monitoring and alerting. IPS takes this further by actively blocking or mitigating malicious traffic in real time.

### Detection Techniques

Traditional IDS detection methods include:

- **Signature-based detection:** Matches traffic against known attack patterns. Highly accurate for known threats but ineffective against zero-day exploits.
- **Anomaly-based detection:** Models normal behavior and flags deviations. Effective for unknown threats but prone to high false positives.
- **Stateful protocol analysis:** Tracks protocol states and identifies anomalous sequences.

Modern IDS are undergoing a revolution with AI integration. Machine learning algorithms—ranging from supervised (e.g., support vector machines, decision trees, random forests, neural networks) to unsupervised approaches—can detect complex patterns in large datasets. Deep learning architectures (autoencoders, CNNs, multilayer neural networks), evolutionary computation techniques (e.g., genetic algorithms for lightweight detection rule generation), and hybrid methods are further enhancing accuracy and efficiency.

### IDS Architecture and Deployment

A typical IDS architecture includes three main components:

- **Sensors or Data Collectors:** Capture information from network traffic, host logs, system calls, and application logs. These sensors can be deployed across the network or on individual hosts.
- **Detection Engine/Analyzer:** Processes and interprets collected data to identify potential intrusions.
- **Response Module/Management Console:** Centralizes alerts for administrators and may provide response coordination.

In distributed and cloud environments, deployment is especially critical. Sensors and detection engines may reside in virtual machines (VMs), hypervisors, or at network boundaries. Cloud providers may manage network-based IDS, while customers handle host-based IDS within their own VMs.

### Challenges and Future Directions

Despite their importance, IDS face several challenges:

- Limited visibility into encrypted traffic
- Scalability issues in large-scale networks
- High false positive and false negative rates
- Evasion techniques used by attackers
- Resource constraints in IoT environments
- Ongoing maintenance of detection rules

AI-driven IDS technologies are addressing many of these limitations, offering adaptive, intelligent detection capabilities to counter increasingly complex threats. As cybersecurity moves from static, rule-based systems to dynamic, learning-driven defenses, IDS will continue to play a critical role at the core of intelligent security infrastructures.

---

## 2. Intrusion Detection Systems (IDS) with AI

Traditional Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) primarily relied on **signature-based detection** to monitor network traffic and identify known threats. While effective for previously cataloged attack patterns, this approach faces critical limitations when dealing with **zero-day exploits**, **advanced persistent threats (APTs)**, and other novel, complex attack vectors. These systems cannot effectively detect unknown or evolving attack patterns.

The integration of **machine learning (ML)** has fundamentally transformed IDS/IPS capabilities. By leveraging anomaly detection and threat classification, ML-enhanced systems can learn patterns, detect deviations from normal behavior, and make real-time intrusion decisions—often without predefined signatures. Research demonstrates significant improvements: for example, in IoT environments, a hybrid **CNN–GRU architecture** (combining convolutional neural networks with gated recurrent units) paired with **feature-weighted SMOTE** to address data imbalance has achieved detection accuracies as high as **99.60%**.

ML-driven IDS/IPS offer several key benefits:

- **Anomaly detection:** Identifying suspicious deviations from baseline behavior.
- **Threat classification:** Categorizing different types of attacks for more precise responses.
- **Real-time adaptability:** Dynamic detection to address fast-evolving threats.
- **Automated response:** Enabling AI-powered mitigation strategies.

To evaluate the effectiveness of AI-based IDS/IPS, researchers rely on benchmark datasets such as **NSL-KDD, CIC-IDS2017, and UNSW-NB15**, and performance metrics including **accuracy, precision, recall, F1-score, throughput, false positive rate (FPR), and false negative rate (FNR)**.

### Supervised Learning in IDS/IPS

Supervised learning depends on **labeled datasets** containing pre-classified benign and malicious network traffic. Using classification algorithms such as **decision trees, support vector machines (SVM), artificial neural networks (ANN), k-nearest neighbors (k-NN), random forests, and deep neural networks (DNNs)**, IDS/IPS systems learn from historical attack data to predict new threats.

This method excels at detecting structured, known attacks such as phishing, malware injection, and botnets. However, it faces challenges:

- **Dependence on labeled data:** Generating quality datasets requires significant human effort and time.
- **Poor generalization to zero-day attacks:** Models often fail against threats absent in training data.
- **Scalability issues:** Real-time environments demand constant retraining and updates.

To address these limitations, researchers are combining supervised learning with **transfer learning** and **semi-supervised learning**, enabling models to adapt prior knowledge and detect emerging threats even with limited labeled data.

### Unsupervised Learning in IDS/IPS

Unlike supervised methods, unsupervised learning requires no labeled data. Instead, it identifies patterns and structures in network traffic, flagging anomalies as potential threats. This makes it particularly effective against **zero-day attacks, APTs, and insider threats**.

Techniques include:

- **Clustering:** Algorithms such as **K-Means, DBSCAN, and Gaussian Mixture Models (GMM)** group traffic by similarity; deviations signal anomalies.
- **Density-based detection:** Methods like **Local Outlier Factor (LOF)** and **Isolation Forest** identify rare or isolated behaviors.
- **Neural autoencoders & VAEs:** Learn compact representations of normal traffic, flagging reconstruction errors as possible attacks.

The tradeoff: these models often suffer from **high false positive rates**, require **continuous fine-tuning**, and can be computationally intensive in real-time, high-volume environments.

### Hybrid and Advanced Approaches

To balance the strengths and weaknesses of each paradigm, modern IDS/IPS solutions increasingly adopt **hybrid learning models**:

- **Semi-supervised learning:** A small set of labeled data guides anomaly detection.
- **Reinforcement learning (RL):** IDS dynamically adapts detection strategies using real-time feedback.
- **Federated learning:** Distributed IDS nodes collaboratively train models across organizations while preserving data privacy.

### Deep Learning in IDS

Deep learning has emerged as a powerful approach due to its ability to model **complex, multi-dimensional data** like network traffic.

- **RNNs (Recurrent Neural Networks):** Capture sequential dependencies in traffic patterns.
- **LSTM (Long Short-Term Memory networks):** Solve RNN gradient vanishing issues using gated memory cells, making them effective for long-term anomaly detection (e.g., APTs, ransomware).
- **GRUs (Gated Recurrent Units):** Simplified LSTM variant, balancing efficiency and accuracy in time-series traffic analysis.
- **Transformers:** Abandon recurrence entirely, using **attention mechanisms** to capture long-range dependencies in parallel. This improves scalability, interpretability, and efficiency in analyzing massive, high-dimensional security data (e.g., traffic logs, code analysis).

These models are especially effective against **advanced threats** like **polymorphic malware**, which constantly mutates its code to evade signature-based systems.

### Threat Classification

Beyond anomaly detection, ML significantly improves **threat classification** within IDS/IPS. Algorithms such as **decision trees, SVMs, and random forests** are trained on features like packet size, session duration, source IP, and destination port. By classifying flows as benign or malicious, models refine their accuracy over time, reducing false alarms and improving precision.

For example, in high-traffic environments, random forest models trained on historical flows can distinguish between benign traffic and intrusions by learning subtle feature correlations.

### Emerging Technologies and Complementary Defenses

In addition to ML integration, several complementary technologies enhance IDS/IPS systems:

- **Blockchain:** Provides decentralized data exchange and immutable audit trails, though with added energy costs.
- **Multi-Factor Authentication (MFA):** Mitigates unauthorized access attempts, reinforcing layered security.
- **Regulatory frameworks:** Standards such as **NIST** and **NERC CIP** play critical roles, but real-time enforcement remains challenging.

---

## 3. Applications of IDS

### 3.1 RNN, LSTM, and PSO

Recurrent Neural Networks (RNNs) have been widely applied in intrusion detection systems (IDS) across different architectures. RNNs are particularly effective at capturing sequential dependencies in data streams, which is essential for modeling the temporal nature of network traffic. However, conventional RNNs suffer from the **vanishing gradient problem**, which limits their ability to retain long-term dependencies. To address this, **Long Short-Term Memory (LSTM)** networks were introduced as an advanced form of RNN. LSTMs preserve the strengths of RNNs while effectively mitigating gradient vanishing, thereby capturing long-range dependencies in sequential data.

Each type of network or internet attack follows a distinct lifecycle that unfolds over time, making temporal features crucial for detection. For example:

- **Denial of Service (DoS):** Attackers overwhelm system resources, rendering them unavailable to legitimate users.
- **Probing (Probe):** Attackers scan networks (e.g., via Nmap) to gather information or identify weaknesses in topology.
- **Remote-to-Local (R2L):** Attackers send packets to a system over the network without authorization, exploiting vulnerabilities (e.g., multi-hop attacks).
- **User-to-Root (U2R):** Attackers with local user access exploit system vulnerabilities to escalate privileges (e.g., malicious kernel module loading).

To enhance detection accuracy, **neural network–based IDS** often incorporate optimization algorithms. For instance, Artificial Bee Colony (ABC) optimization has been integrated with multilayer perceptrons (MLPs) and fuzzy clustering to classify normal versus abnormal traffic. In these systems, MLPs perform classification, while ABC optimizes the weights and biases during training. Performance has been validated using benchmark datasets such as NSL-KDD and CloudSim.

Other approaches leverage Artificial Neural Networks (ANNs) trained on datasets like **KDD CUP’99**. Network-based IDS (NIDS) built on ANN models typically focus on anomaly-based detection. One such system, NNIDS, successfully identified UDP flood attacks, SYN flood attacks, and various scanning techniques in addition to benign traffic. To further improve accuracy, feature selection techniques based on correlation and information gain have been employed to reduce dimensionality. Selected features are then fed into feedforward neural networks (FFNNs), tested across multiple datasets for validation.

Hybrid architectures also show promise. For instance, CNN–RNN combinations have been developed for host-based IDS. In one model, **Gated Recurrent Units (GRUs)** were used instead of LSTMs to achieve comparable accuracy while significantly reducing training time. Stacked CNN layers extract spatial features, while GRUs capture temporal dependencies. The final softmax layer produces probability distributions over classes, enabling effective anomaly detection.

In the context of **Software-Defined Networking (SDN)**, IDS integration has also emerged as a critical area. SDNs offer flexibility but also introduce novel security risks. A GRU–RNN–based IDS was proposed for SDN environments, validated on the NSL-KDD dataset, achieving an accuracy of 89% without degrading network performance. To further improve performance, researchers have developed hybrid optimization algorithms such as **HNADAM-SGD** (Hybrid Nesterov Accelerated Adaptive Moment Estimation with Stochastic Gradient Descent). While outperforming traditional classifiers (logistic regression, ridge regression, ensemble methods), its effectiveness is highly dependent on hyperparameter tuning, such as layer size, learning rate, and regularization strength.

Beyond neural networks, **Particle Swarm Optimization (PSO)** has been applied in IDS for both feature selection and model optimization. The **PSO-LightGBM** approach extracts data features using PSO-enhanced LightGBM and then employs a one-class SVM (OCSVM) for anomaly detection. Validated on the **UNSW-NB15 dataset**, this method effectively identified small-sample attacks such as backdoors, shellcode, and worms. However, its computational cost remains a challenge, particularly for IoT applications.

Another variant, **LPPSO**, modifies particle movement by guiding particles toward neighboring high-fitness solutions rather than only global optima. Coupled with **ELSTM-RNN frameworks**, this approach enhances classification accuracy in detecting and distinguishing attack patterns.

In PSO-based feature selection, each particle represents a potential subset of features. The algorithm iteratively updates particles using both **personal best (pbest)** and **global best (gbest)** values. Fitness functions typically combine measures such as **Balanced Accuracy (BA)** and inter-feature distance. Optimized subsets improve IDS performance by reducing redundancy and dimensionality, enabling efficient training of classifiers.

<details><summary>Code</summary>

```Algorithm

  Algorithm：Feature Selection Using LPPSO Algorithm

  1. Input: Preprocessed Data (NSL-KDD Dataset)
  2. Output: Set of Selected Features
  3. Calculate the
        Gain (Cp,B;P) > log2(|P|−1)/|P| + ∂(Cp,B;P)/|P|  (1)
  4. Calculate
        ∂(Cp,B;P) = log2(3^{kP}−2) − [kPE(P) − kP1E(P1) − kP2E(P2)]  (2)
  5. Calculate Thresh = 0.5
  6. Calculate
        a^(time+1)_id = N(σ,μ), rand() < thresh; K^time_id, otherwise  (3)
  7. Input D-distance
  8. Calculate BA-Balanced Accuracy
  9. Calculate
        fitness = (μ * BA + (1−μ) * D)  (4)
  10. Begin loop
  11.     l_p ← All the entropy cut points that satisfy Eq. (1) of each feature;
  12. Repeat loop
  13.     Initialize particles using the Best index of l_p in l_p;
  14.     While Failure to meet the stop criteria do
  15.         For each particle i do
  16.             C'pi = Transform training collection Cp based on particle location i;
  17.             φi ← Calculate fitness of particle i based on C'pi using Eq.(4)
  18.             ξi ← pbest's fitness;
  19.             if φi is better than ξi then
  20.             Update pbest;
  21.             End of If
  22.          End of For
  23.          Update gbest;
  24.          if It meets the criterion of scale then
  25.          Increase the initial size;
  26.          break;
  27.          End of If
  28.          For each particle i do
  29.          For each dimension j do
  30.             Update particle i location at dimension j with Eq.(3)
  31.          End of For
  32.        End of For
  33.     End of While
  34. until It does not meet the criterion of scale;
  35. Return the selected features and their l_p from the position of gbest;
  36. End of Algorithm

```

</details>

### LSTM in IDS

LSTM networks are particularly effective in IDS when data is structured as time series or sequential logs. Each LSTM cell includes three primary gates:

- **Input Gate:** Determines how much new information enters memory,
  $in^L_T = \sigma(w^L_{in} X^L_T + v^L_{in} H^L_{T-1} + u^L_{in} c^L_{T-1} + B^L_{in})$.

- **Forget Gate:** Decides which parts of the previous memory state should be discarded,
  $F^L_T = \sigma(w^L_{F} X^L_T + v^L_{F} H^L_{T-1} + u^L_{F} c^L_{T-1} + B^L_{F})$.

- **Candidate State:** Generates candidate memory content,
  $\tilde{C}^L_T = \tanh(w^L_{C} X^L_T + v^L_{C} H^L_{T-1} + B^L_{C})$.

- **Updated Cell State:** Combines new and old memory contributions,
  $C^L_T = F^L_T \odot C^L_{T-1} + in^L_T \odot \tilde{C}^L_T$.

- **Output Gate:** Regulates the amount of memory exposed as output,
  $O^L_T = \sigma(w^L_{O} X^L_T + v^L_{O} H^L_{T-1} + u^L_{O} c^L_{T-1} + B^L_{O})$.

- **Hidden State Update:**
  $H^L_T = W_{proj} (O^L_T \odot \tanh(C^L_T))$.

By maintaining a memory of past states and selectively updating information through gates, LSTMs excel at modeling the sequential dependencies inherent in network traffic, making them highly effective for detecting intrusions with temporal dynamics.

---

## 3.2 DDoS Attacks and Transformer-Based IDS

**Distributed Denial of Service (DDoS)** is one of the most destructive and persistent threats to network security. Unlike a traditional Denial of Service (DoS) attack—where a single source floods a target with overwhelming traffic—DDoS attacks coordinate multiple DoS instances against a single target system. This distributed nature makes DDoS significantly more powerful, as it leverages many compromised resources at once. Shutting down all infected nodes simultaneously is practically impossible, making DDoS attacks more resilient and difficult to mitigate.

DoS and DDoS attacks can take many forms, including:

- **ICMP flood**
- **UDP flood**
- **SYN flood**
- **HTTP flood**

All variants share the same effect: saturating system resources, blocking legitimate users from accessing services, and often leading to complete service failure. Over time, the number, scale, frequency, and complexity of DoS/DDoS attacks have continued to rise, requiring advanced detection methods capable of adaptive reasoning.

### Transformers for IDS

Traditionally, **Transformer architectures** were designed for natural language processing (NLP), where data is structured as sentences made up of tokens (words). In contrast, network security data is **multivariate time-series**, consisting of numerical features (e.g., IP addresses, port numbers, packet sizes, and protocol types) rather than words with inherent contextual meaning.

To bridge this gap, researchers reformulate network traffic into a "sentence-like" structure. Each feature is treated as a “word,” and multiple time steps of features are combined to form a “sentence.” This approach allows the Transformer to learn long-range dependencies and contextual relationships within network activity. By applying this method, the model can identify complex multi-stage attacks across time—such as _scanning → login attempts → privilege escalation → data exfiltration_—and distinguish them from benign traffic through contextual analysis. This greatly improves both accuracy and intelligence in intrusion detection.

### Transformer Encoding Process

The **encoding pipeline** adapts standard Transformer mechanisms for network traffic analysis:

- **Input Representation**
  The network traffic sequence is represented as:
  $\{x_i \in \mathbb{R}^J \mid i = 1, \dots, N\}$

  - $J$: number of features per time step (analogous to word embedding dimension).
  - $N$: total number of time steps.

- **Patch Embedding**
  Input sequences are divided into small chunks (patches), then linearly transformed:
  $Z^0 = [x_1E; x_2E; \dots; x_P E], \quad E \in \mathbb{R}^{J \times C}, \; Z^0 \in \mathbb{R}^{T \times C}$

  - $C$: embedding dimension.
  - $T$: number of patches.
    This mirrors the Vision Transformer (ViT) approach to images, where patches are embedded into tokens.

- **Self-Attention Mechanism**
  Each patch is transformed into queries ($Q$), keys ($K$), and values ($V$):
  $A(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{P}}\right)V$

  - $QK^T$: captures pairwise relevance between tokens.
  - Softmax normalizes relevance into weights.
  - Weighted $V$: produces contextualized feature representations.
    Scaling by $\sqrt{m}$ stabilizes gradients, preventing vanishing/exploding values.

- **Multi-Head Attention (MHA)**
  Multiple attention operations are performed in parallel across different subspaces:
  $\text{MHA} = \text{Concat}(A_i(.))W, \quad i=1,\dots,H$
  This allows the model to capture dependencies from diverse perspectives.

- **Transformer Layer Stacking**
  Each Transformer layer consists of:

  - **Multi-Head Attention + Residual Connection + Layer Normalization**
  - **Feedforward Network (MLP) + Residual + Layer Normalization**
  - **Masking** is applied to prevent the model from “peeking ahead” at future time steps, ensuring temporal causality.

  The layer operations can be expressed as:

  - $Z_l = \text{Mask}(\text{MHA}(\text{LN}(Z_{l-1}))) + Z_{l-1}$
  - $Z_l = \text{MLP}(\text{LN}(Z_l)) + Z_l$
  - $Z_l = \text{LN}(Z_l), \quad l=1,2,\dots,L$

- **Output Representation**
  After $L$ Transformer layers, the output has shape:
  $\mathbb{R}^{B \times T \times C}$

  - $B$: batch size.
  - $T$: token count (per sequence).
  - $C$: embedding dimension.

  A pooling operation or selection of the \[CLS] token aggregates token-level outputs into a **sample-level representation**.

- **Classification**
  The aggregated vector is passed through a fully connected (FC) layer:
  $h \in \mathbb{R}^{B \times 2}$
  representing binary classification (normal vs. attack).
  Applying **Softmax** yields prediction probabilities:
  $p = \text{Softmax}(FC(h)) \in \mathbb{R}^{B \times 2}$

- **Training Objective**
  Binary Cross-Entropy (BCE) loss is used to train the classifier:
  $CE(y,p) = -\frac{1}{n}\sum_{i=1}^n y_i \log(p_i)$

  - $y_i$: ground truth labels (0 = normal, 1 = attack).
  - $p_i$: predicted probability.

  Minimizing BCE encourages the model to correctly separate normal traffic from malicious flows.

By transforming network traffic into structured sequences, Transformer-based IDS systems exploit the architecture’s strength in modeling long-range dependencies. This enables the detection of **multi-step, context-dependent cyberattacks**, making Transformers a powerful tool in combating modern DDoS and other evolving threats.

---

[Forta](https://github.com/forta-network)
[Snort3](https://github.com/snort3/snort3)
[Zeek](https://github.com/zeek/zeek)
[DDoS_DetectioNet](https://github.com/mturkoglu23/DDoS_DetectioNet)
[RNN-Effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
[LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
