# ETAAcademy-Audit: 33. Physical Attacks

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>33 PA</td>
    <td>
      <table>
        <tr>
          <th>audit</th>
          <th>basic</th>
          <th>article</th>
          <td>PA</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-Audit](https://github.com/ETAAcademy/ETAAcademy-Audit)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# Physical Attacks: Hardware Security, Reverse Engineering, and Defensive Architectures

Embedded systems security relies on a multilayered architecture spanning physical PCB design to secure software stacks, anchored by an immutable Boot ROM that establishes a cryptographic Chain of Trust.

Hardware threat modeling prioritizes attack scalability and cost-benefit analysis, distinguishing between the high initial cost of vulnerability identification and the potentially zero cost of widespread exploitation.

Systems are vulnerable to a spectrum of physical and logical attacks, including fault injection techniques like voltage glitching and Rowhammer, side-channel analysis of power and timing signatures to recover secrets like PINs or cryptographic keys from RSA and ECDSA implementations, and microarchitectural exploits such as Spectre and Meltdown.

Hardware reverse engineering, exemplified by bus eavesdropping on high-speed interfaces like HyperTransport using FPGAs, allows for the interception of hidden boot sequences and decryption keys. Supply chain risks extend from component replacement and hardware implants to sophisticated silicon-level modifications during manufacturing, such as mask tampering or die-stacking with through-silicon vias (TSVs).

Robust defense strategies integrate active shielding with conductive grids, friable potting materials, and autonomous tamper response mechanisms like zeroization of battery-backed RAM (BBRAM) and signal isolation. To counter environmental failures and fault attacks, systems employ sensors for radiation, temperature, and voltage, alongside clock integrity monitoring via phase-locked loops (PLLs) and software-level data inversion to mitigate remanence effects.

Ultimate hardware trust and privacy are pursued through user-verifiable designs using "soft silicon" on FPGAs and plausible deniability mechanisms like the Plausible Deniable Database (PDDB), which hides encrypted data within indistinguishable random noise to protect against coercion.

---

## 1. Embedded Hardware and Software Components

Digital systems employ discrete symbols and abstraction to represent information, providing inherent noise immunity compared to continuous analog systems by mapping physical quantities like voltage into logic levels. This robustness is achieved through specific signal thresholds and noise margins, where device output standards are stricter than input requirements ($V_{OL} < V_{IL} < V_{IH} < V_{OH}$) to ensure minor signal degradations do not lead to logic errors. The preservation of signal integrity relies on Voltage Transfer Characteristics (VTC) requiring active components with a gain greater than one to re-quantize and restore degraded signals at each logic stage.

Digital circuits are categorized into combinational systems, which compute outputs based purely on current inputs, and sequential systems, which introduce memory and state through feedback loops. While basic storage elements like D-latches are level-sensitive and prone to race conditions, D-flip-flops utilize a master-slave configuration to achieve edge-triggered behavior, isolating inputs from outputs during transitions. Synchronous sequential circuits facilitate complex system modeling as finite state machines (FSMs) by using a global clock to discretize time. Reliable data capture in these systems necessitates rigorous static timing analysis (STA) to satisfy setup time constraints, which define the maximum operating frequency based on critical paths, and hold time constraints, which ensure signal stability after a clock edge to prevent data corruption.

- Tips: Before diving in, you may want to review [28_CLSL](./28_CLSL.md) and [30_HSI](./30_HSI.md)

Embedded systems consist of a physical layer (PCB, SoC, memory, and analog components) and a layered software stack that begins with an immutable Boot ROM to establish a secure "Chain of Trust." The fundamental building blocks of **embedded system architecture** and the **Root of Trust** include: **System on Chip (SoC)**, which is an integrated circuit that integrates all components of a computer or other electronic systems on a single chip, including the CPU, memory, and peripheral interfaces. The **Chain of Trust** is a security architecture where each software component verifies the digital signature of the next component before execution, ensuring only authorized code runs. A **Trusted Execution Environment (TEE)** is a secure area within the main processor that guarantees the confidentiality and integrity of code and data loaded into it. **Volatile vs. Non-volatile Memory** distinguishes between memory that requires power to maintain stored information (DRAM, SRAM) and memory that provides persistent storage (Flash, EEPROM, ROM).

A typical Printed Circuit Board (PCB) for an embedded device includes key components: the processor (usually within a SoC), volatile memory (DRAM) for real-time processing, and non-volatile memory (Flash/EEPROM) for persistent storage. Analog components (resistors, capacitors, inductors) are common targets for side-channel and fault injection attacks.

The software portion includes various stages of system boot:

- **Initial Boot Code (Boot ROM)**: Immutable code stored in the chip that serves as the first instructions the system executes.
- **Bootloader**: Mutable code used to initialize hardware and verify the identity of the operating system.
- **Operating System and Applications**: High-level software providing services and process isolation.

Particularly in the **Chain of Trust**, the Boot ROM (Root of Trust) uses an embedded public key to verify the bootloader, which in turn verifies the operating system.

The **Chain of Trust** process can be viewed as an algorithmic sequence:

- **Boot ROM**.
- **Boot ROM** verifies the **First Stage Bootloader** (using a public key/hash in fuses).
- **First Stage Bootloader** verifies the **N-th Stage Bootloader**.
- **Final Bootloader** verifies the **Operating System/Applications**.

This implements the theory of **Transitive Trust**, which extends trust from small, immutable hardware to large, complex software packages through cryptographic verification.

**Boot ROM** is usually immutable and thus a "high-risk" target; vulnerabilities here often cannot be patched. **One-Time Programmable (OTP) fuses** are considered a physical mechanism for storing unique identities and anti-rollback versions. The distinction between the **Secure World** (TEE) and the **Normal World** (REE) serves as a virtualization of the SoC hardware.

---

## 1.1 Hardware Threat Modeling, Attack Analysis, and Countermeasures

Hardware threat modeling involves categorizing potential attackers based on their motivations, resources, and technical capabilities, and assessing threats through cost-benefit analysis. **Threat Modeling** is a structured approach used to identify, quantify, and address security risks. **Cost-Benefit Analysis** evaluates the feasibility of an attack by comparing the required investment (time, money, equipment) with the potential gains (money, reputation, data). **Scalability** is the ability of an attack to be replicated across multiple devices with minimal additional investment after the initial vulnerability is discovered. **Attack Trees** are visual representations of the paths an attacker can take to compromise system assets.

Perfect security is unattainable ("unicorns don't exist"). Instead, security is "good enough" if the cost of an attack exceeds the gain. **Scalability** is a key differentiator. Attacks requiring physical modification (e.g., soldering a modchip) are less scalable than software exploits or reusable hardware. **Identification vs. Exploitation** is critical. The initial cost to find a vulnerability (identification) is often high, but for scalable attacks, the cost to replicate the vulnerability (exploitation) may be near zero. A system is secure if the resources an attacker loses during execution exceed what they gain from a successful attack. This defines the "Standard" of security engineering.

### Attack Types (Hardware, Software, Logic)

Hardware-related attacks range from software-driven techniques (such as fault injection and side-channel analysis) to physical PCB tampering and logical exploitation of existing interfaces. Methods for compromising embedded systems are divided by target and mechanism:

- **Fault Injection**: Intentionally introducing errors into the processor or memory (e.g., voltage glitches, overclocking) to bypass security checks. Techniques such as **Rowhammer** (DRAM) and **CLKSCREW** (voltage/frequency) are used to flip bits or skip instructions.
- **Side-Channel Analysis**: Extracting secret information by measuring unintended physical emissions (power consumption, timing, electromagnetic signals). Examples include timing attacks on functions like `strcmp` or measuring power consumption during cryptographic operations.
- **Logic Attacks**: Exploiting errors in software/firmware logic or protocol implementations without physically modifying hardware. This includes sending malicious commands via standard I/O, exploiting memory corruption (buffer overflows), or abusing hidden debug consoles.
- **Microarchitectural Attacks**: Exploiting CPU optimizations (such as caching or speculative execution) to leak data. **Spectre** and **Meltdown** leverage speculative execution and cache timing to read process memory.
- **PCB-Level Attacks**: Physical manipulation of the circuit board, such as reading flash memory chips, modifying "jumper resistors" (zero-ohm resistors), or eavesdropping on JTAG/SWD debug interfaces.
- **Fuzzing**: Sending random/structured data to interfaces to find crashes or edge cases.

The general timing leakage model illustrates that if a function's execution time depends on a secret value (e.g., how many characters in a password match before `strcmp` exits), an attacker can infer the secret by measuring time. It turns a black-box system into a transparent one through a side channel.

$$
T_{execution} = f( \text{Data}_{Public}, \text{Data}_{Secret}) + \text{Noise}
$$

- $T_{execution}$: Execution time, measured in clock cycles or wall-clock time.
- $\text{Data}_{Public}$: Attacker input, e.g., a candidate password.
- $\text{Data}_{Secret}$: Hidden asset, e.g., the real password.

**Logic of the `strcmp` Vulnerability**: Compares two strings. As soon as a character mismatch occurs, it exits the `while` loop. The more characters match, the longer the loop runs. An attacker can brute-force the password character by character by looking for "timing spikes" that occur with each successful match.

<details><summary>Logic of the `strcmp` Vulnerability</summary>

```c
int strcmp(const char *s1, const char *s2) {

    while (*s1 && (*s1 == *s2)) {
        s1++;
        s2++;
    }

    return *(const unsigned char*)s1 - *(const unsigned char*)s2;
}
```

</details>

**Additional Resistors**: Zero-ohm resistors highlight a common "hidden" hardware configuration. These are methods attackers can easily toggle. **Rowhammer** is a fascinating mechanism that uses software access patterns to cause physical hardware failures (bit flips). **Optimization** (speed) often conflicts with **Security**, as exemplified by the Spectre vulnerability.

### Advanced Chip Invasive Attacks and Countermeasures

Chip invasive attacks use high-precision equipment to bypass physical protections at the silicon level, requiring multi-layered defense strategies including protection, detection, and active response. **Physical Cryptanalysis** and **Silicon-Level Attacks** include: **Decapsulation**, using chemicals (acid) to remove the plastic packaging of an IC to expose the die. **Focused Ion Beam (FIB)** is a nanoscale tool that can "mill" away silicon or "deposit" metal, essentially allowing an attacker to rewire the chip's internal logic. **Microprobing** uses tiny physical probes to tap into microscopic buses or signals inside the chip. **Standard Cells** are basic logic building blocks (gates) arranged in silicon layers.

"Micro" attack methods:

- **Physical Access**: Using acid to expose the die (decapsulation) or immersing the entire chip in acid (delayering).
- **Imaging**: Using optical or Scanning Electron Microscopes (SEM) to reverse-engineer the "netlist" (the blueprint of all gate connections).
- **Active Attacks**: FIB editing can cut security fuses or create "probe pads," allowing microprobing of internal buses.
- **Photonic Analysis**: Hot-carrier luminescence (photons emitted during switching) can be used for side-channel analysis.

Defense strategies are categorized as follows:

- **Protection**: Avoiding attacks (e.g., encryption, active shields).
- **Detection**: Monitoring for anomalies (e.g., voltage sensors, light detectors to see if the chip is removed, stack canaries).
- **Response**: Immediate action upon detecting an anomaly (e.g., erasing keys/self-destruction, extending boot time, falling back to a secure mode).

The IoT Toothbrush Attack Tree follows a set of logical algorithms for compromising a system.

**Toothbrush Attack Flow:**

- **Identification Phase:** Read Flash $\to$ Find encrypted data. Use **Differential Power Analysis (DPA)** to find AES keys. Decrypt Flash $\to$ Reverse engineer to discover vulnerabilities.
- **Exploitation Phase:** Modify image $\to$ Add backdoor. Use **Voltage Fault Injection** to skip RSA signature verification logic in ROM. Boot modified image $\to$ Achieve remote control.

This demonstrates the cost gap between **Identification and Exploitation**. High-cost hardware identification enables low-cost, scalable remote software exploitation.

**FIB Editing** being able to "rewire" a chip is the ultimate bypass; if an attacker is skilled enough, hardware-level security logic (like active shields) can be rendered useless. FIB editing attempts to bypass active shields by creating "hat" structures, and **Active Shielding** is precisely one of the countermeasures. The **Response Dilemma** is that for a payment card, "erasing" data is good; but for an autonomous vehicle, "erasing" data during operation could be fatal. This highlights the tradeoff between **Security and Reliability/Resilience**.

---

### 1.2 Power Analysis and PIN Verification

Sequential PIN verification logic is vulnerable to side-channel analysis, particularly leveraging timing information to attack linear search patterns. **Side-Channel Analysis** is a form of security attack that exploits information leaked from the physical implementation of a system (e.g., timing or power consumption) rather than flaws in the algorithm itself. **Timing Attacks** occur when a program's execution time varies based on input data, thereby leaking secret information.

A typical C implementation for 4-digit PIN verification. While brute-forcing a 4-digit PIN takes 10,000 attempts on average, a timing attack can significantly reduce this cost. The core vulnerability lies in the `if` statement within the comparison loop: the function returns early as soon as a mismatch is found, meaning processing a partially correct PIN takes longer than processing a completely incorrect one.

Brute-force complexity $N = 10^k$, where N is the total possible combinations. For a k-digit PIN, each digit has 10 possibilities. k is the number of digits, e.g., k=4. The total number of unique sequences equals the number of possibilities for each digit raised to the power of the number of digits. It defines the "upper bound" of security against an uninformed attacker. This provides a baseline for comparing the efficiency of timing attacks.

Timing attack complexity $N_{timing}$ is the maximum number of guesses for a timing attack, assuming 10 digits per position. By identifying digits one by one, the complexity becomes additive ($10+10+10+10$) rather than multiplicative ($10 	imes 10 	imes 10 	imes 10$). This shows a massive decrease in security (from 10,000 guesses to 40). This is mathematical proof of why side-channel information is so dangerous.

$$
N_{timing} = 10 \times k
$$

<details><summary>Listing 1-1: Sample PIN code check written in C</summary>

```c
// Store four most recent buttons
for(int i = 0; i < 4; i++) {
    user_pin[i] = read_button();
}
// Wait until user presses 'Valid' button
while(valid_pressed() == 0);
// Check stored button press with correct PIN
for(int i = 0; i < 4; i++) {
    if(user_pin[i] != correct_pin[i]) {
        error_led_on();
        return 0;
    }
}
return 1;
```

</details>

Records 4 button presses into an array, waits for a validation signal, then iterates through the array, comparing each digit with the stored PIN. If any digit mismatches, it triggers an error LED and exits immediately. The `if` statement followed by `return 0` creates a timing leak. Each successful `user_pin[i] == correct_pin[i]` comparison allows the loop to continue to the next iteration, adding a fixed amount of processing time. The attack assumes the attacker can measure the time between the "Valid" button press and the "Error LED" light turning on with enough precision to distinguish loop iterations.

### Timing Attack Concepts and Hard Drive Timing Attacks - Practical measurement techniques and a case study on the Vantec Vault

Executing a timing attack on a real device requires recovering a multi-digit PIN by externally measuring scan and processing latencies within a microcontroller. **Button Scanning (Polling)** is a common hardware technique where a microcontroller checks input states periodically (e.g., every 50ms) rather than being interrupted continuously. It also relies on **Signal Edge Triggering**, using specific voltage transitions (e.g., a button scan signal) as a reference point to start high-precision time measurements.

Attackers use oscilloscopes to measure the time delay between "Valid" and "Error" signals, comparing $t_{bad}$ (processing time when the first digit is completely wrong) and $t_{correct}$ (processing time when the first digit is correct, requiring the loop to process the second digit). For example, with the Vantec Vault drive enclosure, the difficulty lies in the fact that it only periodically scans button presses. By identifying the "button scan signal" (line B), attackers can synchronize measurements with the exact moment the microcontroller processes input, revealing the processing time for each digit.

Processing delay comparison: a partially correct PIN sequence leads to a significantly longer delay before generating an error response. This difference is the "leak" that allows for step-by-step discovery of the PIN. This is the physical implementation of the branch logic in Listing 1-1.

$$
t_{correct} > t_{bad}
$$

- $t_{correct}$: Time to signal error when the first digit is correct, including time for at least one extra loop iteration.
- $t_{bad}$: Time to signal error when the current digit is incorrect, the shortest error path.

**Scanning Resolution**: The attack needs to trigger off the microcontroller's internal scan clock, not the user's physical button press, as the latter has too much jitter relative to the 100ms scan interval.

**Hardware Access**: The attacker has already "teardown" a copy of the device to find internal probe points (e.g., scan signals or error lines). The difference between "one million combinations" (mathematical complexity) and the "60 guesses" actually required (implementation complexity).

### Power Measurement for Timing Attacks and Simple Power Analysis (SPA) - Transitioning from simple timing to power signature analysis

Transitioning from simple timing measurements to **Simple Power Analysis (SPA)**, which directly observes the internal execution path of an algorithm through the characteristic power consumption of individual instructions. **Simple Power Analysis (SPA)** is a technique for extracting secret information by directly observing a device's power consumption during a single cryptographic operation. It is based on the fact that different operations (e.g., multiplication vs. addition) activate different sets of transistors, creating distinct **power signatures**. **Countermeasures** like random delays are "seen through" by SPA by identifying the characteristic patterns of the delay loops themselves.

Even if developers add random delays to thwart timing attacks, the timing of underlying operations is still reflected in the device's power consumption. Repeating patterns in a power trace correspond to loop iterations; by identifying these, an attacker can isolate comparison functions from the noise of random delays. SPA recovers secrets by observing a single (or few) traces, contrasting with **Differential Power Analysis (DPA)**, which uses statistical analysis over thousands of traces.

Instruction-level leakage indicates that power consumption at any moment depends on the specific instruction being executed and, to a lesser extent, the data it processes. This allows an attacker to "read" code as it executes on hardware. This is why window A remains a target even if window B (delay) changes.

$$
P(t) \approx f( \text{Instruction}_t, \text{Data}_t)
$$

- $P(t)$: Instantaneous power, measured via voltage drop across a shunt resistor.
- $\text{Instruction}_t$: CPU instruction executed at time t, e.g., `MUL`, `ADD`, `LOAD`.
- $\text{Data}_t$: Data being processed by the instruction, which can also leak (more subtle).

**SPA vs. DPA**: SPA is _visual_ or _direct_ path analysis, while DPA is _statistical_.

**Micro-timing**: SPA allows for timing individual instructions, which is much finer than measuring intervals between external I/O events. **Visibility of Randomness**: Pseudo-random processes may reveal themselves through repeating power patterns after a device reset, highlighting the weakness of pure software randomness.

### Applying SPA to RSA - Visual identification of Square vs. Multiply operations to recover private keys

By distinguishing "Square" and "Multiply" operations in modular exponentiation algorithms, visual analysis of power blocks (SPA) directly reveals the private key of an RSA implementation. **Modular Exponentiation**, specifically the **Square-and-Multiply algorithm**, is used to efficiently compute $c = m^e \pmod n$. Security vulnerabilities arise because the "Multiply" step is only executed when a specific bit of the secret exponent $e$ is set to 1, creating a data-dependent execution path.

Listing 1-2 is a simplified implementation of the square-and-multiply algorithm. In each iteration of the loop, a value is "squared" ($s = (s \times s) \mod n$). If the current bit of the key is 1, an additional "multiplication" ($P = (P \times s) \mod n$) is performed. This shows a power trace with distinct "Quick" (Q) blocks and "Long" (L) blocks. By observing that L blocks never occur consecutively and always follow a Q block, Q represents Square (S) and L represents Multiply (M). This allows for reconstruction of the entire 10-bit key.

Iterates through each bit of `secret_data`; it always performs a square operation (after the first iteration) and only performs a multiplication if the current Least Significant Bit (LSB) is 1. The conditional statement `if (secret_data & 0x01)` creates a branch. If the bit is 1, the CPU executes the multiplication code path. In the power trace, this appears as a "Q" block immediately followed by an "L" block.

<details><summary>Listing 8-2: Square-and-multiply algorithm</summary>

```c
for(i = 0; i < 10; i++) {
    if (i > 0)
        s = (s * s) % n;      // Square (S)
    if (secret_data & 0x01)
        P = (P * s) % n;      // Multiply (M) - Conditional
    secret_data = secret_data >> 1;
}
```

</details>

**Block Order Logic**: Inferring that `{Q -> S, L -> M}` is based on the structure of the algorithm: multiplication _must_ be preceded by squaring. If the mapping were `{L -> SM}`, two consecutive 1s would produce an "LL" sequence. Since "LL" is absent and "QL" is present, the mapping is confirmed. The code skips squaring on the first iteration (`i > 0`). This is a "known anomaly" attackers must account for when aligning traces to bits. **Key Order**: Bits are processed from Least Significant Bit (LSB) to Most Significant Bit (MSB).

### Applying SPA to RSA, Redux - Analysis of a real-world vulnerability in MBED-TLS due to logical AND short-circuiting

A critical side-channel vulnerability in the MBED-TLS library, where C's short-circuit evaluation of the logical AND operator leads to key bits leaking through the instruction execution order. Based on the concept of **Short-Circuit Evaluation** in C. In an expression like `(A && B)`, if `A` is false, `B` is never evaluated. While this is an optimization, it becomes a **fatal flaw** when `A` depends on secret data and `B` does not. **Windowing** is a modular exponentiation technique that processes multiple bits at once for performance.

The `mbedtls_mpi_exp_mod` function in `bignum.c` is designed to skip leading zeros in a key. However, the comparison `if( ei == 0 && state == 0 )` evaluates the secret bit `ei` before the public flag `state`. This means the CPU executes the `ei == 0` comparison regardless of whether it is still processing. Removing leading zeros. This operation produces a characteristic power signature, leaking every bit of the key.

Logical leakage model: because `ei == 0` is the first operand, the processor _always_ fetches `ei` and performs the comparison. Even if the result of the `if` statement is the same (doing nothing), the _process_ of reaching that decision differs for `ei=0` and `ei=1`. This is a classic example of "constant-time" vs. "non-constant-time" logic at the compiler/instruction level.

$$
\text{Execute}(B) \iff 	\text{True}(A)
$$

For the case `ei == 0 && state == 0`:

$$
\text{Evaluation Path} = \begin{cases} \text{Test } ei, \text{ stop} & \text{if } ei
\neq 0 \\
\text{Test } ei, \text{ then Test } state & \text{if } ei = 0 \end{cases}
$$

- `ei`: Individual bit of the key, the leaked secret.
- `state`: Flag for leading zero processing, becomes non-zero after the first '1' bit.

Code Example: MBED-TLS `mbedtls_mpi_exp_mod` Logic Flaw: The loop iterates through key bits; `state` is initially 0 until an `ei` of 1 is encountered. Since `ei == 0` is checked first, the power signature of this line varies with the value of `ei`. If `state == 0` were checked first (`if( state == 0 && ei == 0 )`), once `state` became non-zero, the branch would not evaluate the secret key `ei`, preventing leakage.

<details><summary>Listing 1-3: Flawed mbedtls_mpi_exp_mod logic</summary>

```c

for( int i = 0; i < secret_key_size; i++ ){
    ei = (secret_key >> i) & 1; // Load secret bit
    if( ei == 0 && state == 0 ) // Flawed comparison
        // Do nothing
    else
        state = 2; // state becomes non-zero
}
```

</details>

**Compiler Dependency**: A "reliable C compiler" will always execute the first comparison. This highlights that security often depends on the behavior of the toolchain, not just source code. The fix is simple: swap the operands. This shows that sometimes side-channel defense can be achieved with minimal overhead code changes. The value of "making basic assumptions as an attacker"—even "best-in-class" implementations like MBED-TLS can have simple, fundamental flaws.

### SPA Attack Against ECDSA - Statistical analysis of elliptic curve scalar multiplication to recover nonces and private keys

A comprehensive SPA attack against the Elliptic Curve Digital Signature Algorithm (ECDSA), demonstrating how secret nonces—and subsequently full private keys—can be recovered through statistical analysis of scalar multiplication loop durations. **Elliptic Curve Cryptography (ECC)**, specifically the **Scalar Multiplication** operation ($k 	imes G$), uses the **Double-and-Add algorithm**, which is the ECC equivalent of RSA's Square-and-Multiply. A key concept here is the **Nonce (k)**, a "use-once" number that must remain secret. If $k$ leaks for any signature, the private key $d$ can be derived mathematically. Furthermore, **Timing Jitter** causes measured execution times to have overlapping distributions, requiring statistical treatment.

During ECDSA signing, the operation $k 	imes G$ (computing a public point from a secret nonce) is the primary leakage point. Similar to RSA, a 0-bit leads to one operation (Point Double), while a 1-bit leads to two (Point Double + Point Add). Noisy timing can simulate this; to recover these bits, a threshold is used to convert power traces into "binary" traces. Measuring the duration of peaks; due to jitter, these durations cannot be perfectly separated into two groups. The attack identifies "uncertain" bits near the cutoff and uses brute-force to test these candidates until the signature is verified.

ECDSA Signature Generation: Multiplies a generator point $G$ by a secret scalar $k$. This is the core operation that leaks $k$ info via the "double-and-add" algorithm. This implements "Square-and-Multiply" logic at the elliptic curve level.

$$
R = k \times G
$$

- $R$: Result point $(x_R, y_R)$, where the $x$-coordinate is used for $r$.
- $k$: Nonce, the **secret target**.
- $G$: Base point (generator), constant, public.

Relationship between $k$ and $d$: If you know the nonce $k$ used for a specific signature $(r, s)$, you can solve for the private key $d$ using simple modular arithmetic. This explains why leaking even one nonce is catastrophic for the security of the entire system. This is the mathematical "bridge" from side-channel leakage to full system compromise.

$$
s = k^{-1}(e + r \times d) \pmod n \implies d = r^{-1}(s \times k - e) \pmod n
$$

- $s$: Part of signature $(r, s)$, public.
- $r$: Part of signature $(r, s)$, public.
- $e$: Message hash, public.
- $d$: Private key, the **final target**.
- $k$: Nonce used for this signature, recovered via SPA.

Logic of `leaky_scalar_mul()` (Pseudocode) implements the double-and-add algorithm. Conditional execution of `point_add` creates a time difference. `duration(0) = T_double`, while `duration(1) = T_double + T_add`.

<details><summary>Logic of leaky_scalar_mul() (Pseudocode)</summary>

```python
for bit in k_bits:
    P = point_double(P)      # Always executed
    if bit == 1:
        P = point_add(P, G)  # Only executed if bit is 1
```

</details>

**Jitter and Overlap**: Indicates that a single threshold is insufficient due to noise. This is why the attack identifies "uncertain" bits for brute-forcing. Attackers do not need the correct $d$ to confirm a successful attack. They can use the guessed $d$ to verify existing signatures $(r, s)$. If it matches, $d$ is correct. This simulate-then-attack approach is vital for practical attacks, as it distinguishes noise-induced failures from logical errors.

---

## 2. Hardware Reverse Engineering + Security Analysis

Due to the low signal count and physical accessibility (compared to the Processor Front Side Bus or Main Memory Bus), the Northbridge-Southbridge connection is the best target for eavesdropping to recover hidden Xbox boot code. **Hardware Eavesdropping** is a technique where an attacker monitors physical communication channels (buses) between integrated circuits to intercept sensitive data such as decryption keys or boot sequences. It requires understanding **Bus Topology** (FSB, Memory Bus, Chipset Interconnects) and their respective **Physical Layer Specifications** (clock frequency, bus width, and signaling conventions).

### Selection of the HyperTransport bus as the optimal eavesdropping target over the FSB or Memory Bus due to signal count and accessibility

The true hardware initialization and boot image decryption sequence for the Xbox are hidden outside the FLASH ROM. To recover this code, there are three potential eavesdropping targets:

- **Front Side Bus (FSB)**: High signal count (50 address/control + 64 data) and complex AGTL+ signaling make this expensive and difficult to tap.
- **Main Memory Bus**: Faster (200 MHz DDR) and uses SSTL-2 signaling. While easier to probe via spare memory footprints, it only captures data written to RAM, missing read-only decryption keys stored in the processor cache.
- **Northbridge-Southbridge Connection**: A pair of 8-bit unidirectional differential buses using the HyperTransport protocol. With only ten unique signals, it offers the lowest risk of hardware failure during soldering and the least difficulty for eavesdropping, despite its 400 MB/s data rate.

**Technical Comparison Metrics**

| Parameter                 | Front Side Bus (P-III) | Main Memory Bus     | Northbridge-Southbridge |
| :------------------------ | :--------------------- | :------------------ | :---------------------- |
| **Data Width**            | 64-bit                 | 128-bit             | 8-bit (unidir)          |
| **Signal Count**          | ~100                   | ~100+               | 10                      |
| **Clock Frequency**       | 133 MHz                | 200 MHz (DDR)       | 200 MHz (DDR)           |
| **Signaling**             | AGTL+                  | SSTL-2              | HyperTransport (Diff)   |
| **Package Compatibility** | Low (BGA/Socket)       | Medium (Spare pads) | High (Parallel traces)  |

Eavesdropping complexity is proportional to the number of signals and the complexity of signaling rules. In hardware security, the "path of least resistance" is often determined by physical accessibility and the number of required solder points. This drives the choice of HyperTransport as the target, balancing data rate challenges with physical implementation feasibility.

**Cache Persistence**: Read-only data (like keys) may flow directly from hidden ROM to processor cache without ever appearing on the memory bus. This critical observation explains why the memory bus is unsuitable for key extraction. **Trace Identification** relies on publicly available chipsets (e.g., nVidia nForce) to infer the Xbox's proprietary implementation, demonstrating the power of **Architectural Inference** in reverse engineering.

### High-Speed Information Transmission (Signaling Standards), including differential vs. single-ended transmission and transmission line effects

High-speed digital communication requires differential signaling and impedance-controlled transmission lines to manage return currents, minimize signal distortion, and prevent energy reflections. Based on **Transmission Line Theory** and **Signal Integrity**. In high-speed transmission, wires are no longer just DC conductors but transmission lines where signals propagate as electromagnetic waves, including **Single-Ended vs. Differential Signaling** (how signals are referenced), **Return Paths** (how current loops are formed), and **Termination** (matching impedance to prevent reflections).

Physical limits of digital information transmission:

- Single-ended signaling (e.g., TTL/CMOS) is prone to interference at high speeds because unpredictable return paths through "Ground" cause distortion.
- Differential signaling uses two wires with opposite polarity, providing a clear and balanced return path, making it more robust against noise.
- Signaling specifications (e.g., AGTL+, SSTL, LVDS, etc.) define how voltages translate to logic signals. Modern standards use small signal swings and require termination resistors at the end of the transmission line to dissipate energy and prevent reflections that interfere with subsequent data.
- Transmission line effects depend on signal transition time (rise/fall time), not just clock frequency. Even a low-frequency clock can be affected if transitions are very sharp (e.g., picoseconds).

Ohm's Law (applied to transmission lines): The voltage on a transmission line is the product of the current injected by the driver and the characteristic impedance of the line. In current-mode drivers (like LVDS or HyperTransport), the driver source regulates current $I$. To maintain a specific voltage swing $V$ (e.g., 600 mV), impedance $Z_0$ must be precisely controlled via PCB design. This explains why signals on a bus can be cancelled or overridden by injecting an "antagonistic" current, enabling Man-in-the-Middle attacks.

$$
V = I \cdot Z_0
$$

- $V$: Voltage, potential difference.
- $I$: Current, flow of charge.
- $Z_0$: Characteristic impedance, typically 50 Ω for PCB traces.

**Frequency vs. Transition Time** is a critical factor to distinguish: transmission line effects depend on **transition speed** (how fast 0 becomes 1), not **repetition frequency** (clock speed). **Return Paths** emphasize that current must always return to its source. In high-speed systems, return current follows the path of least **inductance** (directly under the signal trace) rather than least **resistance**. **FPGA Flexibility** illustrates that modern FPGAs (e.g., Xilinx Virtex-II) have programmable I/O standards, making them versatile tools for eavesdropping on various signaling conventions.

### High-Speed Bus Eavesdropping and Interfaces: Technical implementation of the Hardware Tap, covering trace dimensions, LVDS-to-CTT interfacing, and mechanical mounting procedures

Leveraging compatibility between HyperTransport drivers and LVDS receivers, a custom breakout board was designed to physically interface Xbox motherboard traces with an FPGA-based data logger. **Electronic Interfacing** and **PCB Design**, including **LVDS (Low Voltage Differential Signaling)**, a high-speed data transfer standard, and **CTT (Center-Tapped Termination)** signaling, a current-mode convention used in specialized hardware. It also involves **Mechanical Engineering** for PCB mounting (bevelling, sanding) and **Chemical Engineering** for structural integrity (epoxy properties).

Uses the HyperTransport bus because it has the minimum number of signals (10 differential pairs).

- **Physical Tap**: HyperTransport traces on the Xbox motherboard are parallel and evenly spaced (6 mil trace width, 12-13 mil spacing). These were measured with calipers to create a custom "tap board" with matching trace spacing.
- **Interfacing**: HyperTransport (600 mV swing, 600 mV common-mode) is directly compatible with LVDS receivers (100 mV sensitivity, 50 mV - 2.35 V common-mode).
- **Signal Conversion**: To interface with an existing CTT-signaling circuit board, **SN65LVDS386** (LVDS-to-CMOS converters) were used. CMOS drivers can effectively drive CTT-terminated lines because they can source/sink the required ±8 mA.
- **Mounting**: Tap board edges were bevelled and sanded for angled mounting. High-strength epoxy was used to secure it before removing temporary "alignment wires" and performing final soldering.

Trace spacing and bus width illustrate that the total physical size of a bus is the sum of all copper widths and their gaps. Precise measurement is vital for "docking" two PCBs. Even a few mils of deviation in pitch would lead to misalignment over 20 signals. This mechanical precision enables a bridge circuit without relying on messy point-to-point wiring, preserving signal integrity.

$$
W_{total} = (N_{traces} \cdot W_{trace}) + (N_{spaces} \cdot W_{space})
$$

- $W_{total}$: Total bus width, measured with calipers.
- $N_{traces}$: Number of traces, 20 wires (10 pairs).
- $W_{trace}$: Trace width, approx. 6 mil.
- $W_{space}$: Space width, approx. 12-13 mil.

**Current-Mode Overdriving**: Uses "antagonistic" drivers to cancel signals, enabling Man-in-the-Middle attacks. This is possible because HyperTransport drivers are current-regulated, not voltage-regulated.

**LVDS Compatibility**: A key insight is the electrical overlap between HyperTransport and TIA/EIA-644 (LVDS) standards, allowing the use of inexpensive off-the-shelf converter chips.

**Stub Length**: The warning to "keep eavesdropping lines as short as possible" is to avoid creating "stubs"—unterminated branches on a transmission line that cause reflections and signal degradation.

### Building the Data Logger and Data Interpretation, explaining the FPGA demuxing logic and the brute-force sliding-window histogram approach used to extract the RC-4/128 decryption key

Utilizing FPGAs to capture high-speed HyperTransport data, recovering bus bit-order via pattern matching, and extracting the Xbox RC-4 decryption key using brute-force sliding window histogram analysis involves **FPGA Design**, **Digital Signal Processing (DSP)**, and **Cryptanalysis**. Specifically, this includes **Clock-Domain Crossing** (demultiplexing high-speed data to slower internal clocks), **FIFO Buffering**, the **RC-4 Stream Cipher** (a symmetric encryption algorithm), and **Statistical Analysis** (using histograms to distinguish valid decrypted data from "white noise").

To record 400 MB/s data, a Xilinx Virtex-E FPGA was used.

- **Data Acquisition**: High-speed DDR data was demultiplexed into four phases of a quarter-speed clock (4x100 MHz SDR) for processing within the FPGA fabric. A "Do Not Store Zero" (DNSZ) feature was used to filter idle data.
- **Polarity and Order**: Signal polarity was fixed by assuming idle states are all zeros. Bit order was recovered by comparing the count of "1"s (Hamming weight) in captured bytes against known patterns from the FLASH ROM.
- **Boot Sequence**: Discovered that the HyperTransport bus resets twice. By triggering off the first reset, the reset vector `0xFFFF.FFF0` for the secret boot ROM was identified.
- **Key Extraction**: The RC-4 key was found by performing a brute-force search over the captured data stream. Sliding window data was treated as potential keys; the correct key was identified when the histogram of decrypted FLASH ROM showed bias (low entropy) rather than "white noise" (high entropy).

RC-4 Initialization (KSA - Key Scheduling Algorithm): The RC-4 cipher (as the target) first initializes a permutation S with a key K of length L. This key is used to scramble a sequence from 0-255 into a pseudo-random permutation. Since RC-4 is a symmetric cipher, the key must be stored in hardware (Southbridge/secret ROM) for the Xbox to decrypt its own boot code. Finding these "128 bits" is the ultimate goal of the hardware hack.

```text
for i=0 to 255:
    S[i]=i // Initialize permutation S
j=0
for i=0 to 255:
    j=(j+S[i]+K[imodL])mod256
    swap(S[i],S[j])
```

- $S$: State array (permutation), 256 bytes.
- $K$: Key, 128-bit (16 bytes).
- $L$: Key length, 16 bytes for RC-4/128.

Brute-force key search (`trykeys.exe`) code represents the "trykeys" program. It iterates through every possible 16-byte window in the captured data log, treats it as an RC-4 key, and checks if decrypted data looks like structured code. It relies on the property that ciphertext decrypted with the wrong key is indistinguishable from random noise (flat histogram), while valid code or data has a non-uniform distribution (biased histogram).

<details><summary>Algorithm: Brute-Force Key Search (trykeys.exe)</summary>

```c
for (offset = 0; offset < log_size - 16; offset++) {
    unsigned char *candidate_key = log_data + offset;
    decrypt_rc4(flash_rom, candidate_key, output_buffer);
    if (histogram_is_biased(output_buffer)) {
        printf("found possible key combo at offset %d
", offset);
    }
}
```

</details>

**Double Reset**: The major obstacle was realizing the system resets twice. This "hidden" state change is a common pitfall in hardware reverse engineering.

**Security through Obscurity**: Points out that using symmetric encryption (RC-4) means the key is "already inside the device." A public-key system would prevent this specific attack because the decryption key (private key) would never exist in the Xbox hardware. The impact of the Digital Millennium Copyright Act (DMCA) on security research; technical challenges are often overshadowed by legal ones.

---

## 3. Hardware Security and Supply Chain Attacks

Hardware security threats are categorized into direct passive and active attacks, including intrusive techniques such as optical inspection, side-channel analysis, Focused Ion Beam (FIB) modification, and fault injection. We already know that a **Hardware Security Module (HSM)** is a physical computing device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing. **Side-Channel Attacks** are security exploits involving gathering information about the operation of a cryptographic system (e.g., power consumption, electromagnetic leakage, timing) rather than attacking the algorithm itself. Physical implementations leak information that abstract algorithms do not. **Fault Injection (Glitching)** is a technique where an attacker intentionally introduces errors into a system (e.g., via voltage spikes, clock manipulation, or laser pulses) to disrupt its normal operation. This is often used to bypass security checks (e.g., skip a password verification instruction) or leak secrets (e.g., Differential Fault Analysis of cryptographic algorithms). **Focused Ion Beam (FIB)** is a scientific instrument similar to a Scanning Electron Microscope (SEM) that uses a focused beam of ions (typically gallium) to image, remove (ablate), or deposit material at nanoscale precision. In security, it allows for "circuit editing"—cutting wires and creating new connections on a silicon die.

**Direct physical tampering** (attacker has the hardware) differs from **indirect supply chain attacks** (attacker modifies hardware before delivery).

**1. Passive Attacks**: These involve minimal functional modification to the target system, attacking solely through observation.

- **Direct Observation**: Opening a chip and using optical or Scanning Electron Microscopes (SEM) to inspect silicon structures (e.g., e-fuses, ROM).
- **Runtime Measurement**: Probing internal buses (e.g., HyperTransport) to intercept plaintext keys or data moving between components.
- **Optical Emissions**: Detecting infrared photons emitted by switching transistors. These faint emissions can reveal internal states (e.g., memory content, key bits) without physical contact with the active region.
- **Power Side-Channels**: Measuring variations in power consumption. Since different operations or data values consume different amounts of power, these variations can be correlated with cryptographic operations to extract keys (Simple Power Analysis - SPA).
- **RF Side-Channels**: Measuring electromagnetic radiation from a device to infer its internal activity and extract keys.

**2. Active Attacks**: These involve modifying the system or its environment to alter its behavior.

- **FIB Circuit Editing**: Using a focused ion beam to cut traces on a chip or deposit jumpers, effectively rewiring it at the silicon level to bypass protection mechanisms or steal signals.
- **Fault Injection (Glitching)**:
- **Laser Fault Injection**: Using lasers to flip bits or induce logic failures in specific transistors.
- **Voltage/Clock Glitching**: Briefly dropping voltage or altering clock timing to cause instruction skips, branch mispredictions, or corruption of cryptographic state (e.g., causing an RSA signature to fail in a way that leaks the private key).
- **Rowhammer**: Exploiting electromagnetic coupling in high-density DRAM arrays. Rapidly accessing specific memory rows ("hammering") causes bit flips in adjacent rows, potentially corrupting security-critical data.
- **Microarchitectural Side-Channels**: Exploiting processor optimizations (e.g., speculative execution and caching) to leak information through timing differences (e.g., Spectre).

**"Passive" vs. "Non-Destructive" Attacks**: Passive attacks can still be destructive (e.g., decapsulating a chip destroys the package but allows for observation). The distinction is that passive attacks do not necessarily change the _execution_ of the logic, unlike active attacks which force the device into unintended states. **Attack Cost**: Does a hardware attack require a nation-state budget? Attacks can be performed with relatively low-cost equipment (e.g., a bus probe costing approx. $150). **Physics as a Vulnerability**: The described attacks rely on fundamental physical properties of semiconductors (photon emission, power consumption, EM coupling) which are difficult to eliminate entirely through software or simple hardware defenses.

### Supply Chain Tampering and Attacks, including component substitution, malicious implants (add-on chips), and the theoretical "ultimate" attack using Wafer-Level Chip-Scale Packages (WLCSP)

Indirect attacks, supply chain tampering methods range from simple component replacement to complex silicon-level implants that compromise hardware before it reaches the end user. **Supply Chain Attack**: An attacker modifies hardware or software components during manufacturing, distribution, or delivery, rather than attacking a deployed system directly. **Hardware Implants** are physical devices maliciously added to a system (e.g., extra chips, modified cables) designed to intercept data, provide backdoor access, or disrupt operation. **Application-Specific Integrated Circuit (ASIC)**: Chips customized for a specific use rather than general purpose, involving different ASIC design flows (e.g., "ASIC flow" vs. "Customer Owned Tooling (COT)") and their security implications. **Wafer-Level Chip Scale Packaging (WLCSP)**: A technology where solder balls are mounted directly to the silicon die, making the package size essentially the same as the die itself. This is an effective method for implants. **Through-Silicon Via (TSV)**: A vertical electrical connection (via) that passes completely through a silicon wafer or die. TSVs are used to create 3D integrated circuits (stacked chips), but they can be exploited to covertly insert malicious layers.

Supply chain tampering is a major threat because users often trust hardware based solely on labels or packaging ("Look at the label on the box"). For example, the "Blind Trust" problem: **Counterfeit/Substandard Parts**: A 2% failure rate in production was caused by "counterfeit" chips which were actually rejected Engineering Samples (marked "ES") that had been laser-ablated and relabeled as authentic parts. Factories accepted them because the text matched the Bill of Materials (BOM), highlighting the superficiality of standard inspections. **Interception**: For example, the NSA intercepting Cisco routers to implant beacon chips, and embedding recording devices in USB cables.

Categorizing attacks based on **Detection Difficulty** (easy to hard) and **Execution Difficulty** (easy to hard).

- **Replacing Components (Easy to execute / Hard to detect)**: Replacing passive components (resistors, capacitors) with identical-looking parts but different values. Can alter boot sequences (e.g., forcing fallback to unsecure interfaces like UART or JTAG) or modify regulator/clock stability to facilitate glitching attacks.
- **Adding Components (Medium execution / Easy to detect)**: Soldering extra chips or modules onto a board (e.g., a JTAG implant on a Dell server). Easier to detect via visual or X-ray inspection as it changes the expected appearance of the board.
- **In-Package Die-Stacking (Medium execution / Hard to detect)**: Embedding malicious dies within the standard package of another component. Examples include modifying microSD card controllers or stacking a malicious die on top of a legitimate one. **Detection**: X-rays are often insufficient as silicon is transparent to X-rays; only bond wires and lead frames are clearly visible. Requires advanced imaging (CT scans) or destructive analysis (cross-sectioning).
- **Integrated Circuit Modification (Hard to execute / Hard to detect)**: Modifying the chip design itself during manufacturing.
  - **Design Flow Risks**:
    - _ASIC Flow_: Handing over a Netlist/RTL to a third party (Design House) may allow them to insert backdoors (e.g., extra JTAG access) or Trojans.
    - _COT Flow_: Even if you own the masks, you must use "Hard IP" (RAM, PLL, I/O pads) provided by the foundry, which are black boxes and may contain hidden features.
  - **Mask Tampering**: Modifying lithography masks to change transistor properties (dopant-level attacks) or insert logic (using spare cells).
  - **The "Ultimate" Attack (WLCSP + TSV)**: Attack methods using Wafer-Level Chip Scale Packaging (WLCSP + TSV). An attacker can bond a thin malicious silicon layer to the back of a chip package using Through-Silicon Vias (TSVs). This "Man-in-the-Middle" attack is invisible to X-rays (silicon-on-silicon) and extremely difficult to detect without destroying the chip.

The **Check-Time vs. Use-Time (TOCTOU) problem** is a recurring theme. Even if a component is verified at the factory, it cannot be easily re-verified at the point of use without destroying it or using expensive equipment. **Limitations of X-ray Inspection**: Standard X-ray inspection is often over-trusted. It identifies metals well (lead frames, bond wires, solder) but silicon is largely transparent, making "die-on-die" or "die modification" attacks hard to find. **Economic Asymmetry**: Some attacks (e.g., component replacement) are extremely cheap and hard to detect, while detection (e.g., detailed destructive analysis) is expensive and destroys the sample, meaning you still haven't verified the _actual_ device you will use.

### Trustworthy Deniability and Verifiability: Explored the "Precursor" device, the role of open hardware/FPGAs in verification, and the implementation of a Plausibly Deniable Database (PDDB) to counter coercion

The hardware trust model emphasizes user verifiability and plausible deniability—using open hardware designs and encrypted filesystems where secrets are hidden to be indistinguishable from free space—to resist coercion and inspection. **Plausible Deniable Database (PDDB)** is a key-value store with a data structure that makes unallocated space and encrypted data indistinguishable, allowing users to disclose some keys without negating the existence of others.

The limitations of "Open Source" and "Trusted Foundries" in addressing supply chain attacks. Current verification methods (e.g., destructive analysis or complex supply chain audits) do not solve the **TOCTOU** problem.

1. **Three Principles of Verifiable Hardware**:

- **Complexity is the Enemy**: Simple designs are easier to verify. Contrast a simple analog headset (easy to verify) with a complex modern System on Chip (SoC) (nearly impossible to fully verify).
- **Verify the Whole System**: Verifying an encryption chip alone is insufficient if I/O paths (keyboard, screen) are compromised. The "Input Method Editor problem" highlights how software keyboards can log keystrokes before encryption.
- **Empower the End User**: Verification must happen at the point of use (in the user's hands), not just at the factory.

2. **The "Precursor" Device**: This device is designed for high reliability. Features include a physical keyboard (inspectable traces), a black-and-white LCD (pixel structure visible under a microscope), a simple PCB (transparent solder mask), and an FPGA-based SoC. **Verification Strategies**:

- **Visual Inspection**: Users can inspect the PCB and I/O components with simple tools.
- **FPGA as "Soft Silicon"**: Instead of relying on complex ASICs, open-source RTL code is compiled onto an FPGA, shifting trust from opaque silicon manufacturing to inspectable source code and bitstream generation.
- **Bitstream Randomization**: Recompiling an FPGA design with different seeds changes the physical layout of the logic. Hardware Trojans requiring specific cells would need to target a layout that moves with every compile, making universal implants much harder.

3. **Implementing Plausible Deniability**: Due to coercion (e.g., "rubber-hose cryptanalysis"), the user is the "weakest link." Security tools must account for legal/social pressure. Plausible deniability provides a "social" defense mechanism for users.

**Plausible Deniable Database (PDDB)**: Data is stored in "bases" (dictionary sets), the filesystem is initialized with random noise, and valid data is encrypted and scattered. A PDDB-locked base is computationally indistinguishable from free space (random noise) $Enc(K, M) \approx Random$. This is because the virtual address space is mapped via random permutations. Page Table Entries (PTE) are encrypted with AES-ECB (sacrificing some security for lookup performance/speed).

Actual data pages are encrypted with AES-GCM (Authenticated Encryption), $C, T = AES \text{-}GCM_{K}(IV, P, A)$, where $C$ is ciphertext and $T$ is the authentication tag. The system relies on the property that without $K$, $C$ looks like random bits. To avoid leaking usage patterns, writes are directed to a pool of "absolutely free" space (re-encrypted random noise) or overwrite existing space. When space is exhausted, Garbage Collection (GC) events re-encrypt valid data and re-randomize remaining space.

**Algorithm: PDDB Allocation (Concept)**

- **Initialization**: Fill the disk with random data.
- **Open**: User provides a set of keys $\{K_1, ..., K_n\}$.
- **Mount**: Attempt to decrypt headers using $\{K_i\}$. Success reveals "Base addresses."
- **Read/Write**:
  - _Read_: Lookup virtual or physical address. Decrypt page.
  - _Write_: If updating, write to a new location in the "Free Pool."
  - _Free Pool Management_: Periodically re-encrypt legitimate free space to ensure writes do not cause detectable entropy "patterns."
- **Denial**: If coerced, provide a subset $\{K_{public}\}$. Since encrypted data for $\{K_{hidden}\}$ is indistinguishable from unallocated random noise, the existence of $\{K_{hidden}\}$ cannot be proven.

**Performance Tradeoffs**: PDDB uses AES-ECB for page tables to enable fast lookups, acknowledging it is weaker than GCM mode but necessary for resource-constrained FPGAs (100MHz CPU). **"Social Security"**: Directly integrating social engineering defense (plausible deniability) into the hardware/software architecture. **Verification Limits**: Even with FPGAs, one must trust the FPGA vendor (Xilinx/Lattice) and the toolchain. However, the attack surface is reduced from "Trust Foundry + Design House + Courier" to "Trust FPGA Vendor + Toolchain."

---

## 4. Defending Against Physical Threats

Physical security of a secure coprocessor is achieved through a multi-layered strategy including tamper evidence, tamper resistance, detection, and active response to ensure hardware integrity and the safety of confidential information.

The fundamental classification for **Physical Security** of cryptographic modules draws from standards like **FIPS 140-1 Level 4**, representing the highest level of security, requiring complete physical protection against physical intrusion. This includes:

- **Tamper Evidence**: Mechanisms that leave permanent, visible marks if a device is opened.
- **Tamper Resistance**: Hardening a device physically (e.g., steel casing) to increase the difficulty of intrusion.
- **Tamper Detection**: Active sensors capable of perceiving an attack in progress.
- **Tamper Response**: Pre-programmed actions triggered upon detecting tampering (e.g., memory erasure).

While evidence and defense mechanisms are useful, they cannot withstand dedicated, high-performance attacks. Therefore, the **interleaving of defense and detection/response mechanisms** is vital. The goal is to make penetration difficult enough that device responses are triggered before secrets can be extracted. Physical security is an ongoing "race" between defenders and attackers; no system can "prove" absolute tamper resistance. There are no provably absolute tamper-resistant systems; security is a race of risk mitigation, not an absolute state, and there is a clear conflict between high security and the cost of production-ready systems. Specific devices can be evaluated against the FIPS 140-1 Level 4 standard, which implies **Environmental Failure Protection (EFP)** or **Environmental Failure Testing (EFT)**.

### Penetration Detection, including conductive grids, "gift-wrapping" techniques, and the use of chemically-matched potting materials

Penetration detection is achieved through a layered "gift-wrap" style grid of non-metallic conductors embedded in chemically similar potting material, ensuring any drilling or probing attempts alter the electrical characteristics of the grid.

**Active Shielding** techniques:

- **Conductive Grid**: A mesh of fine wires or traces monitored for opens (breaks), shorts, or changes in resistance/capacitance.
- **Potting Material**: A hard epoxy or resin that encapsulates electronic components. In high-security designs, this material is often **friable** or chemically modified so that it cannot be removed without destroying the underlying conductive grid.
- **Electromagnetic Compatibility (EMC)**: Using grounded shields to block **Electromagnetic Interference (EMI)** and prevent **TEMPEST** attacks (eavesdropping on unintended RF emissions).

The detection strategy uses a layered design, forcing an attacker to repeat complex tasks with low success probability.

- **Sensor Grid**: A grid made of non-metallic conductors (making them hard to distinguish from the epoxy) that monitors for physical changes. These grids are flexible, allowing them to completely wrap the package, avoiding standard connectors that would leave holes in the security envelope.
- **Potting and Shielding**: The package is embedded in a potting material physically and chemically similar to the grid conductors. This "material camouflage" prevents an attacker from removing the resin without damaging the sensor grid. Finally, a grounded shield prevents electromagnetic side-channel attacks.

**Inductive/Capacitive Monitoring**: Compared to DC (resistance), AC (capacitance/inductance) monitoring is generally harder to bypass with "jumpers." **Packaging as Security**: The transition from "gift-wrap" to "potting" highlights that the physical packaging itself is a security sensor in this architecture.

### Tamper Response, focusing on the hardware "crowbar" circuit that shorts BBRAM power to ground to rapidly erase secrets

Upon detecting tampering, the device immediately shorts its battery-backed SRAM power to ground and isolates its lines, "zeroizing" its confidential information to prevent data recovery. This involves **Active Defense Measures** and **Volatile Memory Clearing**.

- **BBRAM (Battery-Backed RAM)**: A static RAM that uses a small battery to maintain data when main power is lost. Highly volatile, making it ideal for "zeroization."
- **Zeroization**: The process of erasing sensitive information (cryptographic keys) so it cannot be recovered.
- **Shorting Circuit**: A protection mechanism that shorts power lines to ground, often used to blow fuses or rapidly discharge a storage chip's voltage to erase its contents.
- **High Impedance (Hi-Z)**: A state where a pin is electrically disconnected from the circuit, preventing "back-powering" (where a chip stays powered through parasitic power on data pins).

The primary response measure is erasing keys stored in BBRAM.

- **Always Active**: Detection and response circuitry share the same battery as the BBRAM, ensuring protection even when the coprocessor is powered down.
- **Rapid Erasure**: To ensure memory is cleared before an attacker can intervene, the device "shorts" the SRAM's power connection to ground.
- **Isolation**: Simultaneously, all address, data, and control lines are switched to a high-impedance state. This prevents the SRAM from being accidentally powered by the CPU or other peripherals during erasure. This hardware-level response does not rely on the CPU's ability to execute code.

**Hardware Autonomy**: Using shorting circuits and high-impedance states is a critical "fail-safe" design; it ensures the erasure operation completes even if the CPU crashes or is tampered with. **Tamper Persistence** ensures that after a device is tampered with, it remains permanently in a "reset" state, effectively "bricking" the device to prevent further misuse.

### Detecting Other Physical Attacks, such as "Frozen RAM" attacks (temperature), ionizing radiation, data imprinting, and clock/voltage glitching

Secure systems incorporate environmental sensors for temperature, radiation, voltage, and clock frequency to prevent "fault injection" attacks that could bypass zeroization or cause the CPU to leak confidential information.

**Environmental Failure Protection (EFP)**:

- **Data Remanence (Frozen RAM Attack)**: At extremely low temperatures, SRAM cells can maintain their state even without power (the "vampire" effect).
- **Ionizing Radiation**: High-energy particles can flip bits in memory (Single Event Upsets) or, at high doses, cause memory to "latch-up" and retain data.
- **Data Imprinting (Aging)**: If a bit is set to "1" for a long period, physical changes in the silicon may cause that "1" to persist permanently, even after erasure.
- **Fault Attacks**: Manipulating voltage or clock signals to make a processor skip instructions (e.g., security checks).

Several environmental threats and their countermeasures:

- **Temperature and Radiation**: Low temperatures (preventing erasure) and ionizing radiation trigger immediate zeroization. High temperatures (potentially causing CPU damage/faults) keep the device in reset.
- **Data Imprinting**: Software protocols periodically invert data in SRAM to prevent physical "aging."
- **Voltage Monitoring**: The device monitors for undervoltage (causing CPU reset) and overvoltage (treated as a tamper event). It also manages switching between main power and the battery.
- **Clock Integrity**: Internal **Phase-Locked Loops (PLLs)** and independent clocks are used to detect and prevent "clock glitches" (extra pulses or frequency shifts) that could be used for instruction skipping.

**Inversion Protocols**: Periodic "inversion" of data is a clever software-level mitigation for hardware-level remanence. **Overvoltage as a Tamper Method**: Unlike undervoltage (which just causes a reset), overvoltage is treated as a malicious attempt to stress hardware and triggers a destructive response (zeroization). **The Role of PLLs**: Using PLLs is not just for frequency synthesis; they also act as "filters" for the clock, making it much harder for an attacker to inject a single malicious pulse.
