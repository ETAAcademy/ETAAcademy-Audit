# ETAAcademy-Audit: 29. Abstraction Layers

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>29 AL</td>
    <td>
      <table>
        <tr>
          <th>audit</th>
          <th>basic</th>
          <th>article</th>
          <td>AL</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-Audit](https://github.com/ETAAcademy/ETAAcademy-Audit)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# From Transistors to Abstractions: Engineering the Illusion of Simplicity

The computer system constructs a multi-layered abstraction pyramid extending from physical hardware to high-level applications, effectively shielding underlying complexities through a layered architecture. At the foundation, hardware exists as a physical entity capable only of executing machine language based on electrical signals—the sole instruction set directly intelligible to circuits.

To harness these complex hardware resources, the Operating System (OS) serves as the primary layer of core abstraction. Through efficient software algorithms (such as shift-and-add for multiplication/division and Bresenham’s algorithm for graphics rendering) and memory management mechanisms (like managing heap memory via free lists and enabling direct memory access through address mapping), the OS encapsulates raw CPU cycles, physical memory, and I/O devices into high-level concepts like processes, files, and system calls, providing a unified and stable execution environment for upper-layer software.

Above this, the Virtual Machine (VM) introduces a higher level of abstraction. By adopting a Stack Machine architecture and a two-stage compilation strategy (High-Level Language → VM Code → Machine Language), it constructs a virtual instruction set independent of specific hardware. Utilizing global stack frames to manage complex function calls and resource allocation, the VM not only achieves "write once, run anywhere" cross-platform compatibility but also strikes a perfect balance between compilation simplicity and code expressiveness.

The crucial hub connecting these abstraction layers is the Compiler. Its frontend parses high-level source code, which resembles natural language, into structured token streams and syntax trees via tokenizers and syntax analyzers. Meanwhile, the backend code generator maps variables to VM memory segments using symbol tables, converts infix expressions into postfix instructions via stack operations, and flattens complex control flows (like if/while) into linear jump logic, ultimately transforming human logical thinking into machine-executable instruction sequences.

This entire mechanism—hardware executing instructions, OS managing resources, VM abstracting differences, and compilers translating logic—collectively forms a precisely coordinated system that enables modern software to run efficiently, securely, and across platforms.

## 1. From Hardware to Virtual Machines: How Programs Really Run

At the very bottom of the computing world lies **hardware**—pure physical reality. Circuits, transistors, voltage levels. Hardware understands nothing but electrical signals: current flows or it doesn’t, voltages rise or fall. Everything it does follows the laws of physics, not logic, intention, or meaning.

To make these cold circuits _do something useful_, humans invented **machine language**. Machine language is a carefully designed sequence of bits—zeros and ones—that the hardware can directly execute. Each pattern tells the CPU to perform a specific action: add two numbers, jump to another instruction, read or write memory. This is the _only_ language the hardware truly understands.

But living at the level of raw bits is brutally difficult. Writing real programs directly in machine language is slow, error-prone, and mentally exhausting. To escape that world, we built a powerful layer above it: the **operating system**.

The operating system acts like a city administrator. Sitting on top of machine language, it manages all hardware resources—CPU time, memory, disks, and networks—and organizes them into clean, human-friendly abstractions: **processes**, **threads**, **files**, and **system calls**. Programs no longer need to know which physical wire or memory chip they are using. The OS hides that chaos and provides order.

Above the operating system, we often add yet another abstraction: the **virtual machine (VM)**. A VM does not control real hardware directly. Instead, it defines an _imaginary computer_—a virtual instruction set or virtual hardware model. Examples include the JVM, the EVM, or teaching VMs like the Hack VM. As long as a program obeys the VM’s rules, it can run anywhere that VM is implemented—on Windows, Linux, macOS, or even on a blockchain.

Connecting human thought to all these layers is the **compiler**. A compiler understands high-level languages like C, Rust, or Solidity, and gradually translates human-readable logic into something machines can execute. Sometimes the output is real machine code. Other times, it is **VM bytecode**, which is later translated again into machine instructions.

In essence:

- **Compilers** translate ideas into instructions.
- **Operating systems and virtual machines** manage and abstract execution.
- **Hardware** executes everything in the end.

### 1.1 The Stack Machine: A Perfect Trade-off

Different devices use different processors, and each processor has its own machine language. If we wanted one high-level language to run everywhere, we would need a separate compiler for every hardware platform—_N platforms, N compilers_.

The solution is **two-stage compilation**. Instead of compiling directly to machine code, we introduce a middleman: the virtual machine.

```
High-level language → VM code → Machine code
```

Now we only need:

- One compiler from the high-level language to VM code.
- One VM translator per hardware platform.

The VM translator’s job is simple: read VM instructions and output equivalent assembly instructions. For example, a VM instruction like `push constant 7` expands into several assembly instructions that manipulate memory and registers step by step.

At the heart of many VMs lies a **stack-based architecture**. When designing a VM, there is a fundamental trade-off:

- **Closer to high-level languages** → easier compiler
- **Closer to machine language** → easier translator

The **stack machine** turns out to be an excellent balance. It is simple enough to translate into machine code, yet expressive enough to represent complex high-level logic.

In a stack machine:

- Data is pushed onto a stack.
- Operations implicitly use the top elements of the stack.
- No need to specify registers or memory addresses for operands.

The stack pointer (**SP**) always points to the next free slot at the top of the stack.

#### Arithmetic and Logic Instructions

The first category of VM instructions is **arithmetic and logic operations**, including:

- **Arithmetic**: `add`, `sub`, `neg`
- **Comparison**: `eq`, `gt`, `lt` (return true/false)
- **Logic**: `and`, `or`, `not`

All follow the same rule:

- **Pop** the required operands from the stack.
- **Compute** the result.
- **Push** the result back onto the stack.

**Example: computing `x = 17 + 19`**

VM instructions:

- `push 17` → stack: `[17]`
- `push 19` → stack: `[17, 19]`
- `add` → pop 19 and 17, compute 36, push → `[36]`
- `pop x` → store 36 in `x`, stack becomes empty

#### Memory Segments and Variable Lifetimes

High-level languages have variables with different **scopes** and **lifetimes**:

- **Static**: exist for the entire program
- **Local**: exist during a function call
- **Argument**: function inputs
- **Field (`this`)**: object properties

If a VM exposed only raw memory, the compiler would have to manage all this complexity itself. Instead, the VM provides **eight virtual memory segments** that map directly to high-level concepts.

Key segments include:

- `local`
- `argument`
- `static`
- `this` / `that` (for objects and arrays)

All memory access uses just two commands:

- `push segment index`
- `pop segment index`

Direct memory-to-memory copying is forbidden. The **stack is always the intermediary**.

#### From Theory to Implementation: The VM Translator

At this point, abstraction meets reality. We build a **VM translator** that reads one line of VM code and emits multiple lines of assembly code.

The entire illusion of a stack machine is implemented with:

- A stack pointer stored in memory
- A region of RAM used as stack storage

By convention:

- **RAM[0]** stores the stack pointer (SP)
- The stack begins at **RAM[256]**

If `RAM[0] = 256`, the next push will store data at `RAM[256]`.

**Example: Translating `push constant 17`**

The VM command:

```
push constant 17
```

Conceptually means:

```
*SP = 17
SP++
```

Assembly implementation:

Prepare the value:

```asm
@17
D=A
```

Store it at the stack top:

```asm
@SP
A=M
M=D
```

Advance the stack pointer:

```asm
@SP
M=M+1
```

#### How VM Memory Maps to Hardware RAM

Behind the scenes, VM memory segments map to specific RAM regions:

- `RAM[0]` → `SP`
- `RAM[1–4]` → `LCL`, `ARG`, `THIS`, `THAT`
- `RAM[5–12]` → `temp`
- `RAM[13–15]` → internal VM registers
- `RAM[16–255]` → `static` variables
- `RAM[256–2047]` → **stack**
- `RAM[2048+]` → **heap** (objects and arrays)

Dynamic segments compute addresses as:

```
addr = base_pointer + index
```

Temporary and static segments use fixed or symbol-based addresses.

---

### 1.2 Beyond Computation and Memory: Control Flow in a Virtual Machine

A virtual machine does more than _compute_ (addition, subtraction) and _remember_ (memory access). To be a real execution model, it must also be able to **control** the order of execution. This capability—**control flow**—allows programs to make decisions, repeat actions, and structure logic into reusable units. In a VM, control flow is realized through **branching** and **functions**.

#### Branching: Escaping Linear Execution

Without branching, a program would execute strictly top to bottom, line by line. Branching enables **conditional execution**, **loops**, and **if–else** logic. VM languages deliberately keep branching minimal and explicit, relying on just three commands:

- `label labelName`
  Defines a symbolic destination for jumps.

- `goto labelName`
  An **unconditional jump**. Execution immediately continues at `labelName`.

- `if-goto labelName`
  A **conditional jump**. The VM pops the top value of the stack; if it is non-zero, execution jumps to `labelName`.

Before using `if-goto`, the condition must already have been evaluated and pushed onto the stack as a boolean value (true/false, non-zero/zero).

#### A Loop in Practice: Multiplication by Repeated Addition

Consider a simple high-level loop:

```python
while (n <= y):
    sum = sum + x
    n = n + 1
```

The compiler lowers this structure into a combination of labels and jumps:

```vm
label LOOP
push n
push y
gt                  // compute (n > y)
if-goto END_LOOP    // if true, exit loop

// loop body: sum = sum + x; n = n + 1

goto LOOP
label END_LOOP
```

Note the subtle inversion: although the high-level logic says “continue while `n <= y`,” the low-level implementation usually checks the opposite condition—“if `n > y`, exit.” This inversion simplifies branching and is common in compiler design.

#### From VM Branching to Machine Instructions

Translating VM branching commands into machine-level assembly is straightforward:

- `label XXX` → `(XXX)`
- `goto XXX` → `@XXX` followed by an unconditional jump
- `if-goto XXX` → pop the stack into a register, then jump to `XXX` if the value is non-zero

Branching is thus nothing more than carefully controlled jumps guided by stack values.

#### Functions: Structured Control Flow

While branching controls _where_ execution goes, **functions** control _how_ execution is organized. A function call introduces a new execution context, and returning from a function restores the previous one.

From the caller’s perspective, a function obeys three simple rules:

- **Push arguments** onto the stack.
- **Call** the function.
- **Receive the result**: the arguments disappear, replaced by the return value.

To support this model, the VM defines three function-related commands:

- `function functionName nVars`
  Declares a function and the number of local variables it needs. These locals must be allocated and initialized to zero when the function starts.

- `call functionName nArgs`
  Calls a function with `nArgs` arguments taken from the stack.

- `return`
  Returns control to the caller. Before executing `return`, the function must push a return value onto the stack—even if that value is just `0`.

Importantly, `nArgs` (number of arguments passed) and `nVars` (number of local variables) are entirely different concepts, serving different roles in execution.

#### What Happens During a Function Call?

When a `call` occurs, control moves from the **caller** to the **callee**, but not before the caller’s execution context is carefully preserved.

##### On Call (Caller → Callee)

- **Pass arguments**: the caller has already pushed them onto the stack.
- **Save the current frame**: the VM pushes five critical values onto the stack, in order:

  - Return address
  - Saved `LCL`
  - Saved `ARG`
  - Saved `THIS`
  - Saved `THAT`
    (Global segments like `static` and `temp` do not need saving.)

- **Reposition pointers**:

  - `ARG = SP - nArgs - 5`
  - `LCL = SP`

- **Jump** to the function’s entry point.

##### Nested Calls and the Global Stack

Function calls naturally nest: `foo` calls `bar`, which calls `sqrt`. Each active function has its own _private world_—its locals, arguments, and pointers—yet all of them live inside one **global stack**.

This structure follows a strict **last-in, first-out (LIFO)** rule, just like stacking plates. Each function call creates a **stack frame**, a contiguous block on the stack that stores everything needed to resume the caller later.

##### How the VM Translates `call`

When translating a `call` instruction, the VM translator generates assembly that performs the following steps:

- Push a unique **return address** label.
- Push the current `LCL`.
- Push the current `ARG`.
- Push the current `THIS`.
- Push the current `THAT`.
- Set `ARG = SP - nArgs - 5`.
- Set `LCL = SP`.
- Jump to the function’s label.
- Declare the return label so execution can resume there.

#### Function Definition

When encountering a `function` command, the translator:

- Inserts a label marking the function’s entry point.
- Pushes `nVars` zeros onto the stack to initialize the local variables.

This ensures that each function starts with a clean, predictable local environment.

#### Returning from a Function (Callee → Caller)

Returning is the most delicate operation, because the VM must carefully dismantle the callee’s frame and restore the caller’s world.

- **FRAME = LCL**
  Save the current frame base.
- **RET = (FRAME - 5)**
  Retrieve the return address.
- **Pop return value into `ARG`**
  Overwrite the caller’s first argument with the return value.
- **SP = ARG + 1**
  Restore the caller’s stack pointer.
- Restore saved pointers in reverse order:

  - `THAT = (FRAME - 1)`
  - `THIS = (FRAME - 2)`
  - `ARG  = (FRAME - 3)`
  - `LCL  = (FRAME - 4)`

- **Jump to RET**
  Resume execution in the caller.

---

## 2. From High-Level Code to Machine Instructions: Two-Tier Compilation

Translating a high-level programming language into machine language is a complex process, and modern systems rarely do it in a single step. Instead, many languages—such as Java and C#—adopt a **two-tier (two-stage) compilation model**.

In this model, compilation is divided into two clearly separated phases:

- **Frontend (Compiler)**
  High-level source code → **VM code** (intermediate representation)

- **Backend (VM Translator)**
  VM code → **machine language**

This separation brings portability and simplicity: the frontend only needs to understand the language, while the backend only needs to understand the target machine.

### 2.1 The Compiler Frontend: Understanding the Program

The frontend’s job is not to run the program, but to **understand its structure** and express it in a machine-friendly form. It typically consists of three major components:

- **Tokenizer (Lexical Analyzer)**
  Breaks raw source code into meaningful units called _tokens_.
- **Parser (Syntax Analyzer)**
  Checks whether the token sequence follows the language’s grammar and builds a structured representation.

- **Code Generator**
  Uses the parsed structure to emit actual VM instructions.

In many educational compilers, the syntax analyzer outputs an **XML-formatted parse tree**. This XML is not used for execution; it exists to verify that the compiler correctly understands the program’s grammatical structure.

#### Lexical Analysis: Turning Characters into Tokens

**Lexical analysis**, or tokenization, converts a stream of characters into a stream of **tokens**—the smallest meaningful units of a language.

Most high-level languages define a small, fixed set of token categories. A typical VM-oriented language uses five:

- **Keywords**
  Reserved words such as `class`, `constructor`, `function`, `if`, `while`.

- **Symbols**
  Single-character tokens like `{`, `}`, `(`, `)`, `+`, `-`, `<`, `>`.

- **Integer constants**
  Numeric literals such as `0`, `123`.

- **String constants**
  Quoted strings like `"hello"`.

- **Identifiers**
  All remaining names: variable names (`count`), class names (`Main`), function names (`run`).

**Example**

Source code:

```c
if (x < 0) // check negative
```

Token stream:

```
if, (, x, <, 0, )
```

XML output:

```xml
<keyword> if </keyword>
<symbol> ( </symbol>
<identifier> x </identifier>
<symbol> &lt; </symbol>
<integerConstant> 0 </integerConstant>
<symbol> ) </symbol>
```

Note that special characters like `<` must be escaped in XML.

#### Syntax Analysis: From Tokens to Grammar

Once tokenized, the program must be checked against the language’s **grammar**. Grammar rules describe how tokens combine into valid constructs.

Grammars are made of two kinds of rules:

- **Terminal rules**
  Their right-hand side contains only concrete tokens, for example:

  ```
  op: '+' | '-' | '*' | '/'
  ```

- **Non-terminal rules**
  Their right-hand side references other grammar rules:

  ```
  expression: term op term
  ```

Non-terminal rules are often **recursive**, allowing nested structures. For instance, an `if` statement contains `statements`, and those statements may themselves include another `if`.

**Example Grammar Structure**

- A **program** consists of a sequence of **statements**.
- A **statement** may be one of:

  - `ifStatement`
  - `whileStatement`
  - `letStatement`

An `ifStatement` is defined as:

- Begins with `if`
- Followed by `(`
- Followed by an `expression`
- Followed by `)`
- Followed by `{`
- Followed by zero or more `statements`
- Ends with `}`

#### Parsing and Syntax Errors

**Parsing** is the process of checking whether a given token stream conforms to the grammar. The parser walks through the tokens, rule by rule, matching expectations.

If even a single symbol—such as a missing `)` or `{`—is absent, the tokens themselves may still be valid, but the **parse fails**, and the compiler reports a **syntax error**.

#### Parse Trees and XML Representation

A **parse tree** is a tree-shaped representation of a program’s grammatical structure:

- The **root** corresponds to the highest-level rule (e.g., `class` or `statement`).
- **Internal nodes** represent non-terminal rules (`expression`, `term`, `ifStatement`).
- **Leaves** are terminal tokens (`identifier`, `symbol`, `integerConstant`).

In XML form:

- Each non-terminal rule is wrapped in `<rule> ... </rule>`.
- Each terminal token is wrapped in a type-specific tag, such as `<identifier>` or `<integerConstant>`.

<details><summary>Example: count + 1</summary>

```xml
<expression>
  <term>
    <identifier> count </identifier>
  </term>
  <symbol> + </symbol>
  <term>
    <integerConstant> 1 </integerConstant>
  </term>
</expression>
```

</details>

The nested tags precisely mirror the tree structure of the expression.

#### Parsing Logic as Code

A common compiler design pattern is **one grammar rule, one method**. For every non-terminal symbol, the compiler defines a corresponding function:

- `whileStatement` → `compileWhile()`
- `expression` → `compileExpression()`
- `term` → `compileTerm()`

These methods call one another recursively, following the grammar. For example, `compileWhile()` may call `compileExpression()`, which may in turn call `compileTerm()`.

#### LL(1) Grammars and Predictive Parsing

Many teaching languages are designed to be **LL(1)**, meaning the parser needs to look at **only one token** to decide what to do next—no backtracking required.

**Examples:**

- Seeing `while` → must be a `whileStatement`
- Seeing `if` → must be an `ifStatement`
- Seeing `let` → must be a `letStatement`

Natural languages do not have this property. In English, encountering a word like _“lift”_ does not immediately reveal whether it is a verb or a noun—you must read further. This ambiguity is one reason natural language processing is far harder than compiling programming languages.

#### The `eat()` Pattern

Rather than manually checking tokens everywhere, parsers usually define a helper method:

```text
eat(expectedToken)
```

- If the current token matches, the parser:

  - Outputs the corresponding XML
  - Advances to the next token

- If it does not match, the parser reports a syntax error

This is the precise point at which a compiler detects malformed code.

**Example: Parsing a `while` Statement**

Grammar rule:

```
whileStatement: 'while' '(' expression ')' '{' statements '}'
```

<details><summary>Parsing logic (pseudocode)</summary>

```python
def compileWhile():
    write("<whileStatement>")

    eat('while')
    eat('(')
    compileExpression()
    eat(')')
    eat('{')
    compileStatements()
    eat('}')

    write("</whileStatement>")
```

</details>

#### Lookahead and the Special Case of `term`

Most LL(1) decisions are straightforward—but **terms** are an exception. When the parser sees an **identifier**, it cannot immediately know whether it represents:

- A variable: `foo`
- An array access: `foo[i]`
- A subroutine call: `foo.bar()` or `foo()`

To resolve this ambiguity, the parser uses **lookahead**—peeking at the next token:

- Next token is `[` → array access
- Next token is `.` or `(` → subroutine call
- Otherwise → simple variable

This limited lookahead preserves efficiency while allowing expressive syntax.

---

### 2.2 Code Generation: Turning Understanding into Execution

At this point, the syntax analyzer has already done its job. It can read high-level source code and produce a well-formed XML parse tree, proving that the compiler _understands_ the structure of the program. The next step is to transform this understanding into something executable. This is the role of **code generation**.

In the code generation phase, we evolve the XML generator into a **Code Generator**: instead of emitting XML, it emits **VM code** (`.vm` files). That VM code can later be translated into machine language by a VM translator and finally run on real hardware.

To keep the problem manageable, we do not compile an entire project at once. Instead, we focus on compiling **one class at a time**. Inside a class, apart from a small number of class-level variable declarations, most of the work consists of compiling individual **subroutines**—methods, functions, and constructors. Each subroutine can be compiled largely independently.

#### Bridging the Semantic Gap

The central challenge of code generation is this:
**How do we express rich, high-level language semantics using a very small set of VM instructions?**

High-level languages support concepts such as:

- Variables
- Expressions
- Control flow (`if`, `while`)
- Objects and methods
- Arrays

The VM language, by contrast, provides only:

- Stack operations (`push`, `pop`)
- Basic arithmetic and logical operations
- Function calls
- Simple branching (`goto`, `if-goto`, `label`)

Code generation is the art of _lowering_ high-level constructs into these low-level primitives:

- Variables are mapped to VM **memory segments** (`local`, `argument`, `this`, `that`, `static`).
- Expressions are evaluated using **postfix (stack-based) computation**.
- Control flow is implemented with labels and jumps.
- Objects and arrays are handled through **heap memory** and indirect addressing.

#### Handling Variables with Symbol Tables

Variables in source code have meaningful names—`x`, `sum`, `point`—but the VM understands only memory locations. The compiler bridges this gap using **symbol tables**, which map identifiers to concrete VM locations.

For example:

- Source-level `x` → VM-level `local 0`
- Source-level `sum` → VM-level `this 2`

To handle scope correctly, the compiler maintains **two levels of symbol tables**:

- **Class-level table (Class Scope)**
  Stores:

  - `static` variables
  - `field` variables
    These are valid throughout the entire class.

- **Subroutine-level table (Subroutine Scope)**
  Stores:

  - `argument` variables
  - `local` variables
    This table is created when compiling a new subroutine and discarded afterward.

For methods, `argument 0` is _reserved_ for the implicit `this` reference. Every time a new method is compiled, this convention is enforced automatically.

When the compiler encounters a variable name, it follows a simple lookup rule:

- Check the subroutine-level table (locals and arguments).
- If not found, check the class-level table (fields and statics).

This mapping mechanism is how the compiler “recognizes” and “locates” variables.

#### Compiling Expressions: From Infix to Postfix

Humans write expressions in **infix notation**, such as:

```
a + b
```

Stack machines, however, prefer **postfix notation** (Reverse Polish Notation):

```
push a
push b
add
```

The compiler’s task is to convert infix expressions into a sequence of stack-based VM instructions. Conceptually, this is equivalent to performing a **post-order traversal** of the expression’s parse tree: evaluate operands first, then apply the operator.

**Example: `x + g(2, y)`**

Compilation proceeds as follows:

- The root operator is `+` (a binary operation).
- Compile the **left operand** `x`:

  - Generate `push local 0` (assuming `x` is `local 0`).

- Compile the **right operand** `g(2, y)`:

  - Push argument `2`: `push constant 2`
  - Push argument `y`: `push local 1`
  - Call the function: `call g 2`

- Apply the operator:

  - Generate `add`

Final VM code:

```vm
push local 0
push constant 2
push local 1
call g 2
add
```

#### Control Flow: Flattening Structured Logic

High-level languages use **structured control flow**—nested blocks like:

```c
if (exp) { ... } else { ... }
while (exp) { ... }
```

The VM language, like assembly, relies on **unstructured jumps**. The compiler’s job is to flatten structured constructs into linear sequences of labels and branches, while preserving semantics.

**Key Techniques**

- **Condition negation**: invert conditions to simplify jumps.
- **Automatic label generation**: ensure every jump target is unique.
- **Explicit control transfer**: replace blocks with `goto` and `if-goto`.

#### Translating `if–else`

For:

```c
if (exp) { s1 } else { s2 }
```

The standard translation pattern is:

- Compile `exp`.
- Negate the condition (`not`).
- `if-goto L1` (jump to `else` if condition is false).
- Compile `s1`.
- `goto L2` (skip the `else` block).
- `label L1` (start of `else`).
- Compile `s2`.
- `label L2` (end).

Negating the condition early makes the control flow clearer and easier to generate.

#### Translating `while`

For:

```c
while (exp) { s }
```

The translation template is:

- `label L1` (loop start)
- Compile `exp`
- `not`
- `if-goto L2` (exit loop if condition fails)
- Compile `s`
- `goto L1`
- `label L2` (loop end)

#### Objects and Indirect Addressing

All object access ultimately boils down to **base + offset** addressing. The compiler achieves this by carefully manipulating the `pointer` segment.

- `this` is a virtual segment for accessing fields of the **current object**.
- `that` is typically used for **array access**.
- The real control lies in the `pointer` segment:

  - `pointer 0` → `THIS`
  - `pointer 1` → `THAT`

**Example: Pointing `this` to Address 8000**

```vm
push constant 8000
pop pointer 0
```

Now:

- `push this 0` reads `RAM[8000]`
- `pop this 1` writes `RAM[8001]`

#### Object Construction

Constructing an object is a coordinated dance between caller and callee.

From the caller’s perspective:

```c
let p1 = Point.new(2, 3);
```

- Push arguments: `2`, `3`
- Call constructor: `call Point.new 2`
- Store returned address in `p1`

Inside the constructor:

- **Compute object size** (e.g., 2 fields).
- **Allocate memory**: `call Memory.alloc 2`
- **Anchor `this`**: `pop pointer 0`
- **Initialize fields**:

  - `pop this 0`
  - `pop this 1`

- **Return the object reference**:

  - `push pointer 0`
  - `return`

#### Method Calls: Making `this` Explicit

In object-oriented syntax:

```c
obj.method(x, y)
```

At the VM level, this becomes a plain function call:

```c
method(obj, x, y)
```

The compiler handles this by:

- **Caller side**:

  - Push the object reference first.
  - Push user arguments.
  - Call with `nArgs + 1`.

- **Callee side**:

  - First instruction:
    `push argument 0; pop pointer 0`
  - This initializes `this` for the method.

Void methods still push a dummy return value (`0`) to keep the stack consistent.

#### Handling Arrays and Assignment Conflicts

Array access uses the `that` segment:

```
targetAddress = base + index
```

A subtle challenge arises with assignments like:

```c
a[i] = b[j]
```

Both sides require `pointer 1`. If handled naively, one address overwrites the other.

**Solution: Use the `temp` Segment**

A safe compilation strategy is:

- Compute `a + i`, leave it on the stack.
- Compute `b + j`, store it in `pointer 1`.
- Read `that 0`, store value in `temp 0`.
- Restore `a + i` into `pointer 1`.
- Push `temp 0`, then `pop that 0`.

**General Algorithm**

For:

```c
arr[expression1] = expression2
```

Generate:

- `push arr`
- Compile `expression1`
- `add`
- Compile `expression2`
- `pop temp 0`
- `pop pointer 1`
- `push temp 0`
- `pop that 0`

---

## 3. The Operating System: A Software Bridge Between Applications and Hardware

An **Operating System (OS)** sits between high-level applications and low-level hardware, acting as a carefully designed bridge. Applications speak in rich abstractions—objects, strings, arrays, graphics—while hardware understands only primitive operations on bits and memory. The OS translates between these two worlds, providing a standard library of services that make programming both possible and practical.

In this system, the OS is not a monolithic black box, but a collection of well-defined modules: **Math, Memory, Screen, Output, Keyboard, String, Array, and Sys**. Each module solves a specific problem that hardware alone cannot handle.

### Math: When Hardware Knows Only Addition

At the hardware level, the Arithmetic Logic Unit (ALU) typically supports only **addition and subtraction**. There is no native multiplication, division, or square root. These higher-level operations must therefore be implemented in software.

#### Multiplication

A naïve approach to multiplication computes
$x \times y$ as repeated addition—adding ( x ), ( y ) times.
This takes **O(N)** time and is far too slow.

A practical OS uses a **shift-and-add algorithm**, exploiting the binary representation of numbers:

- Any integer ( y ) can be written as a sum of powers of two.
- For each bit set in ( y ), the algorithm adds a shifted version of ( x ) to the result.
- Each step doubles ( x ) (left shift), and the loop runs only once per bit.

This reduces the complexity to **O(log N)**, which is fast enough for real systems.

<details><summary>Code</summary>

```python
sum = 0
shiftedX = x
for i in range(16):
    if (y[i] == 1):
        sum = sum + shiftedX
    shiftedX = shiftedX + shiftedX
return sum
```

</details>

#### Division and Square Root

Division works by repeatedly doubling the divisor until it nearly exceeds the dividend, then backing off recursively to assemble the quotient bit by bit. Conceptually, this is a controlled search for the largest power of two that fits.

Square root relies on the fact that $y = \sqrt{x}$ is **monotonically increasing**, and that squaring is easy to compute. The OS performs a **binary search** over the valid range of ( y ), quickly converging to the correct integer root.

**Example (16-bit system):**

- The maximum value of ( x ) is 65,535 ($2^{16} - 1$).
- $\sqrt{65{,}535} \approx 255$.
- Notably, ($255 = 2^8 - 1$), which is exactly ($2^{16/2} - 1$).

### Memory: Direct Control Over RAM

The most primitive memory operations are:

- `peek(address)`: read from memory
- `poke(address, value)`: write to memory

At a low level, an array variable does not store data—it stores a **base address**. Accessing `ram[i]` is really accessing `BaseAddress + i`.

- **Steps**:

  - Declare a static array variable in the `Memory` class:

    ```code
    static Array ram;
    ```

  - In the `Memory.init()` initialization function, forcibly set the base address of this array to 0:

    ```code
    let ram = 0;
    ```

- **`peek(address)`**:

  ```code
  function int peek(int address) {
      return ram[address];
  }
  ```

- **`poke(address, value)`**:

  ```code
  function void poke(int address, int value) {
      let ram[address] = value;
      return;
  }
  ```

#### The “Magic Trick” of Direct Memory Access

Normally, arrays are allocated on the heap at unpredictable locations. But by exploiting a weakly typed language and a permissive compiler, the OS can **force an array’s base address to 0**. This turns array indexing into direct physical memory access—effectively granting full control over RAM. This technique is unsafe in application code, but essential for OS kernels and system programming languages.

### Heap Management: Allocating and Freeing Memory

Objects and arrays live on the **heap**, while the stack stores only pointers to them. As programs run, memory must be allocated and later reclaimed.

The OS manages heap memory using a **free list**, where each free block stores:

- its size
- a pointer to the next free block

#### Allocation (`alloc`)

When memory is requested, the OS finds a free block large enough, splits it if necessary, and returns a pointer to usable memory.

#### Deallocation (`deAlloc`)

When memory is freed, the block is simply returned to the free list. Over time, this causes fragmentation, so adjacent free blocks must be merged to form larger contiguous regions.

Notably, this system has **no garbage collector**. Programmers must explicitly free objects when they are no longer needed.

**`alloc(size)` Algorithm**:

- **Input**: The size of the memory block to allocate (size ≥ `size + 2`; why +2? To reserve space for header metadata).
- **Action**: Search the heap for a sufficiently large free block and carve out a piece for the caller.
- **Output**: Returns the **base address (pointer)** of the allocated block.
- **Behind the scenes**: When you write `Point.new()`, the constructor internally calls `Memory.alloc(2)` because the `Point` object has two fields: `x` and `y`.

**`deAlloc(object)` Algorithm**: Simply **append** the returned or freed memory block to the end of the `freeList`.

- **Input**: The address of the object that is no longer needed.
- **Action**: Mark this memory block as free and return it to the pool for future allocations.
- **Behind the scenes**: When you write `do p.dispose()`, the system internally calls `Memory.deAlloc(p)`.

### Graphics and I/O in a Minimal Operating System

The **`Screen` class** in our operating system illustrates the fundamentals of graphics handling. At its core, all graphical operations reduce to **bit manipulation**. The screen is essentially a special block of memory: while high-level programs see lines, circles, and images, the operating system is busy calculating memory addresses and flipping specific bits. This **read-modify-write cycle** forms the backbone of low-level driver development.

There are two primary representations for graphics:

- **Bitmap**: Stores each pixel explicitly as black (1) or white (0). The file size grows with the number of pixels, and enlarging the image causes pixelation.
- **Vector**: Stores drawing instructions (e.g., "draw a line"), which can be scaled infinitely without losing clarity. Vector graphics are suitable for OS-level APIs like `drawLine` and `drawCircle`.

#### Drawing a Pixel

The **`drawPixel(x, y)`** method draws a single black pixel at coordinates `(x, y)`:

- **Address Calculation**: Each 16-pixel block corresponds to a 16-bit memory word. The memory address is calculated as:

  ```math
  Address = ScreenBase + (32 * y) + (x / 16)
  ```

  Here, each row contains 512 pixels (32 × 16). The CPU can only manipulate entire 16-bit words, not individual bits.

- **Read-Modify-Write**:

  - **Read**: Fetch the 16-bit word containing the target pixel (`val = Memory.peek(addr)`).
  - **Modify**: Change only the specific bit corresponding to `(x, y)`.
  - **Write**: Store the modified word back into memory (`Memory.poke(addr, newVal)`).

#### Drawing Lines and Circles

All graphics algorithms in this system approximate shapes using **addition and subtraction**, avoiding expensive multiplication and division.

**Drawing Lines (`drawLine`)**:

- **Problem**: Screens are discrete pixel grids; a straight line appears as a staircase of pixels.
- **Inefficient Approach**: Compute the slope `k = dy/dx` and compare continuously. Slow due to division.
- **Efficient Approach**: **Bresenham's Algorithm**.

  - Maintain an error term `diff = a*dy - b*dx`, where `a` is the horizontal step count and `b` is the vertical step count.
  - **Rules**:

    - `diff < 0`: Line too steep → move right (`a++`, `diff += dy`).
    - `diff > 0`: Line too shallow → move up (`b++`, `diff -= dx`).

  - **Advantage**: Only uses addition/subtraction, very fast.

**Drawing Circles (`drawCircle`)**:

- Slice the circle into horizontal lines.
- For each vertical offset `dy` from the center, calculate the horizontal width using the Pythagorean theorem:

  ```math
  dx = sqrt(r^2 - dy^2)
  ```

- Draw a horizontal line from `(x - dx, y + dy)` to `(x + dx, y + dy)`.

  - **Solid Circle**: Fill the line.
  - **Hollow Circle**: Draw only the endpoints.

**Implementation Notes**:

- Screen coordinates origin is at the top-left (y-axis grows downward), unlike traditional mathematical coordinates. Adjust algorithms accordingly.
- Bresenham’s algorithm must handle all quadrants, not just the northeast direction.
- Optimizations exist for horizontal or vertical lines (can fill memory blocks directly).
- Hardware limits 16-bit integers; the maximum radius for a circle is 181 pixels (181² ≈ 32761).

### Text Output (`Output` class)

The screen can operate in **Graphics Mode (256×512 pixels)** or **Text Mode (23×64 characters)**. The `Output` class bridges these two representations:

- Each character is stored as a bitmap (e.g., 11×8 pixels).
- A static array `characterMaps` encodes 127 ASCII characters.
- A logical cursor `(row, col)` tracks where to print next.
- Actions:

  - Regular characters: Print and move cursor right (`col++`).
  - Newline (`\n`): Move cursor to the next line (`row++, col=0`).
  - Backspace: Move cursor left (`col--`) and erase the character.

### Keyboard Input (`Keyboard` class)

The keyboard maps to a single 16-bit memory register (`RAM[24576]`):

- Pressing a key stores its ASCII code; releasing sets the register to 0.
- To ensure a single keystroke is counted once:

  ```
  while (keyPressed() == 0) {}  // wait for press
  char c = keyPressed();
  while (keyPressed() != 0) {}  // wait for release
  ```

- `readLine()` is built on top of `readChar()`, buffering characters until Enter is pressed:

  - Normal characters → append to buffer and echo.
  - Newline → end input.
  - Backspace → remove last character and erase on screen.

### Strings (`String` class)

Strings are implemented as a **character array with length metadata**:

- `buffer` stores ASCII codes.
- `length` tracks the current number of characters.
- `maxLen` is the maximum capacity.

**Integer-string conversion**:

- **Integer to String**: Repeatedly `% 10` to extract digits, divide by 10, then reverse via recursion.
- **String to Integer**: Accumulate digits left to right using the formula:

  ```math
  val = val * 10 + digit
  ```

### Arrays (`Array` class)

Arrays are essentially memory blocks:

- Compiler translates `a[i] = value` into memory access: `BaseAddress + i`.
- OS responsibilities:

  - **Creation**: `Array.new(size)` → `Memory.alloc(size)`.
  - **Destruction**: `dispose()` → `Memory.deAlloc(this)`.

### System Control (`Sys` class)

`Sys` orchestrates system initialization, pausing, and error handling:

- **Bootstrapping**:

  - Hardware starts at `ROM[0]`.
  - VM writes:

    ```text
    SP = 256   // initialize stack pointer
    call Sys.init
    ```

- **OS Layer (`Sys.init`)**:

  - Initialize all OS components (`Memory.init()`, `Math.init()`, `Screen.init()`, `Output.init()`, `Keyboard.init()`).
  - Call user program: `Main.main()`.

- **User Layer**:

  - Programmer writes `Main.main()` to implement applications (e.g., Tetris).

**Additional APIs**:

- `halt()`: Enters infinite loop (`while(true){}`) to simulate shutdown.
- `wait(duration)`: Busy-loop to delay for `duration` milliseconds; adjusted by CPU speed.
- `error(errorCode)`: Prints the error code (`Output.printInt(errorCode)`) and halts the system.
